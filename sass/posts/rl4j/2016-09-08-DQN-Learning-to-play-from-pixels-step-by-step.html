<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Reinforcement Learning and DQN, learning to play step by step from pixels - rubenfiszel's website</title>
	<link rel="stylesheet" type="text/css" href="../../css/syntax.css" />
	<link rel="stylesheet" type="text/css" href="../../sass/main.css" />
	<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
</head>

<body>
	<div id="header">
		<div id="logo">
			<a href="../../">rubenfiszel's website</a>
		</div>
		<div id="navigation">
			<a href="../../">Home</a>
			<a href="../../about.html">About</a>
			<a href="../../contact.html">Contact</a>
		</div>
	</div>

	<div id="content">
		<h1>Reinforcement Learning and DQN, learning to play step by step from pixels</h1>

		<div class="info">
			Posted on September 8, 2016

		</div>

		<h1 id="introduction">Introduction</h1>
		<p>My 2 month summer internship comes to an end and this is a post to summarize what I have been working on:
			Building a deep reinforcement learning library for DL4J: … [drum build-up] … RL4J! This post begins by an
			introduction to reinforcement learning and is then followed by a detailed explanation of DQN for pixel
			inputs and is concluded by an RL4J example. But first, lets talk about the core concepts of reinforcement
			learning.</p>
		<div class="figure">
			<img src="cartpole.gif" alt="Cartpole" />
			<p class="caption">Cartpole</p>
		</div>
		<div class="figure">
			<img src="doom.gif" alt="Doom" />
			<p class="caption">Doom</p>
		</div>
		<h1 id="preliminaries">Preliminaries</h1>
		<p>Reinforcement Learning is an exciting area of machine learning. It is basically the learning of an efficient
			strategy in a given environment. Informally, this is very similar to Pavlovian conditionning: you assign a
			reward for a given behavior and over time, the agents learn to reproduce that behavior in order to receive
			more rewards.</p>
		<h2 id="markov-decision-process">Markov Decision Process</h2>
		<p>Formally, an environment is defined as a Markov Decision Process (MDP). Behind this scary name is nothing
			else than the combination (5-tuple):</p>
		<ul>
			<li>A set of states <span class="math inline">\(S\)</span> (eg: in chess, a state is the board
				configuration)</li>
			<li>A set of possible action <span class="math inline">\(A\)</span> (In chess, all the move that could be
				possible in every configuration possible, eg: e4-e5)</li>
			<li>The conditional distribution <span class="math inline">\(P(s'| s, a)\)</span> of next state given a
				current state and an action. (In a deterministic environment like chess, there is only one state s’ with
				probability 1, and all the other with probability 0. Nevertheless, in a stochastic (involving
				randomness, eg: a coin toss) environment, the distribution is not as simple.)</li>
			<li>The reward function of transitionning from state s to s’: <span class="math inline">\(R(s, s')\)</span>
				(eg: In chess, +1 for a final move that leads to a victory, -1 for a final move that leads to a defeat,
				0 otherwise).</li>
			<li>The discount factor: <span class="math inline">\(\gamma\)</span>. This is the preference for present
				rewards compared to future rewards. (A concept very common in <a
					href="https://en.wikipedia.org/wiki/Discounted_utility">finance</a>.)</li>
		</ul>
		<p>Note: It is usually more convenient to use the set of Action <span class="math inline">\(A_s\)</span> which
			is the set of available move from a given state, than the complete set A. <span
				class="math inline">\(A_s\)</span> is simply the a elements <span class="math inline">\(\in A\)</span>
			such that <span class="math inline">\(P(s' | s, a) &gt; 0\)</span>.</p>
		<p>The markov property is to be memoryless. Once you reach a state, the past history (the states visited before)
			should not affect the next transition and reward. Only the present state matters.</p>
		<div class="figure">
			<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Markov_Decision_Process_example.png/400px-Markov_Decision_Process_example.png"
				alt="Schema of a MDP" />
			<p class="caption">Schema of a MDP</p>
		</div>
		<p><strong>Final states</strong>: The states that have no available actions are final states.</p>
		<p><strong>Episode</strong>: An episode is a complete play from one of the initial state to a final state. <span
				class="math display">\[s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_n\]</span></p>
		<p><strong>Cumulative reward</strong>: The cumulative reward is the discounted sum of reward accumulated
			throughout an episode: <span class="math display">\[R=\sum_{t=0}^n \gamma^t r_{t+1}\]</span></p>
		<p><strong>Policy</strong>: A Policy is the agent’s strategy to choose an action at each state. It is noted by
			<span class="math inline">\(\pi\)</span>.</p>
		<p><strong>Optimal policy</strong>: The optimal policy is the theoritical policy that maximizes the expectation
			of cumulative reward. From the definition of expectation and the law of large numbers, this policy has the
			highest average cumulative rewards given sufficient episode. This policy might be intractable.</p>
		<p><strong>The objective of reinforcement learning is to train an agent such that he learns a policy as close as
				possible to the optimal policy</strong>.</p>
		<h1 id="different-settings">Different settings</h1>
		<p>« <em>Qui peut le plus peut le moins</em> »</p>
		<p>(He who can do the greather things, can do the lesser things)</p>
		<h2 id="model-free">Model-free</h2>
		<p>The conditional distribution and the reward function are the model of the environment. Some reinforcement
			learning algorithms can work without being given the model. Thus, in order to learn the best strategy, they
			also have to learn the model during the training. This is called model-free reinforcement learning.
			Model-free algorithms are very important because a large majority of real world complex problems fall in
			that category. Futhermore, model free is simply an additional constraint. Model-free reinforcement learning
			is simply more powerful since it is a superset of model based reinforcement learning.</p>
		<h2 id="observation-setting">Observation setting</h2>
		<p>Instead of being given access to the state, you might being given access to a partial observation of the
			state only. It is the same idea behind Hidden Markov Chain. This is the difference between the partial and
			fully observed setting. For instance, our field of vision is a very partial observation of the full state of
			the universe (the position and energy of every particule in the universe). Fortunately, the partial
			observation settings can be reduced to a fully observed setting by using an history (the state is an
			accumulation of previous states).</p>
		<p>Nevertheless, it is most common to not accumulate the whole history. Either only the last h observations are
			stacked (in a windowed fashion) or you can use a Recurrent Neural Network (RNN) to learn what to keep and
			what to forget.</p>
		<p>Abuse the language slighty for consistency purpose with the existing notation, history (even truncated ones)
			will also be called “states” and also symbolized <span class="math inline">\(S_t\)</span></p>
		<h2 id="single-player-and-adversarial-games">Single player and adversarial games</h2>
		<p>A single player game has a natural translation into a MDP. The states represent the moment where the player
			is in control. The observations from those states are all the informations that happen during a transition.
			An action is all the available command at the disposal of the player (In doom, go up, right, left, shoot).
		</p>
		<p>Reinforcement learning can also be applied to adversarial games by self-play: The agent plays against itself.
			Often in this setting, there exists a <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash
				equilibrium</a> such that it is always in your interest to play as if your opponent was a perfect
			player. This makes sense in chess by example, where if given a board configuration, a move would be good
			against a chess master, it would still be a good move against a beginner [^1]. Whatever is the current level
			of the agent, by playing against himself, he still get information about the quality of his previous moves
			(good moves if he won, bad moves if he lost on average). Of course the information, which is a gradient in
			the context of a neural network, is of «higher quality» if he played directly against a very good agent from
			the start. But it is really magic that an agent can learn to increase his level of play by playing against
			himself, an agent of the same level. That is actually the method of training employed by AlphaGo. The policy
			was bootstrapped on a dataset of master moves, then they used reinforcement learning and self play to
			increase furthermore the level of their agent (and in the end gets better than the original dataset). They
			used their policy gradient in combination with a Monte-Carlo Search Tree on a huge amount of computation
			power.</p>
		<p>This setting is a bit different and not exactly where most of the research is currently focusing. In the
			following of this post, I will talk exclusively about the 1-player setting.</p>
		<p>[^1] (Note: Nevertheless As my semester project advisor remarked, this is not always true. Imagine you play
			against a beginner at chess and you are badly loosing, you might want to “bait” him to reverse the
			situation. A bait is only beneficial when the opponent has a low level of play. This proove that chess is
			not a real Nash equilibrium. This does not matter much because the goal is often to build a high level of
			play agent that plays against other high quality agents. At this level of play, you do not loose if given a
			significant advantage.</p>
		<h1 id="q-learning">Q-learning</h1>
		<h2 id="from-policy-to-neural-network">From policy to neural network</h2>
		<p>Our goal is learn the optimal policy <span class="math inline">\(\pi^*\)</span> that maximize <span
				class="math display">\[E[R]=E[\sum_{t=0}^n \gamma^t r_{t+1}]\]</span> Let’s introduce an auxilliary
			function: <span class="math display">\[V_\pi(s) = E \{ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots +
				\gamma^n r_{n} \mid s_t = s, \text{policy followed at each state is }\pi \}\]</span> which is the
			expected cumulative reward from a state <span class="math inline">\(s\)</span> following the policy <span
				class="math inline">\(\pi\)</span>.</p>
		<p>Let suppose an oracle <span class="math display">\[V_\pi^*(s)\]</span>, the V function of the optimal policy.
			From it, we could retrieve the optimal policy by defining the policy that among all available actions at the
			current action, choose the action that maximize the expectation of <span
				class="math inline">\(V_\pi^*(s)\)</span>. This is a greedy behavior. The optimal policy is the greedy
			policy w.r.t to <span class="math inline">\(V_\pi^*\)</span>.</p>
		<p><span class="math display">\[ \pi(s) ~\text{chooses a s.t}~a=\max_a[E(r_t + \gamma V(s_{t+1}) \mid s_t=s,
				a_t=a)] \]</span></p>
		<p>If you were extremely attentive, something would sound wrong here. In the model-free setting, we cannot
			predict the after-state <span class="math inline">\(s_{t+1}\)</span> from <span
				class="math inline">\(s_{t}\)</span> because we ignore the transition model.</p>
		<p>To solve this very annoying issue, we are gonna use another auxiliarry function, the Q function:</p>
		<p><span class="math display">\[Q_\pi(s, a) = E[r_t + \gamma V_\pi(s_{t+1} \mid s_t, a_t = a]\]</span></p>
		<p>In a greedy setting, we have the relationship:</p>
		<p><span class="math display">\[V(s_t) = \max_a Q(s_t, a)\]</span></p>
		<p>Now, let suppose instead of the V oracle, we have the Q oracle. We can now redefine <span
				class="math inline">\(\pi^*\)</span>.</p>
		<p><span class="math display">\[ \pi(s) ~ \text{chooses a s.t}~ a= \max_a[Q(s, a)] \]</span></p>
		<p><strong>Neat</strong></p>
		<p>Unfortunately, oracles do not exist in the real world.</p>
		<p>The trick here is that we have reduced an abstract notion that is a policy into a numerical function.
			Fortunately for us, there is one weapon at our disposal to approximate complex functions: <strong>Neural
				networks</strong>.</p>
		<p>Neural networks are universal function approximator. They can approximate any differentiable function.
			Although they can get stuck in local extrema and many proofs are not valid anymore when throwing neural
			networks in the equation. This is because their learning is not as deterministic or boundable than a linear
			function. Nonetheless, in most case, with the right hyperparameters, they are unreasonnably powerful. Using
			deep learning with reinforcement learning is called deep reinforcement learning.</p>
		<h2 id="policy-iteration">Policy iteration</h2>
		<p>Now machine learning knowledge and common sense tells you that there is still something missing about our
			approeach. Neural networks can approximate functions that already have labels. Unfortunately for us oracles
			do not exist in the real world, so we will have to get our labels another way. (:&lt;).</p>
		<p>This is where the magic of Monte Carlo come in. Monte carlo methods are methods that rely on repeated random
			sampling to calculate an estimator. (A famous example is the <a
				href="http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopimod.html">pi calculation</a>).</p>
		<p>If we play randomly from a given state, the better state should get better reward on average. So without
			knowing anything about the environment, you can get some information about the expected value of a state.
			For example, at poker, better hands will win more often on average than lesser hands when every decision is
			taken randomly. Monte Carlo Search Tree are based on this property. This is a phase of exploration that lead
			to unsupervised learning and enable us to extract meaningful label.</p>
		<p>More formally,</p>
		<p>Given a policy <span class="math inline">\(\pi\)</span>, a state s and an action a, to get an approximation
			of <span class="math inline">\(Q(s, a)\)</span> we sample it according to its definition:</p>
		<span class="math display">\[\begin{align*}
			Q_\pi(s, a) &amp;= E[r_t + V_\pi(s_{t+1}) \mid s_t = s, a_t = a] \\
			&amp;= E[r_t + \gamma E \{ r_{t+1} + \gamma r_{t+2} + \ldots + \gamma^{n-1} r_{n}\} \mid s_t = s, a_t = a]
			\\
			&amp;= E[r_t + \gamma r_{t+1} + \ldots + \gamma^n r_{n}\} \mid s_t = s, a_t = a]
			\end{align*}\]</span>
		<p>In plain english, we can get a label for <span class="math inline">\(Q_\pi(s, a)\)</span> by playing a
			sufficient number of time from s according to the policy <span class="math inline">\(\pi\)</span>.</p>
		<p>From an aggregate of signals:</p>
		<div class="figure">
			<img src="qtarget.png" alt="One signal" />
			<p class="caption">One signal</p>
		</div>
		<p>We use the Mean-Square Error loss function (l2 loss) with a learning rate of <span
				class="math inline">\(\alpha\)</span> and apply Stochastic Gradient Descent (On a batch of size 1):
			<span class="math display">\[Q_\pi(s_t, a_t) \leftarrow Q_\pi(s_t, a_t) + \alpha [R_t-Q_\pi(s_t,
				a_t)]\]</span></p>
		<p>Note: There is no square in the formula because the loss is applied afterwards on the difference of the
			expected output <span class="math inline">\(Q_\pi(s_t, a_t)\)</span> and the label <span
				class="math inline">\(\alpha [R_t-Q_\pi(s_t, a_t)]\)</span></p>
		<p>Repeat many times: Sampling from <span class="math inline">\(\pi\)</span> <span
				class="math display">\[Q_\pi(s_t, a_t) \leftarrow E_\pi[R_t] = E_{s_t, a_t, ..., s_n \sim
				\pi}[\sum_{i=t}^n \gamma^{i-t}r_i]\]</span></p>
		<p>We can converge to the rightful expectation</p>
		<div class="figure">
			<img src="qexpect.png" alt="Many signals" />
			<p class="caption">Many signals</p>
		</div>
		<p>So we can now design a naive prototype of our learning algorithm (in Scala but it is intelligible without any
			Scala knowledge):</p>
		<div class="sourceCode">
			<pre class="sourceCode scala"><code class="sourceCode scala">
<span class="co">//A randomly uninitialized neural network</span>
<span class="kw">val</span> neuralNet: NeuralNet

<span class="co">//Iterate until you reach max epoch</span>
<span class="kw">for</span> (t &lt;- (<span class="dv">1</span> to MaxEpoch))
	<span class="fu">epoch</span>()


<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//pick a random state and action</span>
	<span class="kw">val</span> state = randomState
	<span class="kw">val</span> action = <span class="fu">randomAction</span>(state)

	<span class="co">//transition to a new state, initalize thethe reward</span>
	<span class="kw">var</span> (new_state, accuReward) = <span class="fu">transitition</span>(state, action)

	<span class="co">//play until terminal state and accumulate the reward</span>
	accuReward += <span class="fu">playRandomly</span>(state)

	<span class="co">//Do SGD over input and label!</span>
	<span class="fu">fit</span>((state, action), accuReward)
}



<span class="co">// MDP specific, return the new state and the reward</span>
<span class="kw">def</span> <span class="fu">transition</span>(state: State, action: Action): (State, Double)

<span class="co">//return a randomly sampled state among all the state space</span>
<span class="kw">def</span> randomState: State

<span class="co">//play until terminal state</span>
<span class="kw">def</span> <span class="fu">playRandomly</span>(state): Double = {
	<span class="kw">var</span> s = state
	<span class="kw">var</span> accuReward = <span class="dv">0</span>
	<span class="kw">var</span> k = <span class="dv">0</span>
	<span class="kw">while</span> (!s.<span class="fu">isTerminal</span>) {
		<span class="kw">val</span> action = <span class="fu">randomAction</span>(s)
		<span class="kw">val</span> (state, reward) = <span class="fu">transition</span>(s, action)
		k += <span class="dv">1</span>
		accuReward += Math.<span class="fu">pow</span>(gamma, k) * reward
		s = state
	}
	accuReward
}


<span class="co">//choose a random action among all the available actions at this state</span>
<span class="kw">def</span> <span class="fu">randomAction</span>(state: State): Action =
	<span class="fu">oneOf</span>(state.<span class="fu">available_action</span>)

<span class="co">//helper function, pick one among</span>
<span class="kw">def</span> <span class="fu">oneOf</span>(seq: Seq[Action]): Action =
	seq.<span class="fu">get</span>(Random.<span class="fu">nextInt</span>(seq.<span class="fu">size</span>))


<span class="co">//How it would be roughly done with DL4J</span>
<span class="kw">def</span> <span class="fu">fit</span>(input: (State, Action), label: Double) =
	neuralNet.<span class="fu">fit</span>(<span class="fu">toTensor</span>(input), <span class="fu">toTensor</span>(label))

<span class="co">//return an INDArray from ND4J</span>
<span class="kw">def</span> <span class="fu">toTensor</span>(array: Array[_]): Tensor =
	Nd4j.<span class="fu">create</span>(array)</code></pre>
		</div>
		<p>There is multiple issues: This should work but this is terribly inefficient. We are playing a full game with
			n state and n actions for a single label and that label might not be very meaningful (If the interesting
			trajectories are hard to reach at random).</p>
		<h2 id="offline-and-online-reinforcement-learning">Offline and online reinforcement learning</h2>
		<p>To learn more the difference between online and offline reinforcement learning, see this excellent post from
			<a
				href="https://kofzor.github.io/Reinforcement_Learning_101/#comparing-reinforcement-learning-algorithms">kofzor</a>
		</p>
		<h2 id="explorationexploitation">Exploration/exploitation</h2>
		<p>Exploring at random the environment will converge to the optimal policy … after an almost infinite time: you
			will have to visit every possible trajectories (a trajectory is the state visited and action choosen during
			an episode) at least once. In the real world, we do not have infinite time (and time is money).</p>
		<p>Thus, we should exploit the past informations and our learning of them to focus our exploration on the most
			promising possible trajectories. This can be achieved through different ways, and one of them that we are
			gonna use is <span class="math inline">\(\epsilon\)</span>-greedy exploration. <span
				class="math inline">\(\epsilon\)</span>-greedy exploration is fairly simple. It is a policy that choose
			an action at random with odd <span class="math inline">\(epsilon\)</span> or the best action as deemed by
			our current policy. Usually <span class="math inline">\(epsilon\)</span> is annealed over time to privilege
			exploitation over exploration. This is a trade-off between exploration and exploitation.</p>
		<p>At each new information, our actual Q functions gets more accurate about the present policy and the
			exploration is focused on better paths. The policy based on our new Q function gets better (since Q is more
			accurate) and the <span class="math inline">\(\epsilon\)</span>-greedy exploration reach better path.
			Focused on those better paths, our q function explore even more the better parts and has to update its
			returns according to the new policy. This is an iterative cycle that enable convergence to the optimal
			policy called policy iteration. Unfortunately, the convergence can takes infitnite time and is not even
			guaranteed when Q is approximated by neural networks. Nevertheless, impressive results can make up for the
			lack of formal convergence proofs.</p>
		<div class="figure">
			<img src="policyiter.png" alt="Policy Iteration" />
			<p class="caption">Policy Iteration</p>
		</div>
		<p>It also requires you to be able to sample the states in a good manner: It should be proportionally
			representative of the states that are usally present in a game. (On a sidenote, this is possible in some
			case, see <a href="#giraffe">Giraffe</a> that uses TD-Lambda).</p>
		<p>There is 3 important observations:</p>
		<h2 id="bellman-equation">Bellman equation</h2>
		<ul>
			<li>We can transform the Q equation into a <strong>Bellman equation</strong>: <span
					class="math display">\[Q_\pi(s, a)= E[r_t + \gamma r_{t+1} + \ldots + \gamma^n r_{n} \mid s_t = s,
					a_t = a]\]</span> <span class="math display">\[Q_\pi(s, a)= E[r_t + \gamma r_{t+1} + V(s_{t+1}) \mid
					s_t = s, a_t = a]\]</span> <span class="math display">\[Q_\pi(s, a)= E[r_t + \gamma r_{t+1} + \ldots
					+ \gamma \max_a' Q(s_{t+1}, a')\} \mid s_t = s, a_t = a]\]</span></li>
		</ul>
		<p>As in the Monte-Carlo method, we can do many updates of Q.</p>
		<p>MSE: <span class="math display">\[Q_\pi(s_t, a_t) \leftarrow Q_\pi(s_t, a_t) + \alpha
				[(\underbrace{\underbrace{r_t+\max_a Q_\pi(s_{t+1}, a)}_{\text{target}}-Q_\pi(s_t,
				a_t)}_{\text{TD-error}})]\]</span></p>
		<p>TD-error is the “Temporal difference error”. Indeed we are actually calculating the difference between what
			the Q approximation expects in the future and its present value and the realized reward.</p>
		<p>if s is terminal:</p>
		<p><span class="math display">\[V(s) = 0\]</span></p>
		<p>and</p>
		<p><span class="math display">\[Q(s_{t-1}, a) = r_t\]</span></p>
		<p>The states near the terminal states are the first to converge because they are closer in the chain to that
			truth label: In Go or Chess, reinforcement learning is applied by assigning +1 to the transitions that lead
			to a final winning board (respectively -1 for a loosing board) and 0 otherwise. This lead to diffuse back
			how good is a transition by assigning it a value between [-1; 1]. A transition with Q value close to 0 lead
			to a balanced board. A transition with Q value close to 1 lead to a near certain victory.</p>
		<p>As long as we sample sufficiently transitions near the terminal states, Q-learning is able to converge. The
			incredible power of deep reinforcement learning is that it will be able to generalize its learning from
			visited states to unvisited states. It should be able to understand what is a balanced or winning board even
			if it never seen it before. This is because the network should be able to abstract patterns and understand
			the strength of an action based on previously seen pattern (eg: shoot an ennemy when recognizing one).</p>
		<h2 id="initial-state-sampling">Initial state sampling</h2>
		<p>In a 1 player setting (like the atari game): We do not actually need to learn to play well in every situation
			(Although, if we did, that would show that we would have reached a very good level of generalization). We
			only need to learn to play efficiently from the states that our policy encounters. Thus, we can sample from
			states that are simply reachable by playing with our current policy from an initial state.</p>
		<h2 id="q-learning-implementation">Q-Learning implementation</h2>
		<p>So we can now design a naive prototype of our Q-Learning:</p>
		<div class="sourceCode">
			<pre class="sourceCode scala"><code class="sourceCode scala">

<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//sample among the initial state space</span>
	<span class="co">//(often unique state)</span>
	<span class="kw">var</span> state = initState

	<span class="co">//while the state is not terminal,</span>
	<span class="co">//play an episode and do a Q-update at each transition</span>
	<span class="kw">while</span>(!state.<span class="fu">isTerminal</span>)  {

		<span class="co">//sample action from eps-greddy policy</span>
		<span class="kw">val</span> action = <span class="fu">epsilonGreedyAction</span>(state)

		<span class="co">//interaction with the environment</span>
   		<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)

		<span class="co">//Q-update</span>
		<span class="fu">update</span>(state, action, reward, nextState)

		state = nextState
	}
}

<span class="co">//Our Q-update as explained above</span>
<span class="kw">def</span> <span class="fu">update</span>(state: State, action: Action, reward: Double, nextState: State) = {
	<span class="kw">val</span> target = reward + <span class="fu">maxQ</span>(nextState)
	<span class="fu">fit</span>((state, action), target)
}


<span class="co">//the eps-greedy policy implementation</span>
<span class="kw">def</span> <span class="fu">epsilonGreedyAction</span>(state: State) = {
	<span class="kw">if</span> (Random.<span class="fu">float</span>() &lt; epsilon)
		<span class="fu">randomAction</span>(state)
	<span class="kw">else</span>
		<span class="fu">maxQAction</span>(state)
}


<span class="co">//Retrive max Q value</span>
<span class="kw">def</span> <span class="fu">maxQ</span>(state: State) =
	<span class="fu">actionsWithQ</span>(state).<span class="fu">maxBy</span>(_.<span class="fu">_2</span>).<span class="fu">_2</span>

<span class="co">//Retrive action of the max Q value</span>
<span class="kw">def</span> <span class="fu">maxQAction</span>(state: State) =
	<span class="fu">actionsWithQ</span>(state).<span class="fu">maxBy</span>(_.<span class="fu">_2</span>).<span class="fu">_1</span>

<span class="co">//return a list of actions and the q-value of their transition from the state</span>
<span class="kw">def</span> <span class="fu">actionsWithQ</span>(state: State) = {
	<span class="kw">val</span> stateActionList = available_actions.<span class="fu">map</span>(action =&gt; (state, action))
   	available_actions.<span class="fu">zip</span>(neural_net.<span class="fu">output</span>(<span class="fu">toTensor</span>(state_action_list)))

<span class="kw">def</span> initState: State

</code></pre>
		</div>
		<h2 id="modeling-qs-a">Modeling Q(s, a)</h2>
		<p>Instead of having <span class="math inline">\(a\)</span> as an additional input of the neural net combined
			with the state, the state is the only input and the output contain the Q value of every output possible.
			This make sense only when the availables actions are consistent accross the full episode.</p>
		<div class="figure">
			<img src="qmodeling.png" alt="Q modeling" />
			<p class="caption">Q modeling</p>
		</div>
		<p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Source</a></p>
		<h2 id="experience-replay">Experience replay</h2>
		<p>There is one issue with using neural network as Q approximator with neural networks. The transitions are very
			correlated. After all, they are all extracted from the same episode. Imagine if you had to learn a task
			without any memory (not even short-term), you would always optimise your learning based on the last episode.
		</p>
		<p>The DeepMind team have had a genius idea to use an experience replay, which is a windowed buffer of the last
			N transition (N being a million in the original paper). Instead of updating from the last transition, you
			store it inside the experience replay and update from a batch of randomly sampled transitions from the same
			experience replay.</p>
		<p>epoch() becomes:</p>
		<div class="sourceCode">
			<pre class="sourceCode scala"><code class="sourceCode scala">
<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//sample among the initial state space</span>
	<span class="co">//(often unique state)</span>
	<span class="kw">var</span> state = initState

	<span class="co">//while the state is not terminal,</span>
	<span class="co">//play an episode and do a Q-update at each transition</span>
	<span class="kw">while</span>(!state.<span class="fu">isTerminal</span>)  {

		<span class="co">//sample action from eps-greddy policy</span>
		<span class="kw">val</span> action = <span class="fu">epsilonGreedyAction</span>(state)

		<span class="co">//interaction with the environment</span>
   		<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)

		<span class="co">//store transition (Exp Replay is just a Ring buffer)</span>
		expReplay.<span class="fu">store</span>(state, action, reward, nextState)

		<span class="co">//Q update in batch</span>
		<span class="fu">updateFromBatch</span>(expReplay.<span class="fu">getBatch</span>())

		state = nextState
	}
}</code></pre>
		</div>
		<h3 id="compression">Compression</h3>
		<p>ND4J does not support as first class uint8 but pixels in grayscaling are encoded with that precision. To
			avoid wasting too much space on memory, INDArray were compressed to uint8.</p>
		<h2 id="convolutional-layers-and-image-preprocessing">Convolutional layers and image preprocessing</h2>
		<h3 id="convolutional-layers">Convolutional layers</h3>
		<div class="figure">
			<img src="conv.png" alt="Convolutional layer" />
			<p class="caption">Convolutional layer</p>
		</div>
		<p><a href="#atari">Source</a></p>
		<p>Convolutional layers are layers that are excellent to detect local patterns in images. For pixels it is used
			as a processor that is required to reduce the dimension of the input into its real manifold. Given the
			proper manifold of observations, the decision becomes much easier.</p>
		<h3 id="image-processing">Image processing</h3>
		<p>You could feed the neural network with the RGB directly, but then the network would have to also learn that
			additional pattern. It seems like the brain is hard-wired to combine colors so it would seem reasonnable to
			accept that preprocessing.</p>
		<div class="figure">
			<img src="doom-pre.gif" alt="This is what you see" />
			<p class="caption">This is what you see</p>
		</div>
		<div class="figure">
			<img src="doom-post.gif" alt="This is what the Neural Net see" />
			<p class="caption">This is what the Neural Net see</p>
		</div>
		<h3 id="resizing">resizing</h3>
		<p>The image is resized into 84x84. Convolutional layers needs for memory and computations grow with the size of
			their input. The fine details of the image are not required to play the game correctly. Indeed, many are
			purely aesthetic. Resizing to a more reasonnable size speed up the training.</p>
		<h3 id="skip-frame">Skip frame</h3>
		<p>In the original paper, only 1 in 4 frame is actually processed. For the following 3 images, the last action
			is repeated. It speeds up rougly by 4 time the training without loosing much information. Indeed, atari game
			are not supposed to be played frame perfect and for most action it makes more sense to keep them for at
			least 4 frames.</p>
		<h3 id="history-processing">History Processing</h3>
		<p>To give information to the Neural network about the current momentum, the last 4 frame (with skip frame, you
			pick 1 every 4) are stacked into 4 channels. Those 4 frames represent an history as previously discussed in
			the Observation setting section.</p>
		<div class="figure">
			<img src="slowed.gif" alt="Slowed input" />
			<p class="caption">Slowed input</p>
		</div>
		<div class="figure">
			<img src="stacked.png" alt="Stacking" />
			<p class="caption">Stacking</p>
		</div>
		<p>To fill the first frames of the history, a random policy or a noop replay is used.</p>
		<h2 id="double-q-learning">Double Q-learning</h2>
		<p>The idea behind <a href="#doubleq">double DQN</a> is that the network is frozen every M update (hard update)
			or smoothly averaged (<code>target = target * (smooth) + current * (1-smooth)</code>) every update (soft
			update). Indeed, it adds stability to the learning by using a Q evaluation to use in the td-error formula
			that is less prone to unstability. The Q update becomes:</p>
		<p><span class="math display">\[Y_\text{target} = r_t + \gamma*(Q_\text{target}(s_t+1, arg \max_a Q_\text(s_t+1,
				a))) \]</span></p>
		<h2 id="clipping">Clipping</h2>
		<p>The TD-error is clipped such that no outlier update can have too much impact on the learning.</p>
		<h2 id="scaling-rewards">Scaling rewards</h2>
		<p>Scaling the rewards such that Q-values are lower (in a range of [-1; 1] is similar to normalization). It can
			dramatically alter the efficiency of the learning.</p>
		<h2 id="prioritized-replay">Prioritized replay</h2>
		<p>The idea behind <a href="#prio">prioritized replay</a> is that not all transitions are born equal. Some are
			more important than others. One way to sort them is through their TD-error. Indeed, a high TD-error is
			correlated to a high level of information (in the sense of surprise). Those transitions should be sampled
			more often than the others.</p>
		<h2 id="graph-visualisation-and-mean-q">Graph, Visualisation and Mean-Q</h2>
		<p>To visualize and debug the training or a method of RL, it is a useful to have a visual monitoring.</p>
		<div class="figure">
			<img src="screen.png" alt="rl4j-webapp" />
			<p class="caption">rl4j-webapp</p>
		</div>
		<p>The most important is to keep track of cumulative reward. This is a way to check that the agents effectively
			gets better. It is important to notice that it represents the epsilon greedy strategy and not directly
			dervied policy from the Q approximation.</p>
		<div class="figure">
			<img src="rewardgraph.png" alt="Cumulative reward graph" />
			<p class="caption">Cumulative reward graph</p>
		</div>
		<p>But also the loss (score) and mean Q-values:</p>
		<div class="figure">
			<img src="scoregraph.png" alt="Score and mean-Q graph" />
			<p class="caption">Score and mean-Q graph</p>
		</div>
		<p>If used with target network, you should see some discontinuities from the non continuous evaluation of
			different target networks. Loss should decrease w.r.t to a single target network. The mean Q-values should
			smoothly converge towards a value proportionnal to the mean expected reward.</p>
		<h2 id="rl4j">RL4J</h2>
		<p>RL4J is available on <a href="https://github.com/deeplearning4j/rl4j">github</a>. Currently DQN with
			Experience Replay, Double Q-learning and clipping is implemented. It is possible to play both from pixels or
			low-dimensional problems (like Cartpole). Async Reinforcement Learning is experimental. Hopefully,
			contributions will enrich the library.</p>
		<p>Here is a working example with RL4J to play Cartpole. You can play Doom too. Check <a
				href="https://github.com/rubenfiszel/rl4j-examples">rl4j-examples</a> for more examples. It is also
			possible to provide your own constructed neural network model as an argument to any training method.</p>
		<div class="sourceCode">
			<pre class="sourceCode java"><code class="sourceCode java">
    <span class="kw">public</span> <span class="dt">static</span> QLearning.<span class="fu">QLConfiguration</span> CARTPOLE_QL =
            <span class="kw">new</span> QLearning.<span class="fu">QLConfiguration</span>(
                    <span class="dv">123</span>, <span class="co">//Random seed</span>
                    <span class="dv">500</span>, <span class="co">//Max step By epoch</span>
                    <span class="dv">150000</span>, <span class="co">//Max step</span>
                    <span class="dv">150000</span>, <span class="co">//Max size of experience replay</span>
                    <span class="dv">32</span>, <span class="co">//size of batches</span>
                    <span class="dv">100</span>, <span class="co">//target update (hard)</span>
                    <span class="dv">10</span>,  <span class="co">//num step noop warmup</span>
                    <span class="fl">0.01</span>, <span class="co">//reward scaling</span>
                    <span class="fl">0.99</span>, <span class="co">//gamma</span>
                    <span class="fl">100.0</span>, <span class="co">//td-error clipping</span>
                    <span class="fl">0.</span>1f, <span class="co">//min epsilon</span>
                    <span class="dv">1000</span>, <span class="co">//num step for eps greedy anneal</span>
                    <span class="kw">true</span>
            );


    <span class="kw">public</span> <span class="dt">static</span> DQNFactoryStdDense.<span class="fu">Configuration</span> CARTPOLE_NET =
            <span class="co">//num layers, num hidden nodes, learning rate, l2 regularization</span>
            <span class="kw">new</span> DQNFactoryStdDense.<span class="fu">Configuration</span>(<span class="dv">3</span>, <span class="dv">16</span>, <span class="fl">0.001</span>, <span class="fl">0.00</span>);

    <span class="kw">public</span> <span class="dt">static</span> <span class="dt">void</span> <span class="fu">main</span>( String[] args )
    {

        <span class="co">//true means record this in rl4j-data in a new folder</span>
        DataManager manager = <span class="kw">new</span> <span class="fu">DataManager</span>(<span class="kw">true</span>);

        <span class="co">//define the mdp from gym (name, render)</span>
        GymEnv&lt;Box, Integer, DiscreteSpace&gt; mdp = <span class="kw">new</span> <span class="fu">GymEnv</span>(<span class="st">&quot;CartPole-v0&quot;</span>, <span class="kw">false</span>, <span class="kw">false</span>);

        mdp.<span class="fu">reset</span>();
        <span class="dt">double</span>[] arr = mdp.<span class="fu">step</span>(<span class="dv">1</span>).<span class="fu">getObservation</span>().<span class="fu">toArray</span>();
        System.<span class="fu">out</span>.<span class="fu">println</span>(arr[<span class="dv">0</span>]);
        mdp.<span class="fu">reset</span>();
        <span class="co">//define the training</span>
        QLearningDiscreteDense&lt;Box&gt; dql = <span class="kw">new</span> <span class="fu">QLearningDiscreteDense</span>(mdp, CARTPOLE_NET, CARTPOLE_QL, manager);

        <span class="co">//train</span>
        dql.<span class="fu">train</span>();

        <span class="co">//get the final policy</span>
        DQNPolicy&lt;Box&gt; pol = dql.<span class="fu">getPolicy</span>();

        <span class="co">//serialize and save (serialization showcase, but not required)</span>
        pol.<span class="fu">save</span>(<span class="st">&quot;/tmp/pol1&quot;</span>);

        <span class="co">//close the mdp</span>
        mdp.<span class="fu">close</span>();


    }</code></pre>
		</div>
		<h1 id="conclusion">Conclusion</h1>
		<p>This was an exciting journey through deep reinforcement learning. From equations to code, Q-learning is a
			powerful yet a somewhat simple algorithm. The field of RL is very active and promising. In fact, Supervised
			learning could be considered a subset of Reinforcement learning (by setting the labels as rewards). Maybe
			one day, Reinforcement Learning will be the panacea. Until then, we can expect to be awed by it’s diverse
			application into more and more complex problems. I would also like to thank Skymind and its amazing team for
			this very enriching internship.</p>
		<h1 id="to-feed-your-appetite">To feed your appetite</h1>
		<p>I hope that thanks to this introduction, you are excited about R researchL. Here is a brief summary of
			important research in deep reinforcement learning.</p>
		<h2 id="continuous-domain">Continuous domain</h2>
		<p>When the action space is not discrete, you cannot use DQN. But many problems cannot be discretized. <a
				href="#continuous">Continuous control</a> is achieved through a normal distribution parametrized (mean
			and variance) by the output of the neural network. At each step, the action is sampled from the
			distribution. It also uses soft update of the target network.</p>
		<h2 id="policy-gradient">Policy gradient</h2>
		<p>Policy gradient work by directly learning the stochastic policy from the the log distribution. This excellent
			post from <a href="#karpathy">Karpathy’s blog</a> has more details on the matter. Using a stochastic policy
			feels more natural as it encourage exploration and exploit more fairly the uncertainty we have between the
			different values of the move. Indeed a max operation can end by ignoring fully a branch that is <span
				class="math inline">\(\epsilon\)</span> below in Q-value of another.</p>
		<p>Policy gradient were used by AlphaGo in combination with MonteCarlo Search Tree. The Neural Network was
			bootstrapped (pretrained) on a dataset of master move before they let it improve itself with RL.</p>
		<p>Nowadays, policy gradients are getting more popular. For example, A3C (see below) is based on it.</p>
		<h2 id="asynchronous-methods-for-deep-reinforcement-learning">Asynchronous Methods for Deep Reinforcement
			Learning</h2>
		<p>A3C (Asynchronous Actor Critic) and Async NStep Q learning are a WIP in rl4j. It bypass the need for an
			experience replay by using multiple agents exploring in parrallel the environment. The original <a
				href="#atari">paper</a> uses Hogwild!. In RL4J, a workaround is to use a central thread and accumulate
			gradient from “slave” agents. Having multiple agents exploring the environment enable to decorrelate the
			experience from the past episode and enable to gather more experience (instead of replaying multiple time
			the same transition). It is very efficient and the authors were able to train efficiently on a single
			machine!</p>
		<h2 id="deep-exploration">Deep exploration</h2>
		<p>Deep exploration was the subject of a semester project during my master. I wrote the Scala library <a
				href="https://github.com/rubenfiszel/scala-drl">scala-drl</a> about it. Deep exploration is defined as
			the multi-step ahead planning of the exploration.</p>
		<p>In <a href="#bootstrapped">Bootstrapped DQN</a>, the term bootstrapping comes from <a
				href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">statistics</a>. Multiple neural network
			are constructured in parralel. At each epoch, one neural network explore the environment. Then the
			transition are randomly redistributed to each model. This has a similar effect than resampling. Having had
			differences experiences to learn from, each model has its own opinion about the the best move and its own
			uncertainty about the environment. This encourage deep exploration.</p>
		<p>Another way is to use <a href="#autoencoder">Autoencoder</a> to quantify the uncetainty about a novel state
			and attribute an exploration bonus based on the reconstruction ability.</p>
		<h2 id="other-interesting-papers-to-discover-by-yourself.">Other interesting papers to discover by yourself.
		</h2>
		<h2 id="deterministic-policy-gradient">Deterministic Policy Gradient</h2>
		<p><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf"
				class="uri">http://jmlr.org/proceedings/papers/v32/silver14.pdf</a></p>
		<h2 id="trusted-region-policy-optimisation">Trusted Region Policy Optimisation</h2>
		<p><a href="https://arxiv.org/abs/1502.05477" class="uri">https://arxiv.org/abs/1502.05477</a></p>
		<h2 id="dueling-network-architectures-for-deep-reinforcement-learning">Dueling Network Architectures for Deep
			Reinforcement Learning</h2>
		<p><a href="http://arxiv.org/abs/1511.06581" class="uri">http://arxiv.org/abs/1511.06581</a></p>
		<h1 id="references">References</h1>
		<p><a name="karpathy"></a> <a href="http://karpathy.github.io/2016/05/31/rl/">Karpathy’s post about Policy
				Gradient</a></p>
		<p><a name="atari"></a> <a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement
				Learning</a></p>
		<p><a name="doubleq"></a> <a href="http://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double
				Q-learning</a></p>
		<p><a name="a3c"></a> <a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement
				Learning</a></p>
		<p><a name="prio"></a> <a href="http://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p>
		<p><a name="continuous"></a> <a href="http://arxiv.org/abs/1509.02971">Continuous control with deep
				reinforcement learning</a></p>
		<p><a name="giraffe"></a> <a href="https://arxiv.org/abs/1509.01549">Giraffe: Using Deep Reinforcement Learning
				to Play Chess</a></p>
		<p><a name="bootstrapped"></a> <a href="http://arxiv.org/abs/1602.04621">Deep Exploration via Bootstrapped
				DQN</a></p>
		<p><a name="autoencoder"></a> <a href="http://arxiv.org/abs/1507.00814">Incentivizing Exploration In
				Reinforcement Learning With Deep Predictive Models</a></p>

	</div>

	<hr>

	<div id="footer">
		<section class="social">
			<a href="https://ch.linkedin.com/in/rubenfiszel" target="_blank"><i class="fa fa-linkedin fa-2x"></i></a>
			<a href="https://github.com/rubenfiszel" target="_blank"><i class="fa fa-github fa-2x"></i></a>
			<a href="mailto:ruben@rubenfiszel.com"><i class="fa fa-envelope-o fa-2x"></i></a>
		</section>
	</div>


	<script>
		(function (i, s, o, g, r, a, m) {
			i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
				(i[r].q = i[r].q || []).push(arguments)
			}, i[r].l = 1 * new Date(); a = s.createElement(o),
				m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
		})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

		ga('create', 'UA-3040887-4', 'auto');
		ga('send', 'pageview');

	</script>

</body>

</html>
