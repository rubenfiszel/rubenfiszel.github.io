<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Reinforcement Learning and DQN, learning to play from pixels - Ruben Fiszel's website</title>
    <link rel="icon" type="image/png" href="../../images/lambda-xl.png">
      <link rel="stylesheet" type="text/css" href="../../css/syntax.css" />
      <link rel="stylesheet" type="text/css" href="../../sass/main.css" />
      <link type="text/css" href="../../css/font-awesome.min.css" rel="stylesheet">
	<link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
       <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
       <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
      </head>
      <body>
	<div id="header">
          <div id="logo">
	    <img src="../../images/lambda.png" alt="Lambda">
	      <a href="../../">Ruben Fiszel's website</a>
            </div>

            <div id="navigation">
              <a href="../../">Home</a>
              <a href="../../about.html">About</a>
              <a href="../../contact.html">Contact</a>
            </div>
	    <div class="clear"></div>
          </div>

          <div id="content">
            <h1> Reinforcement Learning and DQN, learning to play from pixels</h1>

            <div class="info">
    Posted on August 24, 2016
    
        by Ruben Fiszel
    
</div>
<div class="post">
  <h1 id="introduction">Introduction</h1>
<p>My 2 month summer internship at Skymind (the company behind the open source deeplearning library <a href="http://deeplearning4j.org">DL4J</a>) comes to an end and this is a post to summarize what I have been working on: Building a deep reinforcement learning library for DL4J: … (drums roll) … RL4J! This post begins by an introduction to reinforcement learning and is then followed by a detailed explanation of DQN (Deep Q-Network) for pixel inputs and is concluded by an RL4J example. I will assume from the reader some familiarity with neural networks. But first, lets talk about the core concepts of reinforcement learning.</p>
<div class="figure">
<img src="cartpole.gif" alt="Cartpole" />
<p class="caption">Cartpole</p>
</div>
<video autoplay loop>
<source src="doom.webm" type="video/webm">
</video>
<h1 id="preliminaries">Preliminaries</h1>
<blockquote>
<p>A “simple aspect of science” may be defined as one which, through good fortune, I happen to understand. (Isaac Asimov)</p>
</blockquote>
<p>Reinforcement Learning is an exciting area of machine learning. It is basically the learning of an efficient strategy in a given environment. Informally, this is very similar to Pavlovian conditioning: you assign a reward for a given behavior and over time, the agents learn to reproduce that behavior in order to receive more rewards. It is an iterative trial and error process.</p>
<h2 id="markov-decision-process">Markov Decision Process</h2>
<p>Formally, an environment is defined as a Markov Decision Process (MDP). Behind this scary name is nothing else than the combination of (5-tuple):</p>
<ul>
<li>A set of states <span class="math inline">\(S\)</span> (eg: in chess, a state is the board configuration)</li>
<li>A set of possible actions <span class="math inline">\(A\)</span> (In chess, all the move that could be possible in every configuration possible, eg: e4-e5)</li>
<li>The conditional distribution <span class="math inline">\(P(s'| s, a)\)</span> of next states given a current state and an action. (In a deterministic environment like chess, transitioning from state <span class="math inline">\(s\)</span> with action <span class="math inline">\(a\)</span>, there is only one state s’ with probability 1, and all the others have probability 0. Nevertheless, in a stochastic (involving randomness, eg: a coin toss) environment, the distribution is not as simple.)</li>
<li>The reward function of transitionning from state s to s’: <span class="math inline">\(R(s, s')\)</span> (eg: In chess, +1 for a final move that leads to a victory, -1 for a final move that leads to a defeat, 0 otherwise. In Cartpole, +1 for each step.).</li>
<li>The discount factor: <span class="math inline">\(\gamma\)</span>. This is the preference for present rewards compared to future rewards. (A concept very common in <a href="https://en.wikipedia.org/wiki/Discounted_utility">finance</a>.)</li>
</ul>
<p>Note: It is usually more convenient to use the set of Action <span class="math inline">\(A_s\)</span> which is the set of available move from a given state, than the complete set A. <span class="math inline">\(A_s\)</span> is simply the elements <span class="math inline">\(a\)</span> in <span class="math inline">\(A\)</span> such that <span class="math inline">\(P(s' | s, a) &gt; 0\)</span>.</p>
<p>The markov property is to be memoryless. Once you reach a state, the past history (the states visited before) should not affect the next transitions and rewards. Only the present state matters.</p>
<div class="figure">
<img src="mdp.png" alt="Schema of a MDP" />
<p class="caption">Schema of a MDP</p>
</div>
<h2 id="some-terminologies">Some terminologies</h2>
<p><strong>Final/terminal states</strong>: The states that have no available actions are final/terminal states.</p>
<p><strong>Episode</strong>: An episode is a complete play from one of the initial state to a final state. <span class="math display">\[s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_n\]</span></p>
<p><strong>Cumulative reward</strong>: The cumulative reward is the discounted sum of reward accumulated throughout an episode: <span class="math display">\[R=\sum_{t=0}^n \gamma^t r_{t+1}\]</span></p>
<p><strong>Policy</strong>: A Policy is the agent’s strategy to choose an action at each state. It is noted by <span class="math inline">\(\pi\)</span>.</p>
<p><strong>Optimal policy</strong>: The optimal policy is the theoretical policy that maximizes the expectation of cumulative reward. From the definition of expectation and the law of large numbers, this policy has the highest average cumulative rewards given sufficient episode. This policy might be intractable.</p>
<p><strong>The objective of reinforcement learning is to train an agent such that his policy converge to the theoretical optimal policy</strong>.</p>
<h1 id="different-settings">Different settings</h1>
<blockquote>
<p>« Qui peut le plus peut le moins » (He who can do the greather things, can do the lesser things)</p>
</blockquote>
<h2 id="model-free">Model-free</h2>
<p>The conditional distribution and the reward function constitute the model of the environment. In a game of backgammon, we know the model (each possible transition is decided by the known dice distribution, and we can predict each reward from transition without realizing them (because we can calculate the new value of the board)). The algorithm <a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node108.html">TD-gammon</a> use that fact to learn the V-function (see below).</p>
<p>Some reinforcement learning algorithms can work without being given the model. Nevertheless, in order to learn the best strategy, they additionaly have to learn the model during the training. This is called model-free reinforcement learning. Model-free algorithms are very important because a large majority of real world complex problems fall in that category. Futhermore, model free is simply an additional constraint. It is simply more powerful since it is a superset of model based reinforcement learning.</p>
<h2 id="observation-setting">Observation setting</h2>
<p>Instead of being given access to the state, you might being given access to a partial observation of the state only. It is the same idea behind Hidden Markov Chain. This is the difference between the partial and fully observed setting. For instance, our field of vision is a very partial observation of the full state of the universe (the position and energy of every particule in the universe). Fortunately, the partial observation setting can be reduced to a fully observed setting with the use an history (the state becomes an accumulation of previous states).</p>
<p>Nevertheless, it is most common to not accumulate the whole history. Either only the last h observations are stacked (in a windowed fashion) or you can use a Recurrent Neural Network (RNN) to learn what to keep in memory and what to forget (that is essentially how a <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a> works).</p>
<p>Abusing the language slighty for consistency purposes with the existing notation, history (even truncated ones) will also be called “states” and also symbolized <span class="math inline">\(S_t\)</span></p>
<h2 id="single-player-and-adversarial-games">Single player and adversarial games</h2>
<p>A single player game has a natural translation into a MDP. The states represent the moment where the player is in control. The observations from those states are all the informations accumulated between states (eg: as many pixel frame as there is in-between frames of control). An action is all the available command at the disposal of the player (In doom, go up, right, left, shoot, etc …).</p>
<p>Reinforcement learning can also be applied to adversarial games by self-play: The agent plays against itself. Often in this setting, there exists a <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a> such that it is always in your interest to play as if your opponent was a perfect player. This makes sense in chess by example. If given a board configuration, a good move against a chess master, would still be a good move against a beginner <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Whatever is the current level of the agent, by playing against himself, the agent stills get information about the quality of his previous moves (seen as good moves if he won, bad moves if he lost).</p>
<p>Of course the information, which is a gradient in the context of a neural network, is of «higher quality» if he played directly against a very good agent from the start. But it is really mind-blowing that an agent can learn to increase his level of play by playing against himself, an agent of the same level. That is actually the method of training employed by <a href="https://deepmind.com/alpha-go">AlphaGo</a> (the Go agent from DeepMind that beat the World Champion). The policy was bootstrapped (initially trained) on a dataset of master moves, then it used reinforcement learning and self play to increase furthermore the level (quantified with elo). In the end, the agent got better than policy it was learning from the original dataset. After all, it beat the master above all the masters. To compute the final policy, they used their policy gradient in combination with a Monte-Carlo Search Tree on a massive amount of computation power.</p>
<p>This setting is a bit different than learning from pixels. Firstly, because the input is not as high-dimensional. The <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">manifold</a> is a lot closer to its embedding space. Nevertheless, a convolutional layer was still used in this case to use efficiently the locality of some subgrid board patterns. Secondly, because AlphaGo is not model-free (it is deterministic). In the following of this post, I will talk exclusively about the model-free 1-player setting.</p>
<h1 id="q-learning">Q-learning</h1>
<blockquote>
<p>I am no friend of probability theory, I have hated it from the first moment when our dear friend Max Born gave it birth. For it could be seen how easy and simple it made everything, in principle, everything ironed and the true problems concealed. (Erwin Schrödinger)</p>
</blockquote>
<h2 id="from-policy-to-neural-network">From policy to neural network</h2>
<p>Our goal is learn the optimal policy <span class="math inline">\(\pi^*\)</span> that maximize: <span class="math display">\[E[R_0]=E[\sum_{t=0}^n \gamma^t r_{t+1}]\]</span> Let’s introduce an auxilliary function: <span class="math display">\[V_\pi(s) = E \{ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots + \gamma^n r_{n} \mid s_t = s, \text{policy followed at each state is }\pi \}\]</span> which is the expected cumulative reward from a state <span class="math inline">\(s\)</span> following the policy <span class="math inline">\(\pi\)</span>. Let suppose an oracle <span class="math display">\[V_{\pi^*}(s)\]</span> The V function of the optimal policy. From it, we could retrieve the optimal policy by defining the policy that among all available actions at the current state, choose the action that maximize the expectation of <span class="math inline">\(V_{\pi^*}(s)\)</span>. This is a greedy behavior. The optimal policy is the greedy policy w.r.t to <span class="math inline">\(V_{\pi^*}\)</span>. <span class="math display">\[ \pi^*(s) ~\text{chooses a s.t}~a= arg \max_a[E_\pi(r_t + \gamma V(s_{t+1}) \mid s_t=s, a_t=a)] \]</span> If you were very attentive, something would sound wrong here. In the model-free setting, we cannot predict the after-state <span class="math inline">\(s_{t+1}\)</span> from <span class="math inline">\(s_{t}\)</span> because we ignore the transition model. Even with that oracle, our model is still not computable!</p>
<p>To solve this very annoying issue, we are gonna use another auxiliarry function, the Q-function: <span class="math display">\[Q_{\pi^*}(s, a) = E_\pi[r_t + \gamma V_{\pi^*}(s_{t+1}) \mid s_t, a_t = a]\]</span> In a greedy setting, we have the relationship: <span class="math display">\[V_\pi(s_t) = \max_a Q_\pi(s_t, a)\]</span> Now, let suppose instead of the V oracle, we have the Q oracle. We can now redefine <span class="math inline">\(\pi^*\)</span>. <span class="math display">\[ \pi^*(s) ~ \text{chooses a s.t}~ a= \max_a[Q_{\pi^*}(s, a)] \]</span> <strong>No more uncomputable expectations, Neat</strong></p>
<p>Nevertheless, we have only moved the expectation from outside to inside the oracle. And unfortunately, oracles do not exist in the real world.</p>
<p>The trick here is that we have reduced an abstract notion that is a policy into a numerical function that might be relatively “smooth” (continuous) thanks to the expectation. Fortunately for us, there is one weapon at our disposal to approximate such complex functions: <strong>Neural networks</strong>.</p>
<p>Neural networks are universal function approximators. They can approximate any continuous differentiable function. Although they can get stuck in local extrema and many proofs of convergence from reinforcement learning are not valid anymore when throwing neural networks in the equation. This is because their learning is not as deterministic or boundable as their tabular counterparts. Nonetheless, in most case, with the right hyperparameters, they are <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">unreasonnably powerful</a>. Using deep learning with reinforcement learning is called deep reinforcement learning.</p>
<h2 id="policy-iteration">Policy iteration</h2>
<p>Now machine learning knowledge and common sense tells you that there is still something missing about our approeach. Neural networks can approximate functions that already have labels. Unfortunately for us oracles are not summonable, so we will have to get our labels another way. (:&lt;).</p>
<p>This is where the magic of Monte Carlo come in. Monte carlo methods are methods that rely on repeated random sampling to calculate an estimator. (A famous example is the <a href="http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopimod.html">pi calculation</a>).</p>
<p>If we play randomly from a given state, the better states should get better rewards on average (thank you <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a>). So without knowing anything about the environment, you can get some information about the expected value of a state. For instance, at poker, better hands will win more often on average than lesser hands even when every decision is taken randomly. The Monte Carlo Search Tree are also based on this property (shocking isn’t it?). This is a phase of exploration that lead to unsupervised learning and enable us to extract meaningful label.</p>
<p>More formally,</p>
<p>Given a policy <span class="math inline">\(\pi\)</span>, a state s and an action a, in order to get an approximation of <span class="math inline">\(Q(s, a)\)</span> we sample it according to its definition:</p>
<span class="math display">\[\begin{align*}
Q_\pi(s, a) &amp;= E[r_t + \gamma r_{t+1} + \ldots + \gamma^n  r_{n} \mid s_t = s, a_t = a]
\end{align*}\]</span>
<p>In plain english, we can get a label for <span class="math inline">\(Q_\pi(s, a)\)</span> by playing a sufficient number of time from s according to the policy <span class="math inline">\(\pi\)</span>.</p>
<p>From an aggregate of signals:</p>
<div class="figure">
<img src="qtarget.png" alt="One signal" />
<p class="caption">One signal</p>
</div>
<p>The actual learning is done by standard Gradient descent, using the labels in batches. The gradient is the standard Mean-Square Error one such that the td-error gets minimised at each iteration.</p>
<p>We use the Mean-Square Error loss function (l2 loss) with a learning rate of <span class="math inline">\(\alpha\)</span> and apply Stochastic Gradient Descent (On a batch of size 1) of: <span class="math display">\[Q_\pi(s_t, a_t) \leftarrow Q_\pi(s_t, a_t) + \alpha [R_t-Q_\pi(s_t, a_t)]\]</span></p>
<p><span class="math inline">\((s_t, a_t)\)</span> is the input, <span class="math inline">\(Q_\pi(s_t, a_t) + \alpha [R_t-Q_\pi(s_t, a_t)]\)</span> is the label aka target.</p>
<p>Note: Even if we use MSE, there is no square in the formula because the loss is applied afterwards on the difference of the expected output <span class="math inline">\(Q_\pi(s_t, a_t)\)</span> and the label <span class="math inline">\(\alpha [R_t-Q_\pi(s_t, a_t)]\)</span>.</p>
<p>Repeat many times: Sampling from <span class="math inline">\(\pi\)</span> <span class="math display">\[Q_\pi(s_t, a_t)    \leftarrow E_\pi[R_t] = E_{s_t, a_t, ..., s_n \sim \pi}[\sum_{i=t}^n \gamma^{i-t}r_i]\]</span> We can converge to the rightful expectation</p>
<div class="figure">
<img src="qexpect.png" alt="Many signals" />
<p class="caption">Many signals</p>
</div>
<p>So we can now design a naive prototype of our learning algorithm (in Scala but it is intelligible without any Scala knowledge):</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">
<span class="co">//A randomly uninitialized neural network</span>
<span class="kw">val</span> neuralNet: NeuralNet

<span class="co">//Iterate until you reach max epoch</span>
<span class="kw">for</span> (t &lt;- (<span class="dv">1</span> to MaxEpoch))
	<span class="fu">epoch</span>()


<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//pick a random state and action</span>
	<span class="kw">val</span> state = randomState
	<span class="kw">val</span> action = <span class="fu">randomAction</span>(state)

	<span class="co">//transition to a new state, initalize thethe reward</span>
	<span class="kw">var</span> (new_state, accuReward) = <span class="fu">transitition</span>(state, action)

	<span class="co">//play until terminal state and accumulate the reward</span>
	accuReward += <span class="fu">playRandomly</span>(state)

	<span class="co">//Do SGD over input and label!</span>
	<span class="fu">fit</span>((state, action), accuReward)
}



<span class="co">// MDP specific, return the new state and the reward</span>
<span class="kw">def</span> <span class="fu">transition</span>(state: State, action: Action): (State, Double)

<span class="co">//return a randomly sampled state among all the state space</span>
<span class="kw">def</span> randomState: State

<span class="co">//play until terminal state</span>
<span class="kw">def</span> <span class="fu">playRandomly</span>(state): Double = {
	<span class="kw">var</span> s = state
	<span class="kw">var</span> accuReward = <span class="dv">0</span>
	<span class="kw">var</span> k = <span class="dv">0</span>
	<span class="kw">while</span> (!s.<span class="fu">isTerminal</span>) {
		<span class="kw">val</span> action = <span class="fu">randomAction</span>(s)
		<span class="kw">val</span> (state, reward) = <span class="fu">transition</span>(s, action)
		accuReward += Math.<span class="fu">pow</span>(gamma, k) * reward
		k += <span class="dv">1</span>
		s = state
	}
	accuReward
}


<span class="co">//choose a random action among all the available actions at this state</span>
<span class="kw">def</span> <span class="fu">randomAction</span>(state: State): Action =
	<span class="fu">oneOf</span>(state.<span class="fu">available_action</span>)

<span class="co">//helper function, pick one among</span>
<span class="kw">def</span> <span class="fu">oneOf</span>(seq: Seq[Action]): Action =
	seq.<span class="fu">get</span>(Random.<span class="fu">nextInt</span>(seq.<span class="fu">size</span>))


<span class="co">//How it would be roughly done with DL4J</span>
<span class="kw">def</span> <span class="fu">fit</span>(input: (State, Action), label: Double) =
	neuralNet.<span class="fu">fit</span>(<span class="fu">toTensor</span>(input), <span class="fu">toTensor</span>(label))

<span class="co">//return an INDArray from ND4J</span>
<span class="kw">def</span> <span class="fu">toTensor</span>(array: Array[_]): Tensor =
	Nd4j.<span class="fu">create</span>(array)</code></pre></div>
<p>There is multiple issues: This should work but this is terribly inefficient. We are playing a full game with n state and n actions for a single label and that label might not be very meaningful (If the interesting trajectories are hard to reach at random).</p>
<h2 id="the-explorationexploitation-dilemma">The Exploration/Exploitation dilemma</h2>
<p>Exploring at random the environment will converge to the optimal policy … but only guaranteed after an almost infinite time: you will have to visit every possible trajectories (a trajectory is the ordered list of al the states visited and actions choosen during an episode) at least once. Considering how many states and branching there is, it is impossible. The branching issue is the reason why Go is so hard but chess is ok. In the real world, we do not have infinite time (and time is money).</p>
<p>Thus, we should exploit the past informations and our learning of them to focus our exploration on the most promising possible trajectories. This can be achieved through different ways, and one of them is <span class="math inline">\(\epsilon\)</span>-greedy exploration. <span class="math inline">\(\epsilon\)</span>-greedy exploration is fairly simple. It is a policy that choose an action at random with odd <span class="math inline">\(\epsilon\)</span> or the best action as deemed by our current policy with odd <span class="math inline">\((1-\epsilon)\)</span>. Usually <span class="math inline">\(\epsilon\)</span> is annealed over time to privilege exploitation over exploration after enough exploration. This is a trade-off between exploration and exploitation.</p>
<p>At each new information, our actual Q functions gets more accurate about the present policy and the exploration is focused on better paths. The policy based on our new Q function gets better (since Q is more accurate) and the <span class="math inline">\(\epsilon\)</span>-greedy exploration reach better paths. Focused on those better paths, our q function explore even more the better parts and has to update its returns according to the new policy. This is an iterative cycle that enable convergence to the optimal policy called policy iteration. Unfortunately, the convergence can takes infinite time and is not even guaranteed when Q is approximated by neural networks. Nevertheless, impressive results can make up for the lack of formal convergence proofs.</p>
<div class="figure">
<img src="policyiter.png" alt="Policy Iteration" />
<p class="caption">Policy Iteration</p>
</div>
<p>This algorithm also requires you to be able to sample the states in a «good» manner: It should be proportionally representative of the states that are usally present in a game (or at least the kind of game at the targeted agent’s level) . On a sidenote, this is possible in some case, see <a href="#giraffe">Giraffe</a> that uses TD-Lambda.</p>
<p>Fortunately, some rearranging and optimisations are possible:</p>
<h2 id="bellman-equation">Bellman equation</h2>
We can transform the Q equation into a <strong>Bellman equation</strong>:
<span class="math display">\[\begin{align*}
Q_\pi(s, a) &amp;= E[r_t + \gamma r_{t+1} + \ldots + \gamma^n  r_{n} \mid s_t = s, a_t = a] \\
&amp;= E[r_t + \gamma r_{t+1} + V(s_{t+1}) \mid s_t = s, a_t = a] \\
&amp;= E[r_t + \gamma r_{t+1} + \ldots + \gamma \max_{a'} Q(s_{t+1}, a')\} \mid s_t = s, a_t = a] \\
\end{align*}\]</span>
<p>As in the Monte-Carlo method, we can do many updates of Q.</p>
<p>MSE: <span class="math display">\[Q_\pi(s_t, a_t) \leftarrow Q_\pi(s_t, a_t) + \alpha [(\underbrace{\underbrace{r_t+\max_a Q_\pi(s_{t+1}, a)}_{\text{target}}-Q_\pi(s_t, a_t)}_{\text{TD-error}})]\]</span></p>
<p>TD-error is the “Temporal difference error”. Indeed we are actually calculating the difference between what the Q approximation expects in the future plus the realized reward and its its present value as evaluated by the neural net.</p>
<p>That bellman equation only make sense with some boundary conditions. if s is terminal: <span class="math display">\[V(s) = 0\]</span> and for any a <span class="math display">\[Q(s_{t-1}, a) = r_t\]</span></p>
<p>The states near the terminal states are the first to converge because they are closer in the chain to the «true» label, the known boundary conditions. In Go or Chess, reinforcement learning is applied by assigning +1 to the transitions that lead to a final winning board (respectively -1 for a loosing board) and 0 otherwise. It diffuses the Q-values by finding a point between the two extremes [-1; 1]. A transition with Q value close to 0 represents a transition leading to a balanced board. A transition with Q value close to 1 represents a near certain victory.</p>
<p>It could be surprising that the moves do not have only -1 and 1 values (since deviating from the optimal path should be fatal). One interesting aspect of calculating Q-values is the realization that in many games/MDP, no mistake in itself is ever really fatal. It is the accumulation of them that really kill you. AI is full of life lessons ;). Moreover, The expected accumulated reward space is a lot smoother than often thought. One possible explanation is that expectations always have an average effect: an expectation is nothing else than a weighted average with probabilities as weights. Furthermore, gamma being <span class="math inline">\(&lt; 1\)</span> the very long-term effects are not too preponderant. Isn’t it exciting to be able to calculate directly the odd of winning a game for every transition ?</p>
<p>As long as we sample sufficiently enough transitions near the terminal states, Q-learning is able to converge. The incredible power of deep reinforcement learning is that it will be able to generalize its learning from visited states to unvisited states. It should be able to understand what is a balanced or winning board even if it has never seen it before. This is because the network should be able to abstract patterns and understand the strength of an action based on previously seen pattern (eg: shoot an enemy when recognizing its form).</p>
<h2 id="offline-and-online-reinforcement-learning">Offline and online reinforcement learning</h2>
<p>To learn more about the differences between online and offline reinforcement learning, see this excellent post from <a href="https://kofzor.github.io/Reinforcement_Learning_101/#comparing-reinforcement-learning-algorithms">kofzor</a>.</p>
<h2 id="initial-state-sampling">Initial state sampling</h2>
<p>In a 1 player setting (like the atari game): We do not actually need to learn to play well in every situation (Although, if we did, that would show that we would have reached a very good level of generalization). We only need to learn to play efficiently from the states that our policy encounters. Thus, we can sample from states that are simply reachable by playing with our current policy from an initial state. This enables to sample directly from a played episode by our agent.</p>
<h2 id="q-learning-implementation">Q-Learning implementation</h2>
<p>So we can now design a naive prototype of our Q-Learning:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">

<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//sample among the initial state space</span>
	<span class="co">//(often unique state)</span>
	<span class="kw">var</span> state = initState

	<span class="co">//while the state is not terminal,</span>
	<span class="co">//play an episode and do a Q-update at each transition</span>
	<span class="kw">while</span>(!state.<span class="fu">isTerminal</span>)  {

		<span class="co">//sample action from eps-greddy policy</span>
		<span class="kw">val</span> action = <span class="fu">epsilonGreedyAction</span>(state)

		<span class="co">//interaction with the environment</span>
   		<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)

		<span class="co">//Q-update</span>
		<span class="fu">update</span>(state, action, reward, nextState)

		state = nextState
	}
}

<span class="co">//Our Q-update as explained above</span>
<span class="kw">def</span> <span class="fu">update</span>(state: State, action: Action, reward: Double, nextState: State) = {
	<span class="kw">val</span> target = reward + <span class="fu">maxQ</span>(nextState)
	<span class="fu">fit</span>((state, action), target)
}


<span class="co">//the eps-greedy policy implementation</span>
<span class="kw">def</span> <span class="fu">epsilonGreedyAction</span>(state: State) = {
	<span class="kw">if</span> (Random.<span class="fu">float</span>() &lt; epsilon)
		<span class="fu">randomAction</span>(state)
	<span class="kw">else</span>
		<span class="fu">maxQAction</span>(state)
}


<span class="co">//Retrive max Q value</span>
<span class="kw">def</span> <span class="fu">maxQ</span>(state: State) =
	<span class="fu">actionsWithQ</span>(state).<span class="fu">maxBy</span>(_.<span class="fu">_2</span>).<span class="fu">_2</span>

<span class="co">//Retrive action of the max Q value</span>
<span class="kw">def</span> <span class="fu">maxQAction</span>(state: State) =
	<span class="fu">actionsWithQ</span>(state).<span class="fu">maxBy</span>(_.<span class="fu">_2</span>).<span class="fu">_1</span>

<span class="co">//return a list of actions and the q-value of their transition from the state</span>
<span class="kw">def</span> <span class="fu">actionsWithQ</span>(state: State) = {
	<span class="kw">val</span> stateActionList = available_actions.<span class="fu">map</span>(action =&gt; (state, action))
   	available_actions.<span class="fu">zip</span>(neural_net.<span class="fu">output</span>(<span class="fu">toTensor</span>(state_action_list)))

<span class="kw">def</span> initState: State

</code></pre></div>
<h2 id="modeling-qs-a">Modeling Q(s, a)</h2>
<p>Instead of having <span class="math inline">\(a\)</span> as an additional input of the neural net combined with the state, the state is the only input and the output contains the Q value of every action possible. This makes sense only when the availables actions are consistent accross the full episode (else the neural output layer would have to be different at each state). It is many times solvable by having the full set <span class="math inline">\(A\)</span> of actions as output and ignore the impossible actions (some papers put the target of impossible actions at 0).</p>
<div class="figure">
<img src="qmodeling.png" alt="Q modeling Source" />
<p class="caption">Q modeling <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Source</a></p>
</div>
<h2 id="experience-replay">Experience replay</h2>
<p>There is one issue with using neural network as Q approximator. The transitions are very correlated. This reduce the overall variance of the transition. After all, they are all extracted from the same episode. Imagine if you had to learn a task without any memory (not even short-term), you would always optimise your learning based on the last episode.</p>
<p>The Google DeepMind research team used experience replay, which is a windowed buffer of the last N transitions (N being a million in the original paper) with DQN and greatly improved their performances on atari. Instead of updating from the last transition, you store it inside the experience replay and update from a batch of randomly sampled transitions from the same experience replay.</p>
<p>epoch() becomes:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">
<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//sample among the initial state space</span>
	<span class="co">//(often unique state)</span>
	<span class="kw">var</span> state = initState

	<span class="co">//while the state is not terminal,</span>
	<span class="co">//play an episode and do a Q-update at each transition</span>
	<span class="kw">while</span>(!state.<span class="fu">isTerminal</span>)  {

		<span class="co">//sample action from eps-greddy policy</span>
		<span class="kw">val</span> action = <span class="fu">epsilonGreedyAction</span>(state)

		<span class="co">//interaction with the environment</span>
   		<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)

		<span class="co">//store transition (Exp Replay is just a Ring buffer)</span>
		expReplay.<span class="fu">store</span>(state, action, reward, nextState)

		<span class="co">//Q update in batch</span>
		<span class="fu">updateFromBatch</span>(expReplay.<span class="fu">getBatch</span>())

		state = nextState
	}
}</code></pre></div>
<h3 id="compression">Compression</h3>
<p>nd4j, the tensor library of dl4j, does not support as first class type uint8. However, pixels in grayscaling are encoded with that precision. To avoid wasting too much space on memory, INDArray were compressed to uint8.</p>
<h2 id="convolutional-layers-and-image-preprocessing">Convolutional layers and image preprocessing</h2>
<h3 id="convolutional-layers">Convolutional layers</h3>
<div class="figure">
<img src="conv.png" alt="Convolutional layer Source" />
<p class="caption">Convolutional layer <a href="#atari">Source</a></p>
</div>
<p>Convolutional layers are layers that are excellent to detect local patterns in images. For pixels it is used as a processor that is required to reduce the dimension of the input into its real manifold. Given the proper manifold of observations, the decision becomes much easier.</p>
<h3 id="image-processing">Image processing</h3>
<p>You could feed the neural network with the RGB directly, but then the network would have to also learn that additional pattern. It seems like the brain is hard-wired to combine colors (fortunately!). Thus, it would seem reasonnable to tolerate that preprocessing.</p>
<p>What you see: <video autoplay loop> <source src="doom-pre.webm" type="video/webm"> </video></p>
<p>What the neural net see:</p>
<div class="figure">
<img src="doom-post.gif" alt="Neural net input" />
<p class="caption">Neural net input</p>
</div>
<h3 id="resizing">resizing</h3>
<p>The image is resized into 84x84. Convolutional layers needs for memory and computations grow with the size of their input. The fine details of the image are not required to play the game correctly. Indeed, many are purely aesthetic. Resizing to a more reasonnable size speed up the training.</p>
<h3 id="skip-frame">Skip frame</h3>
<p>In the original <a href="#atari">atari</a> paper, only 1 in 4 frame is actually processed. For the following 3 images, the last action is repeated. It speeds up roughly by 4 time the training without loosing much information. Indeed, atari game are not supposed to be played frame perfect and for most action it makes more sense to keep them for at least 4 frames.</p>
<h3 id="history-processing">History Processing</h3>
<p>To give information to the Neural network about the current momentum, the last 4 frame (with skip frame, you pick 1 every 4) are stacked into 4 channels. Those 4 frames represent an history as previously discussed in the Observation setting section.</p>
<div class="figure">
<img src="slowed.gif" alt="Slowed input" />
<p class="caption">Slowed input</p>
</div>
<div class="figure">
<img src="stacked.png" alt="Stacking" />
<p class="caption">Stacking</p>
</div>
<p>To fill the first frames of the history, a random policy or a noop replay is used. (Sidenote, <a href="#doubleq">random starts</a> can be used for fair evaluation)</p>
<h2 id="double-q-learning">Double Q-learning</h2>
<p>The idea behind <a href="#doubleq">double DQN</a> is that the network is frozen every M update (hard update) or smoothly averaged (<code>target = target * (smooth) + current * (1-smooth)</code>) every update (soft update). Indeed, it adds stability to the learning by using a Q evaluation to use in the td-error formula that is less prone to “jiggering”. The Q update becomes:</p>
<p><span class="math display">\[Y_\text{target} = r_t + \gamma*(Q_\text{target}(s_t+1, arg \max_a Q_\text(s_t+1, a))) \]</span></p>
<h2 id="clipping">Clipping</h2>
<p>The TD-error can be clipped (bounded between two limit values) such that no outlier update can have too much impact on the learning.</p>
<h2 id="scaling-rewards">Scaling rewards</h2>
<p>Scaling the rewards such that Q-values are lower (in a range of [-1; 1] is similar to normalization). It can dramatically alter the efficiency of the learning. This is an important hyperparameter to not neglect.</p>
<h2 id="prioritized-replay">Prioritized replay</h2>
<p>The idea behind <a href="#prio">prioritized replay</a> is that not all transitions are born equal. Some are more important than others. One way to sort them is through their TD-error. Indeed, a high TD-error is correlated to a high level of information (in the sense of surprise). Those transitions should be sampled more often than the others.</p>
<h2 id="graph-visualisation-and-mean-q">Graph, Visualisation and Mean-Q</h2>
<p>To visualize and debug the training or a method of RL, it is useful to have a visual monitoring of the agent’s progress. This is why I built the dashboard <a href="https://github.com/rubenfiszel/webapp-rl4j">webapp-rl4j</a>.</p>
<div class="figure">
<img src="screen.png" alt="webapp-rl4j" />
<p class="caption">webapp-rl4j</p>
</div>
<p>The most important is to keep track of cumulative rewards. This is a way to check that the agents effectively gets better. It is important to notice that it represents the epsilon greedy strategy and not the directly derived policy from the Q approximation.</p>
<div class="figure">
<img src="rewardgraph.png" alt="Cumulative reward graph" />
<p class="caption">Cumulative reward graph</p>
</div>
<p>But you might want to also track the loss (score of the neural network) and mean Q-values:</p>
<div class="figure">
<img src="scoregraph.png" alt="Score and mean-Q graph" />
<p class="caption">Score and mean-Q graph</p>
</div>
<p>Unlike with classic supervised learning, the loss does not necessarily always decrease because the learning impact the labels!</p>
<p>If used with target network, you should see some discontinuities from the non continuous evaluation of different target networks. Loss should decrease w.r.t to a single target network. The mean Q-values should smoothly converge towards a value proportionnal to the mean expected reward.</p>
<h2 id="rl4j">RL4J</h2>
<p>RL4J is available on <a href="https://github.com/deeplearning4j/rl4j">github</a>. Currently DQN with Experience Replay, Double Q-learning and clipping is implemented. Asynchronous Reinforcement Learning with A3C and Async N-step Q-Learning is included too. It is possible to play both from pixels or low-dimensional problems (like Cartpole). Async Reinforcement Learning is experimental. Hopefully, contributions will enrich the library.</p>
<p>Here is a working example with RL4J to play Cartpole with a simple DQN. You can play Doom too. Check <a href="https://github.com/rubenfiszel/rl4j-examples">rl4j-examples</a> for more examples. It is also possible to provide your own constructed neural network model as an argument to any training method.</p>
<div class="sourceCode"><pre class="sourceCode java"><code class="sourceCode java">
    <span class="kw">public</span> <span class="dt">static</span> QLearning.<span class="fu">QLConfiguration</span> CARTPOLE_QL =
            <span class="kw">new</span> QLearning.<span class="fu">QLConfiguration</span>(
                    <span class="dv">123</span>,    <span class="co">//Random seed</span>
                    <span class="dv">200</span>,    <span class="co">//Max step By epoch</span>
                    <span class="dv">150000</span>, <span class="co">//Max step</span>
                    <span class="dv">150000</span>, <span class="co">//Max size of experience replay</span>
                    <span class="dv">32</span>,     <span class="co">//size of batches</span>
                    <span class="dv">500</span>,    <span class="co">//target update (hard)</span>
                    <span class="dv">10</span>,     <span class="co">//num step noop warmup</span>
                    <span class="fl">0.01</span>,   <span class="co">//reward scaling</span>
                    <span class="fl">0.99</span>,   <span class="co">//gamma</span>
                    <span class="fl">1.0</span>,    <span class="co">//td-error clipping</span>
                    <span class="fl">0.</span>1f,   <span class="co">//min epsilon</span>
                    <span class="dv">1000</span>,   <span class="co">//num step for eps greedy anneal</span>
                    <span class="kw">true</span>    <span class="co">//double DQN</span>
            );

    <span class="kw">public</span> <span class="dt">static</span> DQNFactoryStdDense.<span class="fu">Configuration</span> CARTPOLE_NET =
            <span class="kw">new</span> DQNFactoryStdDense.<span class="fu">Configuration</span>(
                    <span class="dv">3</span>,         <span class="co">//number of layers</span>
                    <span class="dv">16</span>,        <span class="co">//number of hidden nodes</span>
                    <span class="fl">0.001</span>,     <span class="co">//learning rate</span>
                    <span class="fl">0.00</span>       <span class="co">//l2 regularization</span>
            );

    <span class="kw">public</span> <span class="dt">static</span> <span class="dt">void</span> <span class="fu">main</span>( String[] args )
    {

        <span class="co">//record the training data in rl4j-data in a new folder (save)</span>
        DataManager manager = <span class="kw">new</span> <span class="fu">DataManager</span>(<span class="kw">true</span>);

        <span class="co">//define the mdp from gym (name, render)</span>
        GymEnv&lt;Box, Integer, DiscreteSpace&gt; mdp = <span class="kw">new</span> <span class="fu">GymEnv</span>(<span class="st">&quot;CartPole-v0&quot;</span>, <span class="kw">false</span>, <span class="kw">false</span>);

        <span class="co">//define the training</span>
        QLearningDiscreteDense&lt;Box&gt; dql = <span class="kw">new</span> <span class="fu">QLearningDiscreteDense</span>(mdp, CARTPOLE_NET, CARTPOLE_QL, manager);

        <span class="co">//train</span>
        dql.<span class="fu">train</span>();

        <span class="co">//get the final policy</span>
        DQNPolicy&lt;Box&gt; pol = dql.<span class="fu">getPolicy</span>();

        <span class="co">//serialize and save (serialization showcase, but not required)</span>
        pol.<span class="fu">save</span>(<span class="st">&quot;/tmp/pol1&quot;</span>);

        <span class="co">//close the mdp (close http)</span>
        mdp.<span class="fu">close</span>();


    }</code></pre></div>
<h1 id="conclusion">Conclusion</h1>
<p>This was an exciting journey through deep reinforcement learning. From equations to code, Q-learning is a powerful, yet a somewhat simple algorithm. The field of RL is very active and promising. In fact, Supervised learning could be considered a subset of Reinforcement learning (by setting the labels as rewards). Maybe one day, Reinforcement Learning will be the panacea of AI. Until then, we can expect to be awed by its diverse applications into more and more mind-blowing problems. As a word of acknowledgement, I would like to thank Skymind and its amazing team for this very enriching internship.</p>
<h1 id="to-feed-your-appetite">To feed your appetite</h1>
<div class="figure">
<img src="../../images/bookworm.jpg" alt="Bookworm" />
<p class="caption">Bookworm</p>
</div>
<p>I hope that thanks to this introduction, you are excited about RL research. Here is a brief summary of important research in deep reinforcement learning.</p>
<h2 id="continuous-domain">Continuous domain</h2>
<p>When the action space is not discrete, you cannot use DQN. But many problems cannot be discretized. <a href="#continuous">Continuous control</a> is achieved through a normal distribution parametrized (mean and variance) by the output of the neural network. At each step, the action is sampled from the distribution. It also uses soft update of the target network.</p>
<h2 id="policy-gradient">Policy gradient</h2>
<p>Policy gradient works by directly learning the stochastic policy from cross entropy of the distribution scaled bythe advantage. This excellent post from <a href="#karpathy">Karpathy’s blog</a> has more details on the matter. Using a stochastic policy feels more natural as it encourages exploration and exploit more fairly the uncertainty we have between the different values of the move. Indeed a max operation can end by ignoring fully a branch that is <span class="math inline">\(\epsilon\)</span> below in Q-value of another. Although, This can be solved in DQN by using Boltzmann exploration.</p>
<p>Policy gradient were used by AlphaGo in combination with MonteCarlo Search Tree. The Neural Network was bootstrapped (pretrained) on a dataset of master move before they let it improve itself with RL.</p>
<p>Nowadays, policy gradients are getting more popular. For example, A3C (see below) is based on it.</p>
<h2 id="asynchronous-methods-for-deep-reinforcement-learning">Asynchronous Methods for Deep Reinforcement Learning</h2>
<p>A3C (Asynchronous Actor Critic) and Async NStep Q learning are included in RL4J. It bypass the need for an experience replay by using multiple agents exploring in parrallel the environment. The original <a href="#atari">paper</a> uses <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjaiqzll9bOAhUU3mMKHYl8Bm0QFggeMAA&amp;url=https%3A%2F%2Fwww.eecs.berkeley.edu%2F~brecht%2Fpapers%2FhogwildTR.pdf&amp;usg=AFQjCNE9XrK7aEEQxMC2XkWxXrfyC90y2A&amp;sig2=CeCuHurShRX9cL-MB4hk6A">Hogwild!</a>. In RL4J, a workaround is to use a central thread and accumulate gradient from “slave” agents. Having multiple agents exploring the environment enable to decorrelate the experience from the past episode and enable to gather more experience (instead of replaying multiple time the same transition). It is very efficient and the authors were able to train efficiently on a single machine!</p>
<p>For the curious, here is some Scala-pseudo-code of A3C:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">
<span class="co">//A randomly uninitialized neural network</span>
<span class="kw">val</span> globalNeuralNet: NeuralNet

<span class="co">//Iterate until you reach max epoch</span>
<span class="kw">for</span> (t &lt;- (<span class="dv">1</span> to numThread))
	<span class="fu">launchThread</span>()

<span class="kw">def</span> launchActor) =
  <span class="kw">new</span> Actor(globalNeuralNet).<span class="fu">run</span>()

<span class="kw">var</span> globalT: Int = <span class="dv">0</span>

<span class="kw">class</span> Actor(globalNeuralNet: NeuralNet) {

  <span class="kw">def</span> <span class="fu">run</span>() = {
    <span class="kw">while</span>(globalT &lt; maxStep) {

      <span class="kw">val</span> neuralNet = globalNeuralNet.<span class="fu">clone</span>()

      <span class="kw">var</span> state = initState
      <span class="kw">var</span> i = <span class="dv">0</span>
      <span class="kw">val</span> stack:Stack[(State, Action, Reward)] = <span class="kw">new</span> Stack()

      <span class="kw">while</span>(i &lt; tMax &amp;&amp; !state.<span class="fu">isTerminal</span>){
        globalT += <span class="dv">1</span>
        i += <span class="dv">1</span>

	<span class="co">//sample action from stochastic policy</span>
	<span class="kw">val</span> action = <span class="fu">stochasticPolicy</span>(neuralNet, state)

	<span class="co">//interaction with the environment</span>
   	<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)
        stack.<span class="fu">add</span>((state, action, reward))
        state = nextState

      }

      <span class="kw">var</span> r =
        <span class="kw">if</span> (state.<span class="fu">isTerminal</span>)
          <span class="dv">0</span>
        <span class="kw">else</span>
          <span class="fu">criticOutput</span>(neuralNet, state)

      <span class="kw">val</span> inputs = <span class="kw">new</span> <span class="fu">Tensor</span>()
      <span class="kw">val</span> actorLabels = <span class="kw">new</span> <span class="fu">Tensor</span>()
      <span class="kw">val</span> criticLabels = <span class="kw">new</span> <span class="fu">Tensor</span>()

      <span class="kw">while</span>(!stack.<span class="fu">isEmpty</span>) {
        <span class="kw">val</span> trans = stack.<span class="fu">dequeue</span>()
        r = trans.<span class="fu">reward</span> + gamma*r

        inputs.<span class="fu">appendRow</span>(trans.<span class="fu">state</span>)
        criticLabels.<span class="fu">appendRow</span>(r)

        <span class="kw">val</span> prevCriticOutput = <span class="fu">criticOutput</span>(neuralNet, trans.<span class="fu">state</span>)
        <span class="kw">val</span> advantage = r - prevCriticOutput
        <span class="kw">val</span> prevActorOutput = <span class="fu">actorOutput</span>(neuralNet, trans.<span class="fu">state</span>)

        actorLabel.<span class="fu">appendRow</span>(prevActorOutput.<span class="fu">addScalar</span>(trans.<span class="fu">action</span>, r))

      }

      <span class="fu">asyncUpdate</span>(globalNeuralNet, inputs, criticLabels, actorLabels)

    }
  }

  <span class="kw">def</span> <span class="fu">stochasticPolicy</span>(neuralNet: NeuralNet, state: State) = {
    <span class="kw">val</span> distribution = <span class="fu">actorOutput</span>(neuralNet, state)
    <span class="fu">chooseAccordingToDistribution</span>(state.<span class="fu">availableActions</span>, distribution)
  }


}</code></pre></div>
<h2 id="deep-exploration">Deep exploration</h2>
<p>Deep exploration was the subject of a semester project during my master. I wrote the Scala library <a href="https://github.com/rubenfiszel/scala-drl">scala-drl</a> about it. Deep exploration is defined as the multi-step ahead planning of the exploration.</p>
<p>In <a href="#bootstrapped">Bootstrapped DQN</a>, the term bootstrapping comes from <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">statistics</a>. Multiple neural network are constructured in parralel. At each epoch, one neural network explore the environment. Then the transition are randomly redistributed to each model. This has a similar effect than resampling. Having had differences experiences to learn from, each model has its own opinion about the the best move and its own uncertainty about the environment. This encourage deep exploration.</p>
<p>Another way is to use <a href="#autoencoder">Autoencoder</a> to quantify the uncertainty about a novel state and attribute an exploration bonus based on the reconstruction loss.</p>
<h2 id="other-interesting-papers-to-discover-by-yourself.">Other interesting papers to discover by yourself.</h2>
<h3 id="deterministic-policy-gradient">Deterministic Policy Gradient</h3>
<p><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf" class="uri">http://jmlr.org/proceedings/papers/v32/silver14.pdf</a></p>
<h3 id="trusted-region-policy-optimisation">Trusted Region Policy Optimisation</h3>
<p><a href="https://arxiv.org/abs/1502.05477" class="uri">https://arxiv.org/abs/1502.05477</a></p>
<h3 id="dueling-network-architectures-for-deep-reinforcement-learning">Dueling Network Architectures for Deep Reinforcement Learning</h3>
<p><a href="http://arxiv.org/abs/1511.06581" class="uri">http://arxiv.org/abs/1511.06581</a></p>
<h1 id="references">References</h1>
<p><a name="karpathy"></a> <a href="http://karpathy.github.io/2016/05/31/rl/">Karpathy’s post about Policy Gradient</a></p>
<p><a name="atari"></a> <a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></p>
<p><a name="doubleq"></a> <a href="http://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a></p>
<p><a name="a3c"></a> <a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a></p>
<p><a name="prio"></a> <a href="http://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p>
<p><a name="continuous"></a> <a href="http://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a></p>
<p><a name="giraffe"></a> <a href="https://arxiv.org/abs/1509.01549">Giraffe: Using Deep Reinforcement Learning to Play Chess</a></p>
<p><a name="bootstrapped"></a> <a href="http://arxiv.org/abs/1602.04621">Deep Exploration via Bootstrapped DQN</a></p>
<p><a name="autoencoder"></a> <a href="http://arxiv.org/abs/1507.00814">Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</a></p>
<p><a href="http://cs.stanford.edu/people/karpathy/reinforcejs/index.html">REINFORCEjs</a></p>
<p><a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/">Sutton holy bible on reinforcement learning</a></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>(Note: As my semester project advisor remarked, this is not always true. Imagine you play against a beginner at chess and you are badly losing, you might want to “bait” him to reverse the situation. A bait is only beneficial when the opponent has a low level of play. This proves that chess is not a real Nash equilibrium. This does not matter much because the goal is often to build a high level of play agent that plays against other high quality agents. At this level of play, you do not loose if given a significant advantage.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
</div>

          </div>

	  <hr>

            <div id="footer">
	      <section class="social">
		<a href="https://ch.linkedin.com/in/rubenfiszel" target="_blank"><i class="fa fa-linkedin fa-2x"></i></a>
		<a href="https://github.com/rubenfiszel" target="_blank"><i class="fa fa-github fa-2x"></i></a>
		<a href="mailto:ruben.fiszel@epfl.ch"><i class="fa fa-envelope fa-2x"></i></a>
		<a href="assets/RubenFiszel_resume.pdf"><i class="fa fa-file fa-2x"></i></a>
	      </section>
            </div>

	    <script>
	      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	      ga('create', 'UA-3040887-4', 'auto');
	      ga('send', 'pageview');

	    </script>


	  </body>
	</html>
