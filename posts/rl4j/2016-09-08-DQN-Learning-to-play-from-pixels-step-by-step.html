<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Ruben Fiszel's website - Reinforcement Learning and DQN, learning to play from pixels</title>
    <link rel="icon" type="image/png" href="../../images/lambda-xl.png">
      <link rel="stylesheet" type="text/css" href="../../css/syntax.css" />
      <link rel="stylesheet" type="text/css" href="../../sass/main.css" />
      <link type="text/css" href="../../css/font-awesome.min.css" rel="stylesheet">
	<link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
       <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
       <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
      </head>
      <body>
	<div id="header">
          <div id="logo">
	    <img src="../../images/lambda.png" alt="Lambda">
	      <a href="../../">Ruben Fiszel's website</a>
            </div>

            <div id="navigation">
              <a href="../../">Home</a>
              <a href="../../about.html">About</a>
              <a href="../../contact.html">Contact</a>
            </div>
	    <div class="clear"></div>
          </div>

          <div id="content">
            <h1> Reinforcement Learning and DQN, learning to play from pixels</h1>

            <div class="info">
    Posted on September  8, 2016
    
        by Ruben Fiszel
    
</div>

<h1 id="introduction">Introduction</h1>
<p>My 2 month summer internship comes to an end and this is a post to summarize what I have been working on: Building a deep reinforcement learning library for DL4J: … (drums build-up) … RL4J! This post begins by an introduction to reinforcement learning and is then followed by a detailed explanation of DQN for pixel inputs and is concluded by an RL4J example. But first, lets talk about the core concepts of reinforcement learning.</p>
<div class="figure">
<img src="cartpole.gif" alt="Cartpole" />
<p class="caption">Cartpole</p>
</div>
<video autoplay loop>
<source src="doom.webm" type="video/webm">
</video>
<h1 id="preliminaries">Preliminaries</h1>
<p>Reinforcement Learning is an exciting area of machine learning. It is basically the learning of an efficient strategy in a given environment. Informally, this is very similar to Pavlovian conditionning: you assign a reward for a given behavior and over time, the agents learn to reproduce that behavior in order to receive more rewards.</p>
<h2 id="markov-decision-process">Markov Decision Process</h2>
<p>Formally, an environment is defined as a Markov Decision Process (MDP). Behind this scary name is nothing else than the combination of (5-tuple):</p>
<ul>
<li>A set of states <span class="math inline">\(S\)</span> (eg: in chess, a state is the board configuration)</li>
<li>A set of possible actions <span class="math inline">\(A\)</span> (In chess, all the move that could be possible in every configuration possible, eg: e4-e5)</li>
<li>The conditional distribution <span class="math inline">\(P(s'| s, a)\)</span> of next states given a current state and an action. (In a deterministic environment like chess, there is only one state s’ with probability 1, and all the others have probability 0. Nevertheless, in a stochastic (involving randomness, eg: a coin toss) environment, the distribution is not as simple.)</li>
<li>The reward function of transitionning from state s to s’: <span class="math inline">\(R(s, s')\)</span> (eg: In chess, +1 for a final move that leads to a victory, -1 for a final move that leads to a defeat, 0 otherwise. In Cartpole, +1 for each step.).</li>
<li>The discount factor: <span class="math inline">\(\gamma\)</span>. This is the preference for present rewards compared to future rewards. (A concept very common in <a href="https://en.wikipedia.org/wiki/Discounted_utility">finance</a>.)</li>
</ul>
<p>Note: It is usually more convenient to use the set of Action <span class="math inline">\(A_s\)</span> which is the set of available move from a given state, than the complete set A. <span class="math inline">\(A_s\)</span> is simply the elements <span class="math inline">\(a\)</span> in <span class="math inline">\(A\)</span> such that <span class="math inline">\(P(s' | s, a) &gt; 0\)</span>.</p>
<p>The markov property is to be memoryless. Once you reach a state, the past history (the states visited before) should not affect the next transitions and rewards. Only the present state matters.</p>
<div class="figure">
<img src="mdp.png" alt="Schema of a MDP" />
<p class="caption">Schema of a MDP</p>
</div>
<h2 id="some-terminologies">Some terminologies</h2>
<p><strong>Final/terminal states</strong>: The states that have no available actions are final/terminal states.</p>
<p><strong>Episode</strong>: An episode is a complete play from one of the initial state to a final state. <span class="math display">\[s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_n\]</span></p>
<p><strong>Cumulative reward</strong>: The cumulative reward is the discounted sum of reward accumulated throughout an episode: <span class="math display">\[R=\sum_{t=0}^n \gamma^t r_{t+1}\]</span></p>
<p><strong>Policy</strong>: A Policy is the agent’s strategy to choose an action at each state. It is noted by <span class="math inline">\(\pi\)</span>.</p>
<p><strong>Optimal policy</strong>: The optimal policy is the theoritical policy that maximizes the expectation of cumulative reward. From the definition of expectation and the law of large numbers, this policy has the highest average cumulative rewards given sufficient episode. This policy might be intractable.</p>
<p><strong>The objective of reinforcement learning is to train an agent such that he learns a policy as close as possible to the theoritical optimal policy</strong>.</p>
<h1 id="different-settings">Different settings</h1>
<blockquote>
<p>« Qui peut le plus peut le moins » (He who can do the greather things, can do the lesser things)</p>
</blockquote>
<h2 id="model-free">Model-free</h2>
<p>The conditional distribution and the reward function constitute the model of the environment. In a game of backgammon, we know the model (each possible transition is decided by the known dice distribution, and we can predict each reward from transition without realizing them (because we can calculate the new value of the board)). The algorithm <a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node108.html">TD-gammon</a> use that fact to learn the V-function (see below).</p>
<p>Some reinforcement learning algorithms can work without being given the model. Nevertheless, in order to learn the best strategy, they additionaly have to learn the model during the training. This is called model-free reinforcement learning. Model-free algorithms are very important because a large majority of real world complex problems fall in that category. Futhermore, model free is simply an additional constraint. Model-free reinforcement learning is simply more powerful since it is a superset of model based reinforcement learning.</p>
<h2 id="observation-setting">Observation setting</h2>
<p>Instead of being given access to the state, you might being given access to a partial observation of the state only. It is the same idea behind Hidden Markov Chain. This is the difference between the partial and fully observed setting. For instance, our field of vision is a very partial observation of the full state of the universe (the position and energy of every particule in the universe). Fortunately, the partial observation setting can be reduced to a fully observed setting with the use an history (the state becomes an accumulation of previous states).</p>
<p>Nevertheless, it is most common to not accumulate the whole history. Either only the last h observations are stacked (in a windowed fashion) or you can use a Recurrent Neural Network (RNN) to learn what to keep in memory and what to forget (that is essentially how a <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a> works).</p>
<p>Abusing the language slighty for consistency purposes with the existing notation, history (even truncated ones) will also be called “states” and also symbolized <span class="math inline">\(S_t\)</span></p>
<h2 id="single-player-and-adversarial-games">Single player and adversarial games</h2>
<p>A single player game has a natural translation into a MDP. The states represent the moment where the player is in control. The observations from those states are all the informations accumulated between states (eg: as many pixel frame as there is in-between frames of control). An action is all the available command at the disposal of the player (In doom, go up, right, left, shoot, etc …).</p>
<p>Reinforcement learning can also be applied to adversarial games by self-play: The agent plays against itself. Often in this setting, there exists a <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a> such that it is always in your interest to play as if your opponent was a perfect player. This makes sense in chess by example. If given a board configuration, a good move against a chess master, would still be a good move against a beginner <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Whatever is the current level of the agent, by playing against himself, the agent stills get information about the quality of his previous moves (seen as good moves if he won, bad moves if he lost).</p>
<p>Of course the information, which is a gradient in the context of a neural network, is of «higher quality» if he played directly against a very good agent from the start. But it is really mind-blowing that an agent can learn to increase his level of play by playing against himself, an agent of the same level. That is actually the method of training employed by <a href="https://deepmind.com/alpha-go">AlphaGo</a> (the Go agent from DeepMind that beat the World Champion). The policy was bootstrapped (initially trained) on a dataset of master moves, then it used reinforcement learning and self play to increase furthermore the level (quantified with elo). In the end, the agent got better than policy it was learning from the original dataset. After all, it beat the master above all the masters. To compute the final policy, they used their policy gradient in combination with a Monte-Carlo Search Tree on a massive amount of computation power.</p>
<p>This setting is a bit different than learning from pixels. Firstly, because the input is not as high-dimensional. The <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">manifold</a> is a lot closer to its embedding space. Nevertheless, a convolutional layer was still used in this case to use efficiently the locality of some configuration patterns. Secondly, because AlphaGo is not model-free (it is deterministic). In the following of this post, I will talk exclusively about the model-free 1-player setting.</p>
<h1 id="q-learning">Q-learning</h1>
<h2 id="from-policy-to-neural-network">From policy to neural network</h2>
<p>Our goal is learn the optimal policy <span class="math inline">\(\pi^*\)</span> that maximize <span class="math display">\[E[R_0]=E[\sum_{t=0}^n \gamma^t r_{t+1}]\]</span> Let’s introduce an auxilliary function: <span class="math display">\[V_\pi(s) = E \{ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots + \gamma^n r_{n} \mid s_t = s, \text{policy followed at each state is }\pi \}\]</span> which is the expected cumulative reward from a state <span class="math inline">\(s\)</span> following the policy <span class="math inline">\(\pi\)</span>.</p>
<p>Let suppose an oracle</p>
<p><span class="math display">\[V_{\pi^*}(s)\]</span></p>
<p>The V function of the optimal policy. From it, we could retrieve the optimal policy by defining the policy that among all available actions at the current state, choose the action that maximize the expectation of <span class="math inline">\(V_{\pi^*}(s)\)</span>. This is a greedy behavior. The optimal policy is the greedy policy w.r.t to <span class="math inline">\(V_{\pi^*}\)</span>.</p>
<p><span class="math display">\[ \pi(s) ~\text{chooses a s.t}~a=\max_a[E(r_t + \gamma V(s_{t+1}) \mid s_t=s, a_t=a)] \]</span></p>
<p>If you were extremely attentive, something would sound wrong here. In the model-free setting, we cannot predict the after-state <span class="math inline">\(s_{t+1}\)</span> from <span class="math inline">\(s_{t}\)</span> because we ignore the transition model.</p>
<p>To solve this very annoying issue, we are gonna use another auxiliarry function, the Q function:</p>
<p><span class="math display">\[Q_\pi(s, a) = E[r_t + \gamma V_\pi(s_{t+1} \mid s_t, a_t = a]\]</span></p>
<p>In a greedy setting, we have the relationship:</p>
<p><span class="math display">\[V(s_t) = \max_a Q(s_t, a)\]</span></p>
<p>Now, let suppose instead of the V oracle, we have the Q oracle. We can now redefine <span class="math inline">\(\pi^*\)</span>.</p>
<p><span class="math display">\[ \pi^*(s) ~ \text{chooses a s.t}~ a= \max_a[Q(s, a)] \]</span></p>
<p><strong>Neat</strong></p>
<p>Unfortunately, oracles do not exist in the real world.</p>
<p>The trick here is that we have reduced an abstract notion that is a policy into a numerical function. Fortunately for us, there is one weapon at our disposal to approximate complex functions: <strong>Neural networks</strong>.</p>
<p>Neural networks are universal function approximator. They can approximate any differentiable function. Although they can get stuck in local extrema and many proofs are not valid anymore when throwing neural networks in the equation. This is because their learning is not as deterministic or boundable than a linear function. Nonetheless, in most case, with the right hyperparameters, they are unreasonnably powerful. Using deep learning with reinforcement learning is called deep reinforcement learning.</p>
<h2 id="policy-iteration">Policy iteration</h2>
<p>Now machine learning knowledge and common sense tells you that there is still something missing about our approeach. Neural networks can approximate functions that already have labels. Unfortunately for us oracles do not exist in the real world, so we will have to get our labels another way. (:&lt;).</p>
<p>This is where the magic of Monte Carlo come in. Monte carlo methods are methods that rely on repeated random sampling to calculate an estimator. (A famous example is the <a href="http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopimod.html">pi calculation</a>).</p>
<p>If we play randomly from a given state, the better state should get better reward on average. So without knowing anything about the environment, you can get some information about the expected value of a state. For example, at poker, better hands will win more often on average than lesser hands when every decision is taken randomly. Monte Carlo Search Tree are based on this property. This is a phase of exploration that lead to unsupervised learning and enable us to extract meaningful label.</p>
<p>More formally,</p>
<p>Given a policy <span class="math inline">\(\pi\)</span>, a state s and an action a, to get an approximation of <span class="math inline">\(Q(s, a)\)</span> we sample it according to its definition:</p>
<span class="math display">\[\begin{align*}
Q_\pi(s, a) &amp;= E[r_t + V_\pi(s_{t+1}) \mid s_t = s, a_t = a] \\
&amp;= E[r_t + \gamma E \{ r_{t+1} + \gamma r_{t+2} + \ldots + \gamma^{n-1}  r_{n}\} \mid s_t = s, a_t = a] \\
&amp;= E[r_t + \gamma r_{t+1} + \ldots + \gamma^n  r_{n}\} \mid s_t = s, a_t = a]
\end{align*}\]</span>
<p>In plain english, we can get a label for <span class="math inline">\(Q_\pi(s, a)\)</span> by playing a sufficient number of time from s according to the policy <span class="math inline">\(\pi\)</span>.</p>
<p>From an aggregate of signals:</p>
<div class="figure">
<img src="qtarget.png" alt="One signal" />
<p class="caption">One signal</p>
</div>
<p>We use the Mean-Square Error loss function (l2 loss) with a learning rate of <span class="math inline">\(\alpha\)</span> and apply Stochastic Gradient Descent (On a batch of size 1): <span class="math display">\[Q_\pi(s_t, a_t) \leftarrow Q_\pi(s_t, a_t) + \alpha [R_t-Q_\pi(s_t, a_t)]\]</span></p>
<p>Note: There is no square in the formula because the loss is applied afterwards on the difference of the expected output <span class="math inline">\(Q_\pi(s_t, a_t)\)</span> and the label <span class="math inline">\(\alpha [R_t-Q_\pi(s_t, a_t)]\)</span></p>
<p>Repeat many times: Sampling from <span class="math inline">\(\pi\)</span> <span class="math display">\[Q_\pi(s_t, a_t)    \leftarrow E_\pi[R_t] = E_{s_t, a_t, ..., s_n \sim \pi}[\sum_{i=t}^n \gamma^{i-t}r_i]\]</span></p>
<p>We can converge to the rightful expectation</p>
<div class="figure">
<img src="qexpect.png" alt="Many signals" />
<p class="caption">Many signals</p>
</div>
<p>So we can now design a naive prototype of our learning algorithm (in Scala but it is intelligible without any Scala knowledge):</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">
<span class="co">//A randomly uninitialized neural network</span>
<span class="kw">val</span> neuralNet: NeuralNet

<span class="co">//Iterate until you reach max epoch</span>
<span class="kw">for</span> (t &lt;- (<span class="dv">1</span> to MaxEpoch))
	<span class="fu">epoch</span>()


<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//pick a random state and action</span>
	<span class="kw">val</span> state = randomState
	<span class="kw">val</span> action = <span class="fu">randomAction</span>(state)

	<span class="co">//transition to a new state, initalize thethe reward</span>
	<span class="kw">var</span> (new_state, accuReward) = <span class="fu">transitition</span>(state, action)

	<span class="co">//play until terminal state and accumulate the reward</span>
	accuReward += <span class="fu">playRandomly</span>(state)

	<span class="co">//Do SGD over input and label!</span>
	<span class="fu">fit</span>((state, action), accuReward)
}



<span class="co">// MDP specific, return the new state and the reward</span>
<span class="kw">def</span> <span class="fu">transition</span>(state: State, action: Action): (State, Double)

<span class="co">//return a randomly sampled state among all the state space</span>
<span class="kw">def</span> randomState: State

<span class="co">//play until terminal state</span>
<span class="kw">def</span> <span class="fu">playRandomly</span>(state): Double = {
	<span class="kw">var</span> s = state
	<span class="kw">var</span> accuReward = <span class="dv">0</span>
	<span class="kw">var</span> k = <span class="dv">0</span>
	<span class="kw">while</span> (!s.<span class="fu">isTerminal</span>) {
		<span class="kw">val</span> action = <span class="fu">randomAction</span>(s)
		<span class="kw">val</span> (state, reward) = <span class="fu">transition</span>(s, action)
		k += <span class="dv">1</span>
		accuReward += Math.<span class="fu">pow</span>(gamma, k) * reward
		s = state
	}
	accuReward
}


<span class="co">//choose a random action among all the available actions at this state</span>
<span class="kw">def</span> <span class="fu">randomAction</span>(state: State): Action =
	<span class="fu">oneOf</span>(state.<span class="fu">available_action</span>)

<span class="co">//helper function, pick one among</span>
<span class="kw">def</span> <span class="fu">oneOf</span>(seq: Seq[Action]): Action =
	seq.<span class="fu">get</span>(Random.<span class="fu">nextInt</span>(seq.<span class="fu">size</span>))


<span class="co">//How it would be roughly done with DL4J</span>
<span class="kw">def</span> <span class="fu">fit</span>(input: (State, Action), label: Double) =
	neuralNet.<span class="fu">fit</span>(<span class="fu">toTensor</span>(input), <span class="fu">toTensor</span>(label))

<span class="co">//return an INDArray from ND4J</span>
<span class="kw">def</span> <span class="fu">toTensor</span>(array: Array[_]): Tensor =
	Nd4j.<span class="fu">create</span>(array)</code></pre></div>
<p>There is multiple issues: This should work but this is terribly inefficient. We are playing a full game with n state and n actions for a single label and that label might not be very meaningful (If the interesting trajectories are hard to reach at random).</p>
<h2 id="offline-and-online-reinforcement-learning">Offline and online reinforcement learning</h2>
<p>To learn more the difference between online and offline reinforcement learning, see this excellent post from <a href="https://kofzor.github.io/Reinforcement_Learning_101/#comparing-reinforcement-learning-algorithms">kofzor</a></p>
<h2 id="explorationexploitation">Exploration/exploitation</h2>
<p>Exploring at random the environment will converge to the optimal policy … after an almost infinite time: you will have to visit every possible trajectories (a trajectory is the state visited and action choosen during an episode) at least once. In the real world, we do not have infinite time (and time is money).</p>
<p>Thus, we should exploit the past informations and our learning of them to focus our exploration on the most promising possible trajectories. This can be achieved through different ways, and one of them that we are gonna use is <span class="math inline">\(\epsilon\)</span>-greedy exploration. <span class="math inline">\(\epsilon\)</span>-greedy exploration is fairly simple. It is a policy that choose an action at random with odd <span class="math inline">\(epsilon\)</span> or the best action as deemed by our current policy. Usually <span class="math inline">\(epsilon\)</span> is annealed over time to privilege exploitation over exploration. This is a trade-off between exploration and exploitation.</p>
<p>At each new information, our actual Q functions gets more accurate about the present policy and the exploration is focused on better paths. The policy based on our new Q function gets better (since Q is more accurate) and the <span class="math inline">\(\epsilon\)</span>-greedy exploration reach better path. Focused on those better paths, our q function explore even more the better parts and has to update its returns according to the new policy. This is an iterative cycle that enable convergence to the optimal policy called policy iteration. Unfortunately, the convergence can takes infitnite time and is not even guaranteed when Q is approximated by neural networks. Nevertheless, impressive results can make up for the lack of formal convergence proofs.</p>
<div class="figure">
<img src="policyiter.png" alt="Policy Iteration" />
<p class="caption">Policy Iteration</p>
</div>
<p>It also requires you to be able to sample the states in a good manner: It should be proportionally representative of the states that are usally present in a game. (On a sidenote, this is possible in some case, see <a href="#giraffe">Giraffe</a> that uses TD-Lambda).</p>
<p>There is 3 important observations:</p>
<h2 id="bellman-equation">Bellman equation</h2>
<ul>
<li>We can transform the Q equation into a <strong>Bellman equation</strong>: <span class="math display">\[Q_\pi(s, a)= E[r_t + \gamma r_{t+1} + \ldots + \gamma^n  r_{n} \mid s_t = s, a_t = a]\]</span> <span class="math display">\[Q_\pi(s, a)= E[r_t + \gamma r_{t+1} + V(s_{t+1}) \mid s_t = s, a_t = a]\]</span> <span class="math display">\[Q_\pi(s, a)= E[r_t + \gamma r_{t+1} + \ldots + \gamma \max_a' Q(s_{t+1}, a')\} \mid s_t = s, a_t = a]\]</span></li>
</ul>
<p>As in the Monte-Carlo method, we can do many updates of Q.</p>
<p>MSE: <span class="math display">\[Q_\pi(s_t, a_t) \leftarrow Q_\pi(s_t, a_t) + \alpha [(\underbrace{\underbrace{r_t+\max_a Q_\pi(s_{t+1}, a)}_{\text{target}}-Q_\pi(s_t, a_t)}_{\text{TD-error}})]\]</span></p>
<p>TD-error is the “Temporal difference error”. Indeed we are actually calculating the difference between what the Q approximation expects in the future and its present value and the realized reward.</p>
<p>if s is terminal:</p>
<p><span class="math display">\[V(s) = 0\]</span></p>
<p>and</p>
<p><span class="math display">\[Q(s_{t-1}, a) = r_t\]</span></p>
<p>The states near the terminal states are the first to converge because they are closer in the chain to that truth label: In Go or Chess, reinforcement learning is applied by assigning +1 to the transitions that lead to a final winning board (respectively -1 for a loosing board) and 0 otherwise. This lead to diffuse back how good is a transition by assigning it a value between [-1; 1]. A transition with Q value close to 0 lead to a balanced board. A transition with Q value close to 1 lead to a near certain victory.</p>
<p>As long as we sample sufficiently transitions near the terminal states, Q-learning is able to converge. The incredible power of deep reinforcement learning is that it will be able to generalize its learning from visited states to unvisited states. It should be able to understand what is a balanced or winning board even if it never seen it before. This is because the network should be able to abstract patterns and understand the strength of an action based on previously seen pattern (eg: shoot an ennemy when recognizing one).</p>
<h2 id="initial-state-sampling">Initial state sampling</h2>
<p>In a 1 player setting (like the atari game): We do not actually need to learn to play well in every situation (Although, if we did, that would show that we would have reached a very good level of generalization). We only need to learn to play efficiently from the states that our policy encounters. Thus, we can sample from states that are simply reachable by playing with our current policy from an initial state.</p>
<h2 id="q-learning-implementation">Q-Learning implementation</h2>
<p>So we can now design a naive prototype of our Q-Learning:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">

<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//sample among the initial state space</span>
	<span class="co">//(often unique state)</span>
	<span class="kw">var</span> state = initState

	<span class="co">//while the state is not terminal,</span>
	<span class="co">//play an episode and do a Q-update at each transition</span>
	<span class="kw">while</span>(!state.<span class="fu">isTerminal</span>)  {

		<span class="co">//sample action from eps-greddy policy</span>
		<span class="kw">val</span> action = <span class="fu">epsilonGreedyAction</span>(state)

		<span class="co">//interaction with the environment</span>
   		<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)

		<span class="co">//Q-update</span>
		<span class="fu">update</span>(state, action, reward, nextState)

		state = nextState
	}
}

<span class="co">//Our Q-update as explained above</span>
<span class="kw">def</span> <span class="fu">update</span>(state: State, action: Action, reward: Double, nextState: State) = {
	<span class="kw">val</span> target = reward + <span class="fu">maxQ</span>(nextState)
	<span class="fu">fit</span>((state, action), target)
}


<span class="co">//the eps-greedy policy implementation</span>
<span class="kw">def</span> <span class="fu">epsilonGreedyAction</span>(state: State) = {
	<span class="kw">if</span> (Random.<span class="fu">float</span>() &lt; epsilon)
		<span class="fu">randomAction</span>(state)
	<span class="kw">else</span>
		<span class="fu">maxQAction</span>(state)
}


<span class="co">//Retrive max Q value</span>
<span class="kw">def</span> <span class="fu">maxQ</span>(state: State) =
	<span class="fu">actionsWithQ</span>(state).<span class="fu">maxBy</span>(_.<span class="fu">_2</span>).<span class="fu">_2</span>

<span class="co">//Retrive action of the max Q value</span>
<span class="kw">def</span> <span class="fu">maxQAction</span>(state: State) =
	<span class="fu">actionsWithQ</span>(state).<span class="fu">maxBy</span>(_.<span class="fu">_2</span>).<span class="fu">_1</span>

<span class="co">//return a list of actions and the q-value of their transition from the state</span>
<span class="kw">def</span> <span class="fu">actionsWithQ</span>(state: State) = {
	<span class="kw">val</span> stateActionList = available_actions.<span class="fu">map</span>(action =&gt; (state, action))
   	available_actions.<span class="fu">zip</span>(neural_net.<span class="fu">output</span>(<span class="fu">toTensor</span>(state_action_list)))

<span class="kw">def</span> initState: State

</code></pre></div>
<h2 id="modeling-qs-a">Modeling Q(s, a)</h2>
<p>Instead of having <span class="math inline">\(a\)</span> as an additional input of the neural net combined with the state, the state is the only input and the output contain the Q value of every output possible. This make sense only when the availables actions are consistent accross the full episode.</p>
<div class="figure">
<img src="qmodeling.png" alt="Q modeling Source" />
<p class="caption">Q modeling <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Source</a></p>
</div>
<h2 id="experience-replay">Experience replay</h2>
<p>There is one issue with using neural network as Q approximator with neural networks. The transitions are very correlated. After all, they are all extracted from the same episode. Imagine if you had to learn a task without any memory (not even short-term), you would always optimise your learning based on the last episode.</p>
<p>The DeepMind team have had a genius idea to use an experience replay, which is a windowed buffer of the last N transition (N being a million in the original paper). Instead of updating from the last transition, you store it inside the experience replay and update from a batch of randomly sampled transitions from the same experience replay.</p>
<p>epoch() becomes:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">
<span class="kw">def</span> <span class="fu">epoch</span>() = {

	<span class="co">//sample among the initial state space</span>
	<span class="co">//(often unique state)</span>
	<span class="kw">var</span> state = initState

	<span class="co">//while the state is not terminal,</span>
	<span class="co">//play an episode and do a Q-update at each transition</span>
	<span class="kw">while</span>(!state.<span class="fu">isTerminal</span>)  {

		<span class="co">//sample action from eps-greddy policy</span>
		<span class="kw">val</span> action = <span class="fu">epsilonGreedyAction</span>(state)

		<span class="co">//interaction with the environment</span>
   		<span class="kw">val</span> (nextState, reward) = <span class="fu">transition</span>(state, action)

		<span class="co">//store transition (Exp Replay is just a Ring buffer)</span>
		expReplay.<span class="fu">store</span>(state, action, reward, nextState)

		<span class="co">//Q update in batch</span>
		<span class="fu">updateFromBatch</span>(expReplay.<span class="fu">getBatch</span>())

		state = nextState
	}
}</code></pre></div>
<h3 id="compression">Compression</h3>
<p>ND4J does not support as first class uint8 but pixels in grayscaling are encoded with that precision. To avoid wasting too much space on memory, INDArray were compressed to uint8.</p>
<h2 id="convolutional-layers-and-image-preprocessing">Convolutional layers and image preprocessing</h2>
<h3 id="convolutional-layers">Convolutional layers</h3>
<div class="figure">
<img src="conv.png" alt="Convolutional layer Source" />
<p class="caption">Convolutional layer <a href="#atari">Source</a></p>
</div>
<p>Convolutional layers are layers that are excellent to detect local patterns in images. For pixels it is used as a processor that is required to reduce the dimension of the input into its real manifold. Given the proper manifold of observations, the decision becomes much easier.</p>
<h3 id="image-processing">Image processing</h3>
<p>You could feed the neural network with the RGB directly, but then the network would have to also learn that additional pattern. It seems like the brain is hard-wired to combine colors so it would seem reasonnable to accept that preprocessing.</p>
<p>What you see:</p>
<video autoplay loop>
<source src="doom-pre.webm" type="video/webm">
</video>
<p>What the neural net see:</p>
<div class="figure">
<img src="doom-post.gif" alt="Neural net input" />
<p class="caption">Neural net input</p>
</div>
<h3 id="resizing">resizing</h3>
<p>The image is resized into 84x84. Convolutional layers needs for memory and computations grow with the size of their input. The fine details of the image are not required to play the game correctly. Indeed, many are purely aesthetic. Resizing to a more reasonnable size speed up the training.</p>
<h3 id="skip-frame">Skip frame</h3>
<p>In the original paper, only 1 in 4 frame is actually processed. For the following 3 images, the last action is repeated. It speeds up rougly by 4 time the training without loosing much information. Indeed, atari game are not supposed to be played frame perfect and for most action it makes more sense to keep them for at least 4 frames.</p>
<h3 id="history-processing">History Processing</h3>
<p>To give information to the Neural network about the current momentum, the last 4 frame (with skip frame, you pick 1 every 4) are stacked into 4 channels. Those 4 frames represent an history as previously discussed in the Observation setting section.</p>
<div class="figure">
<img src="slowed.gif" alt="Slowed input" />
<p class="caption">Slowed input</p>
</div>
<div class="figure">
<img src="stacked.png" alt="Stacking" />
<p class="caption">Stacking</p>
</div>
<p>To fill the first frames of the history, a random policy or a noop replay is used.</p>
<h2 id="double-q-learning">Double Q-learning</h2>
<p>The idea behind <a href="#doubleq">double DQN</a> is that the network is frozen every M update (hard update) or smoothly averaged (<code>target = target * (smooth) + current * (1-smooth)</code>) every update (soft update). Indeed, it adds stability to the learning by using a Q evaluation to use in the td-error formula that is less prone to unstability. The Q update becomes:</p>
<p><span class="math display">\[Y_\text{target} = r_t + \gamma*(Q_\text{target}(s_t+1, arg \max_a Q_\text(s_t+1, a))) \]</span></p>
<h2 id="clipping">Clipping</h2>
<p>The TD-error is clipped such that no outlier update can have too much impact on the learning.</p>
<h2 id="scaling-rewards">Scaling rewards</h2>
<p>Scaling the rewards such that Q-values are lower (in a range of [-1; 1] is similar to normalization). It can dramatically alter the efficiency of the learning.</p>
<h2 id="prioritized-replay">Prioritized replay</h2>
<p>The idea behind <a href="#prio">prioritized replay</a> is that not all transitions are born equal. Some are more important than others. One way to sort them is through their TD-error. Indeed, a high TD-error is correlated to a high level of information (in the sense of surprise). Those transitions should be sampled more often than the others.</p>
<h2 id="graph-visualisation-and-mean-q">Graph, Visualisation and Mean-Q</h2>
<p>To visualize and debug the training or a method of RL, it is a useful to have a visual monitoring. This is why I built <a href="https://github.com/rubenfiszel/webapp-rl4j">webapp-rl4j</a>.</p>
<div class="figure">
<img src="screen.png" alt="webapp-rl4j" />
<p class="caption">webapp-rl4j</p>
</div>
<p>The most important is to keep track of cumulative reward. This is a way to check that the agents effectively gets better. It is important to notice that it represents the epsilon greedy strategy and not directly dervied policy from the Q approximation.</p>
<div class="figure">
<img src="rewardgraph.png" alt="Cumulative reward graph" />
<p class="caption">Cumulative reward graph</p>
</div>
<p>But also the loss (score) and mean Q-values:</p>
<div class="figure">
<img src="scoregraph.png" alt="Score and mean-Q graph" />
<p class="caption">Score and mean-Q graph</p>
</div>
<p>If used with target network, you should see some discontinuities from the non continuous evaluation of different target networks. Loss should decrease w.r.t to a single target network. The mean Q-values should smoothly converge towards a value proportionnal to the mean expected reward.</p>
<h2 id="rl4j">RL4J</h2>
<p>RL4J is available on <a href="https://github.com/deeplearning4j/rl4j">github</a>. Currently DQN with Experience Replay, Double Q-learning and clipping is implemented. It is possible to play both from pixels or low-dimensional problems (like Cartpole). Async Reinforcement Learning is experimental. Hopefully, contributions will enrich the library.</p>
<p>Here is a working example with RL4J to play Cartpole. You can play Doom too. Check <a href="https://github.com/rubenfiszel/rl4j-examples">rl4j-examples</a> for more examples. It is also possible to provide your own constructed neural network model as an argument to any training method.</p>
<div class="sourceCode"><pre class="sourceCode java"><code class="sourceCode java">
    <span class="kw">public</span> <span class="dt">static</span> QLearning.<span class="fu">QLConfiguration</span> CARTPOLE_QL =
            <span class="kw">new</span> QLearning.<span class="fu">QLConfiguration</span>(
                    <span class="dv">123</span>, <span class="co">//Random seed</span>
                    <span class="dv">500</span>, <span class="co">//Max step By epoch</span>
                    <span class="dv">150000</span>, <span class="co">//Max step</span>
                    <span class="dv">150000</span>, <span class="co">//Max size of experience replay</span>
                    <span class="dv">32</span>, <span class="co">//size of batches</span>
                    <span class="dv">100</span>, <span class="co">//target update (hard)</span>
                    <span class="dv">10</span>,  <span class="co">//num step noop warmup</span>
                    <span class="fl">0.01</span>, <span class="co">//reward scaling</span>
                    <span class="fl">0.99</span>, <span class="co">//gamma</span>
                    <span class="fl">100.0</span>, <span class="co">//td-error clipping</span>
                    <span class="fl">0.</span>1f, <span class="co">//min epsilon</span>
                    <span class="dv">1000</span>, <span class="co">//num step for eps greedy anneal</span>
                    <span class="kw">true</span>
            );


    <span class="kw">public</span> <span class="dt">static</span> DQNFactoryStdDense.<span class="fu">Configuration</span> CARTPOLE_NET =
            <span class="co">//num layers, num hidden nodes, learning rate, l2 regularization</span>
            <span class="kw">new</span> DQNFactoryStdDense.<span class="fu">Configuration</span>(<span class="dv">3</span>, <span class="dv">16</span>, <span class="fl">0.001</span>, <span class="fl">0.00</span>);

    <span class="kw">public</span> <span class="dt">static</span> <span class="dt">void</span> <span class="fu">main</span>( String[] args )
    {

        <span class="co">//true means record this in rl4j-data in a new folder</span>
        DataManager manager = <span class="kw">new</span> <span class="fu">DataManager</span>(<span class="kw">true</span>);

        <span class="co">//define the mdp from gym (name, render)</span>
        GymEnv&lt;Box, Integer, DiscreteSpace&gt; mdp = <span class="kw">new</span> <span class="fu">GymEnv</span>(<span class="st">&quot;CartPole-v0&quot;</span>, <span class="kw">false</span>, <span class="kw">false</span>);

        mdp.<span class="fu">reset</span>();
        <span class="dt">double</span>[] arr = mdp.<span class="fu">step</span>(<span class="dv">1</span>).<span class="fu">getObservation</span>().<span class="fu">toArray</span>();
        System.<span class="fu">out</span>.<span class="fu">println</span>(arr[<span class="dv">0</span>]);
        mdp.<span class="fu">reset</span>();
        <span class="co">//define the training</span>
        QLearningDiscreteDense&lt;Box&gt; dql = <span class="kw">new</span> <span class="fu">QLearningDiscreteDense</span>(mdp, CARTPOLE_NET, CARTPOLE_QL, manager);

        <span class="co">//train</span>
        dql.<span class="fu">train</span>();

        <span class="co">//get the final policy</span>
        DQNPolicy&lt;Box&gt; pol = dql.<span class="fu">getPolicy</span>();

        <span class="co">//serialize and save (serialization showcase, but not required)</span>
        pol.<span class="fu">save</span>(<span class="st">&quot;/tmp/pol1&quot;</span>);

        <span class="co">//close the mdp</span>
        mdp.<span class="fu">close</span>();


    }</code></pre></div>
<h1 id="conclusion">Conclusion</h1>
<p>This was an exciting journey through deep reinforcement learning. From equations to code, Q-learning is a powerful yet a somewhat simple algorithm. The field of RL is very active and promising. In fact, Supervised learning could be considered a subset of Reinforcement learning (by setting the labels as rewards). Maybe one day, Reinforcement Learning will be the panacea. Until then, we can expect to be awed by it’s diverse application into more and more complex problems. I would also like to thank Skymind and its amazing team for this very enriching internship.</p>
<h1 id="to-feed-your-appetite">To feed your appetite</h1>
<p>I hope that thanks to this introduction, you are excited about R researchL. Here is a brief summary of important research in deep reinforcement learning.</p>
<h2 id="continuous-domain">Continuous domain</h2>
<p>When the action space is not discrete, you cannot use DQN. But many problems cannot be discretized. <a href="#continuous">Continuous control</a> is achieved through a normal distribution parametrized (mean and variance) by the output of the neural network. At each step, the action is sampled from the distribution. It also uses soft update of the target network.</p>
<h2 id="policy-gradient">Policy gradient</h2>
<p>Policy gradient work by directly learning the stochastic policy from the the log distribution. This excellent post from <a href="#karpathy">Karpathy’s blog</a> has more details on the matter. Using a stochastic policy feels more natural as it encourage exploration and exploit more fairly the uncertainty we have between the different values of the move. Indeed a max operation can end by ignoring fully a branch that is <span class="math inline">\(\epsilon\)</span> below in Q-value of another.</p>
<p>Policy gradient were used by AlphaGo in combination with MonteCarlo Search Tree. The Neural Network was bootstrapped (pretrained) on a dataset of master move before they let it improve itself with RL.</p>
<p>Nowadays, policy gradients are getting more popular. For example, A3C (see below) is based on it.</p>
<h2 id="asynchronous-methods-for-deep-reinforcement-learning">Asynchronous Methods for Deep Reinforcement Learning</h2>
<p>A3C (Asynchronous Actor Critic) and Async NStep Q learning are a WIP in rl4j. It bypass the need for an experience replay by using multiple agents exploring in parrallel the environment. The original <a href="#atari">paper</a> uses Hogwild!. In RL4J, a workaround is to use a central thread and accumulate gradient from “slave” agents. Having multiple agents exploring the environment enable to decorrelate the experience from the past episode and enable to gather more experience (instead of replaying multiple time the same transition). It is very efficient and the authors were able to train efficiently on a single machine!</p>
<h2 id="deep-exploration">Deep exploration</h2>
<p>Deep exploration was the subject of a semester project during my master. I wrote the Scala library <a href="https://github.com/rubenfiszel/scala-drl">scala-drl</a> about it. Deep exploration is defined as the multi-step ahead planning of the exploration.</p>
<p>In <a href="#bootstrapped">Bootstrapped DQN</a>, the term bootstrapping comes from <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">statistics</a>. Multiple neural network are constructured in parralel. At each epoch, one neural network explore the environment. Then the transition are randomly redistributed to each model. This has a similar effect than resampling. Having had differences experiences to learn from, each model has its own opinion about the the best move and its own uncertainty about the environment. This encourage deep exploration.</p>
<p>Another way is to use <a href="#autoencoder">Autoencoder</a> to quantify the uncetainty about a novel state and attribute an exploration bonus based on the reconstruction ability.</p>
<h2 id="other-interesting-papers-to-discover-by-yourself.">Other interesting papers to discover by yourself.</h2>
<h2 id="deterministic-policy-gradient">Deterministic Policy Gradient</h2>
<p><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf" class="uri">http://jmlr.org/proceedings/papers/v32/silver14.pdf</a></p>
<h2 id="trusted-region-policy-optimisation">Trusted Region Policy Optimisation</h2>
<p><a href="https://arxiv.org/abs/1502.05477" class="uri">https://arxiv.org/abs/1502.05477</a></p>
<h2 id="dueling-network-architectures-for-deep-reinforcement-learning">Dueling Network Architectures for Deep Reinforcement Learning</h2>
<p><a href="http://arxiv.org/abs/1511.06581" class="uri">http://arxiv.org/abs/1511.06581</a></p>
<h1 id="references">References</h1>
<p><a name="karpathy"></a> <a href="http://karpathy.github.io/2016/05/31/rl/">Karpathy’s post about Policy Gradient</a></p>
<p><a name="atari"></a> <a href="http://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></p>
<p><a name="doubleq"></a> <a href="http://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a></p>
<p><a name="a3c"></a> <a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a></p>
<p><a name="prio"></a> <a href="http://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p>
<p><a name="continuous"></a> <a href="http://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a></p>
<p><a name="giraffe"></a> <a href="https://arxiv.org/abs/1509.01549">Giraffe: Using Deep Reinforcement Learning to Play Chess</a></p>
<p><a name="bootstrapped"></a> <a href="http://arxiv.org/abs/1602.04621">Deep Exploration via Bootstrapped DQN</a></p>
<p><a name="autoencoder"></a> <a href="http://arxiv.org/abs/1507.00814">Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</a></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>(Note: As my semester project advisor remarked, this is not always true. Imagine you play against a beginner at chess and you are badly loosing, you might want to “bait” him to reverse the situation. A bait is only beneficial when the opponent has a low level of play. This proove that chess is not a real Nash equilibrium. This does not matter much because the goal is often to build a high level of play agent that plays against other high quality agents. At this level of play, you do not loose if given a significant advantage.<a href="#fnref1">↩</a></p></li>
</ol>
</div>

          </div>

	  <hr>

            <div id="footer">
	      <section class="social">
		<a href="https://ch.linkedin.com/in/rubenfiszel" target="_blank"><i class="fa fa-linkedin fa-2x"></i></a>
		<a href="https://github.com/rubenfiszel" target="_blank"><i class="fa fa-github fa-2x"></i></a>
		<a href="mailto:ruben.fiszel@epfl.ch"><i class="fa fa-envelope fa-2x"></i></a>
		<a href="assets/RubenFiszel_resume.pdf"><i class="fa fa-file fa-2x"></i></a>
	      </section>
            </div>

	    <script>
	      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	      ga('create', 'UA-3040887-4', 'auto');
	      ga('send', 'pageview');

	    </script>


	  </body>
	</html>
