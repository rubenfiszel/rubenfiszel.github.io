<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>(thesis 1/4) Accelerated sensor fusion algorithm for POSE estimation of drones - Ruben Fiszel's website
	</title>
	<link rel="icon" type="image/png" href="../../images/lambda-xl.png">
	<link rel="stylesheet" type="text/css" href="../../css/syntax.css" />
	<link rel="stylesheet" type="text/css" href="../../sass/main.css" />
	<link type="text/css" href="../../css/font-awesome.min.css" rel="stylesheet">
	<link rel="stylesheet" href="../../css/mermaid.css">
	<link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
	<link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
	<link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">
	<script type="text/javascript" async
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1//MathJax.js?config=TeX-AMS_CHTML"></script>
</head>

<body>
	<div id="header">
		<div id="logo">
			<img src="../../images/lambda.png" alt="Lambda">
			<a href="../../">Ruben Fiszel's website</a>
		</div>

		<div id="navigation">
			<a href="../../">Home</a>
			<a href="../../about.html">About</a>
			<a href="../../contact.html">Contact</a>
		</div>
		<div class="clear"></div>
	</div>

	<div id="content">
		<h1 class="post-title"> (thesis 1/4) Accelerated sensor fusion algorithm for POSE estimation of drones</h1>
		<h2 class="post-title"> An Accelerated and Asynchronous Rao-Blackwellized Particle filter</h2>

		<div class="info">
			Posted on August 16, 2017

			by Ruben Fiszel

		</div>
		<div class="post">
			<h3 id="about">About</h3>
			<p>This post is the part I out of IV of my <a href="../../assets/thesis.pdf">master thesis</a> at the <a
					href="http://dawn.cs.stanford.edu/">DAWN lab</a>, Stanford, under <a
					href="http://arsenalfc.stanford.edu/kunle">Prof. Kunle</a> and <a
					href="http://lampwww.epfl.ch/~odersky/">Prof. Odersky</a> supervision. The central themes of this
				thesis are sensor fusion and spatial, an hardware description language (Verilog is also one, but
				tedious).</p>
			<p>This part is about an application of hardware acceleration, sensor fusion for drones. Part II will be
				about <a href="https://github.com/rubenfiszel/scala-flow/">scala-flow</a>, a library made during my
				thesis as a development tool for Spatial inspired by Simulink. This library eased the development of the
				filter but is also intended to be general purpose. Part III is about the development of an interpreter
				for spatial. Finally, Part IV is about the spatial implementation of the asynchronous Rao-Blackwellized
				Particle filter presented in Part I. If you are only interested in the filter, you can skip the
				introduction.</p>
			<h1 id="introduction" class="unnumbered">Introduction</h1>
			<h2 id="the-decline-of-moores-law">The decline of Moore’s law</h2>
			<p>Moore’s law<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> has prevailed in the
				computation world for the last four decades. Each new generation of processor comes the promise of
				exponentially faster execution. However, transistors are reaching the scale of 10nm, only one hundred
				times bigger than an atom. Unfortunately, the quantum rules of physics governing the infinitesimally
				small start to manifest themselves. In particular, quantum tunneling moves electrons across classically
				insurmountable barriers, making computations approximate, resulting in a non negligible fraction of
				errors.</p>
			<div class="figure">
				<img src="moorelaw.png"
					alt="The number of transistors throughout the years. We can observe a recent start of a decline" />
				<p class="caption">The number of transistors throughout the years. We can observe a recent start of a
					decline</p>
			</div>
			<h2 id="the-rise-of-hardware">The rise of Hardware</h2>
			<p>Hardware and Software respectively describe here programs that are executed as code for a general purpose
				processing unit and programs that are a hardware description and synthesized as circuits. The dichotomy
				is not very well-defined and we can think of it as a spectrum. General-purpose computing on graphics
				processing units (GPGPU) is in-between in the sense that it is general purpose but relevant only for
				embarrassingly parallel tasks<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> and very
				efficient when used well. GPUs have benefited from high-investment and many generations of iterations
				and hence, for some tasks they can match with or even surpass hardware such as field-programmable gate
				arrays (FPGA).</p>
			<div class="figure">
				<img src="hwsf.jpg" alt="Hardware vs Software" />
				<p class="caption">Hardware vs Software</p>
			</div>
			<p>The option of custom hardware implementations has always been there, but application-specific integrated
				circuit (ASIC) has prohibitive costs upfront (near $100M for a tapeout). Reprogrammable hardware like
				FPGAs have only been used marginally and for some specific industries like high-frequency trading. But
				now Hardware is the next natural step to increase performance, at least until a computing revolution
				happens, e.g: quantum computing, yet this sounds unrealistic in a near future. Nevertheless, hardware do
				not enjoy the same quality of tooling, language and integrated development environments (IDE) as
				software. This is one the motivations behind Spatial: bridging the gap between software and hardware by
				abstracting control flow through language constructions.</p>
			<h2 id="hardware-as-companion-accelerators">Hardware as companion accelerators</h2>
			<p>In most cases, hardware would be inappropriate: running an OS as hardware would be impracticable.
				Nevertheless, as a companion to a central-processing unit (CPU also called “the host”), it is possible
				to get the best of both worlds: the flexibility of software on a CPU with the speed of hardware. In this
				setup, hardware is considered an “<em>accelerator</em>” (hence, the term “hardware accelerator”). It
				accelerates the most demanding subroutines of the CPU. This companionship is already present in modern
				computer desktops in the form of GPUs for <em>shader</em> operations and sound cards for complex sound
				transformation/output.</p>
			<h2 id="the-right-metric-perfwatt">The right metric: Perf/Watt</h2>
			<p>The right evaluation metric for accelerators is performance per energy, as measured in FLOPS per Watt.
				This is a fair metric for comparing different hardware and architecture because it reveals its intrinsic
				properties as a computing element. If the metric was solely performance, then it would be enough to
				stack the same hardware and eventually reach the scale of a super-computer. Performance per dollar is
				not a good metric either because it does not account for the cost of energy at runtime. Hence, Perf/Watt
				is a fair metric to compare architectures.</p>
			<h2 id="spatial">Spatial</h2>
			<p>At the DAWN lab, under the lead of <a href="http://arsenalfc.stanford.edu/kunle">Prof. Olukotun</a> and
				his grad students, is developed an Hardware Description Language (HDL) implemented as an embedded scala
				DSL <a href="https://github.com/stanford-ppl/spatial-lang">spatial</a> and its compiler to program
				Hardware in a higher-level, more user-friendly, more productive language than Verilog. In particular,
				control flows are automatically generated when possible. This should enable software engineers to unlock
				the potential of Hardware. A custom CGRA, Plasticine, has been developed in parallel to Spatial. It
				leverages some recurrent patterns as the parallel patterns and it aims at being the most efficient
				reprogrammable architecture for Spatial.</p>
			<p>Th upfront cost is large but once at a big enough scale, Plasticine could be deployed as an accelerator
				in a wide range of use-cases, from the most demanding server applications to embedded systems with heavy
				computing requirements.</p>
			<h2 id="embedded-systems-and-drones">Embedded systems and drones</h2>
			<p>Embedded systems are limited by the amount of power at disposal in the battery and may also have size
				constraints. At the same time, especially for autonomous vehicles, there is a great need for computing
				power.</p>
			<p>Thus, developing drone applications with Spatial demonstrates the advantages of the platform. As a matter
				of fact, the filter implementation was only made possible because it is run on a hardware accelerator.
				It would be unfeasible to run it on more conventional micro-transistors. Particle filters, the family of
				filter which encompasses the types developed here, being very computationally expensive, are very seldom
				used for drones.</p>
			<h1
				id="sensor-fusion-algorithm-for-pose-estimation-of-drones-asynchronous-rao-blackwellized-particle-filter">
				Sensor fusion algorithm for POSE estimation of drones: Asynchronous Rao-Blackwellized Particle filter
			</h1>
			<p>POSE is the combination of the position and orientation of an object. POSE estimation is important for
				drones. It is a subroutine of SLAM (Simultaneous localization and mapping) and it is a central part of
				motion planning and motion control. More accurate and more reliable POSE estimation results in more
				agile, more reactive and safer drones. Drones are an intellectually stimulating topic and in the
				near-future they might also see their usage increase exponentially. In this context, developing and
				implementing new filter for POSE estimation is both important for the field of robotics but also to
				demonstrate the importance of hardware acceleration. Indeed, the best and last filter presented here is
				only made possible because it can be hardware accelerated with Spatial. Furthermore, particle filters
				are embarrassingly parallel algorithms. Hence, they can leverage the potential of a dedicated hardware
				design. The Spatial implementation will be presented in Part IV.</p>
			<p>Before expanding on the Rao-Blackwellized Particle Filter (RBPF), we will introduce here several other
				filters for POSE estimation for highly dynamic objects: Complementary filter, Kalman Filter, Extended
				Kalman Filter, Particle Filter and finally Rao-Blackwellized Particle filter. This ranges from the most
				conceptually simple, to the most complex. This order is justified because complex filters aim to
				alleviate some of the flaws of their simpler counterparts. It is important to understand which one and
				how.</p>
			<p>The core of the problem we are trying to solve is to track the current position of the drone given the
				noisy measurements of the sensor. It is a challenging problem because a good algorithm must take into
				account that the measurements are noisy and that the transformation applied to the state are non-linear,
				because of the orientation components of the state. Particle filters are efficient to handle non-linear
				state transformations and that is the intuition behind the development of the RBPF.</p>
			<p>All the following filters are developed and tested in scala-flow. scala-flow will be expanded in part II
				of this thesis. For now, we will focus on the model and the results, and leave the implementation
				details for later.</p>
			<h2 id="drones-and-collision-avoidance">Drones and collision avoidance</h2>
			<p>The original motivation for the development of accelerated POSE estimation is for the task of collision
				avoidance by quadcopters. In particular, a collision avoidance algorithm developed at the <a
					href="https://asl.stanford.edu/">ASL lab</a> and demonstrated here <a
					href="https://www.youtube.com/watch?v=kdlhfMiWVV0">(https://youtu.be/kdlhfMiWVV0)</a></p>
			<iframe width="560" height="315" src="https://www.youtube.com/embed/kdlhfMiWVV0" frameborder="0"
				allowfullscreen>
			</iframe>
			<div class="figure">
				<img src="empty.jpg" alt="Ross Allen fencing with his drone" />
				<p class="caption">Ross Allen fencing with his drone</p>
			</div>
			<p>where the drone avoids the sword attack from its creator. At first, it was thought of accelerating the
				whole algorithm but it was found that one of the most demanding subroutines was pose estimation.
				Moreover, it was wished to increase the processing rate of the filter such that it could match the input
				with the fastest sampling rate: its inertial measurement unit (IMU) containing an accelerometer, a
				gyroscope and a magnetometer.</p>
			<p>The flamewheel f450 is the typical drone in this category. It is surprisingly fast and agile. With the
				proper commands, it can generate enough thrust to avoid any incoming object in a very short lapse of
				time.</p>
			<div class="figure">
				<img src="f450.jpg" alt="The Flamewheel f450" />
				<p class="caption">The Flamewheel f450</p>
			</div>
			<h2 id="sensor-fusion">Sensor fusion</h2>
			<p>Sensor fusion is the combination of sensory data or data derived from disparate sources such that the
				resulting information has less uncertainty than what would be possible if these sources were to be used
				individually. In the context of drones, it is very useful because it enables us to combine many
				imprecise sensor measurements to form a more precise one like having precise positioning from 2 less
				precise GPS (dual GPS setting). It can also allows us to combine sensors with different sampling rates:
				typically, precise sensors with low sampling rate and less precise sensors with high sampling rates.
				Both cases will be relevant here.</p>
			<p>A fundamental explanation of why this is possible at all comes from the central limit theorem: one sample
				from a distribution with a low variance is as good as n samples from a distribution with variance <span
					class="math inline">\(n\)</span> times higher.</p>
			<p><span class="math display">\[\mathbb{V}(X_i)=\sigma^2 ~~~~~ \mathbb{E}(X_i) = \mu\]</span> <span
					class="math display">\[\bar{X} = \frac{1}{n}\sum X_i\]</span> <span
					class="math display">\[\mathbb{V}(\bar{X}) = \frac{\sigma^2}{n} ~~~~~ \mathbb{E}(\bar{X}) =
					\mu\]</span></p>
			<h2 id="notes-on-notation-and-conventions">Notes on notation and conventions</h2>
			<p>The referential by default is the fixed world frame.</p>
			<ul>
				<li><span class="math inline">\(\mathbf{x}\)</span> designates a vector</li>
				<li><span class="math inline">\(x_t\)</span> is the random variable x at time t</li>
				<li><span class="math inline">\(x_{t1:t2}\)</span> is the product of the random variable x between t1
					included and t2 included</li>
				<li><span class="math inline">\(x^{(i)}\)</span> designates the random variable x of the arbitrary
					particle i</li>
				<li><span class="math inline">\(\hat{x}\)</span> designates an estimated variable</li>
			</ul>
			<h2 id="pose">POSE</h2>
			<p>POSE is the task of estimating the position and orientation of an object through time. It is a subroutine
				of Software Localization And Mapping (SLAM). We can formalize the problem as:</p>
			<p>At each timestep, find the best expectation of a function of the hidden variable state (position and
				orientation), from their initial distribution and the history of observable random variables (such as
				sensor measurements).</p>
			<ul>
				<li>The state <span class="math inline">\(\mathbf{x}\)</span></li>
				<li>The function <span class="math inline">\(g(\mathbf{x})\)</span> such that <span
						class="math inline">\(g(\mathbf{x}_t) = (\mathbf{p}_t, \mathbf{q}_t)\)</span> where <span
						class="math inline">\(\mathbf{p}\)</span> is the position and <span
						class="math inline">\(\mathbf{q}\)</span> is the attitude as a quaternion.</li>
				<li>The observable variable <span class="math inline">\(\mathbf{y}\)</span> composed of the sensor
					measurements <span class="math inline">\(\mathbf{z}\)</span> and the control input <span
						class="math inline">\(\mathbf{u}\)</span></li>
			</ul>
			<p>The algorithm inputs are:</p>
			<ul>
				<li>control inputs <span class="math inline">\(\mathbf{u}_t\)</span> (the commands sent to the flight
					controller)</li>
				<li>sensor measurements <span class="math inline">\(\mathbf{z}_t\)</span> coming from different sensors
					with different sampling rate</li>
				<li>information about the sensors (sensor measurements biases and matrix of covariance)</li>
			</ul>
			<h2 id="trajectory-data-generation">Trajectory data generation</h2>
			<p>The difficulties with using real flight data is that you need to get the <em>true</em> trajectory and you
				need enough data to check the efficiency of the filters.</p>
			<p>To avoid these issues, the flight data is simulated through a model of trajectory generation from <span
					class="citation">[<a href="#ref-mueller_computationally_2015">1</a>]</span>. Data generated this way
				is called synthetic data. The algorithm inputs are the motion primitives defined by the quadcopter’s
				initial state, the desired motion duration, and any combination of components of the quadcopter’s
				position, velocity and acceleration at the motion’s end. The algorithm is essentially a closed form
				solution for the given primitives. The closed form solution minimizes a cost function related to the
				input aggressiveness.</p>
			<p>The bulk of the method is that a differential equation representing the difference of position, velocity
				and acceleration between the starting and ending state is solved with the <a
					href="https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle">Pontryagin’s minimum
					principle</a> using the appropriate <a
					href="https://en.wikipedia.org/wiki/Hamiltonian_(control_theory)">Hamiltonian</a>. Then, from that
				closed form solution, a per-axis cost can be calculated to pick the “least aggressive” trajectory out of
				different candidates. Finally, the feasibility of the trajectory is computed using the constraints of
				maximum thrust and body rate (angular velocity) limits.</p>
			<p>For the purpose of this work, a scala implementation of the model was written. Then, some keypoints
				containing Gaussian components for the position, velocity acceleration, and duration were tried until a
				feasible set of keypoints was found. This method of data generation is both fast and a good enough
				approximation of the actual trajectories that a drone would perform in the real world.</p>
			<video autoplay loop>
				<source src="flight.webm" type="video/webm">
			</video>
			<div class="figure">
				<img src="empty.jpg" alt="Visualization of an example of a synthetic generated flight trajectory" />
				<p class="caption">Visualization of an example of a synthetic generated flight trajectory</p>
			</div>
			<h2 id="quaternion">Quaternion</h2>
			<p>Quaternions are extensions of complex numbers with 3 imaginary parts. Unit quaternions can be used to
				represent orientation, also referred to as attitude. Quaternions algebra make rotation composition
				simple and quaternions avoid the issue of gimbal lock.<a href="#fn3" class="footnoteRef"
					id="fnref3"><sup>3</sup></a> In all filters presented, quaternions represent the attitude.</p>
			<p><span class="math display">\[\mathbf{q} = (q.r, q.i, q.j, q.k)^t = (q.r, \boldsymbol{\varrho})^T\]</span>
			</p>
			<p>Quaternion rotations composition is: <span class="math inline">\(q_2 q_1\)</span> which results in <span
					class="math inline">\(q_1\)</span> being rotated by the rotation represented by <span
					class="math inline">\(q_2\)</span>. From this, we can deduce that angular velocity integrated over
				time is simply <span class="math inline">\(q^t\)</span> if <span class="math inline">\(q\)</span> is the
				local quaternion rotation by unit of time. The product of two quaternions (also called Hamilton product)
				is computable by regrouping the same type of imaginary and real components together and accordingly to
				the identity:</p>
			<p><span class="math display">\[i^2=j^2=k^2=ijk=-1\]</span></p>
			<p>Rotation of a vector by a quaternion is done by: <span class="math inline">\(q v q^*\)</span> where <span
					class="math inline">\(q\)</span> is the quaternion representing the rotation, <span
					class="math inline">\(q^*\)</span> its conjugate and <span class="math inline">\(v\)</span> the
				vector to be rotated. The conjugate of a quaternion is:</p>
			<p><span class="math display">\[q^* = - \frac{1}{2} (q + iqi + jqj + kqk)\]</span></p>
			<p>The distance of between two quaternions, useful as an error metric is defined by the squared Frobenius
				norms of attitude matrix differences <span class="citation">[<a
						href="#ref-markley_averaging_2007">2</a>]</span>.</p>
			<p><span class="math display">\[\| A(\mathbf{q}_1) - A(\mathbf{q}_2) \|^2_F = 6 - 2 Tr [
					A(\mathbf{q}_1)A^t(\mathbf{q}_2) ]\]</span></p>
			<p>where</p>
			<p><span class="math display">\[A(\mathbf{q}) = (q.r^2 - \| \boldsymbol{\varrho} \|^2) I_{3 \times 3} +
					2\boldsymbol{\varrho} \boldsymbol{\varrho}^T - 2q.r[\boldsymbol{\varrho} \times]\]</span></p>
			<p><span class="math display">\[[\boldsymbol{\varrho} \times] = \left( \begin{array}{ccc}
					0 &amp; -q.k &amp; q.j \\
					q.k &amp; 0 &amp; -q.i \\
					-q.j &amp; q.i &amp; 0 \\
					\end{array} \right)\]</span></p>
			<h2 id="helper-functions-and-matrices">Helper functions and matrices</h2>
			<p>We introduce some helper matrices.</p>
			<ul>
				<li><span class="math inline">\(\mathbf{R}_{b2f}\{\mathbf{q}\}\)</span> is the body to fixed vector
					rotation matrix. It transforms vector in the body frame to the fixed world frame. It takes as
					parameter the attitude <span class="math inline">\(\mathbf{q}\)</span>.</li>
				<li><span class="math inline">\(\mathbf{R}_{f2b}\{\mathbf{q}\}\)</span> is its inverse matrix (from
					fixed to body).</li>
				<li><span class="math inline">\(\mathbf{T}_{2a} = (0, 0, 1/m)^T\)</span> is the scaling from thrust to
					acceleration (by dividing by the weight of the drone: <span class="math inline">\(\mathbf{F} =
						m\mathbf{a} \Rightarrow \mathbf{a} = \mathbf{F}/m)\)</span> and then multiplying by a unit
					vector <span class="math inline">\((0, 0, 1)\)</span></li>
				<li><span class="math display">\[R2Q(\boldsymbol{\theta}) = (\cos(\| \boldsymbol{\theta} \| / 2),
						\sin(\| \boldsymbol{\theta} \| / 2) \frac{\boldsymbol{\theta}}{\| \boldsymbol{\theta} \|}
						)\]</span> is a function that convert from a local <em>rotation vector</em> <span
						class="math inline">\(\boldsymbol{\theta}\)</span> to a local quaternion rotation. The
					definition of this function come from converting <span
						class="math inline">\(\boldsymbol{\theta}\)</span> to a body-axis angle, and then to a
					quaternion.</li>
				<li><span class="math display">\[Q2R(\mathbf{q}) = (q.i*s, q.j*s, q.k*s) \]</span> is its inverse
					function where <span class="math inline">\(n = \arccos(q.w)*2\)</span> and <span
						class="math inline">\(s = n/\sin(n/2)\)</span></li>
				<li><span class="math inline">\(\Delta t\)</span> is the lapse of time between t and the next tick (t+1)
				</li>
			</ul>
			<h2 id="model">Model</h2>
			<p>The drone is assumed to have rigid-body physics. It is submitted to the gravity and its own inertia. A
				rigid body is a solid body in which deformation is zero or so small it can be neglected. The distance
				between any two given points on a rigid body remains constant in time regardless of external forces
				exerted on it. This enables us to summarize the forces from the rotor as a thrust oriented in the
				direction normal to the plane formed by the 4 rotors, and an angular velocity.</p>
			<p>Those variables are sufficient to describe the evolution of our drone with rigid-body physics:</p>
			<ul>
				<li><span class="math inline">\(\mathbf{a}\)</span> the total acceleration in the fixed world frame</li>
				<li><span class="math inline">\(\mathbf{v}\)</span> the velocity in the fixed world frame</li>
				<li><span class="math inline">\(\mathbf{p}\)</span> the position in the fixed world frame</li>
				<li><span class="math inline">\(\boldsymbol{\omega}\)</span> the angular velocity</li>
				<li><span class="math inline">\(\mathbf{q}\)</span> the attitude in the fixed world frame</li>
			</ul>
			<h2 id="sensors">Sensors</h2>
			<p>The sensors at the drone’s disposition are:</p>
			<ul>
				<li>
					<p><strong>Accelerometer</strong>: It generates <span class="math inline">\(\mathbf{a_A}\)</span> a
						measurement of the total acceleration in the body frame referential the drone is submitted to at
						a <strong>high</strong> sampling rate. If the object is submitted to no acceleration then the
						accelerometer measure the earth’s gravity field. From that information, it could be possible to
						retrieve the attitude. Unfortunately, we are in a highly dynamic setting. Thus, it is possible
						when we can subtract the drone’s acceleration from the thrust to the total acceleration. This
						would require to know exactly the force exerted by the rotors at each instant. In this work, we
						assume that doing that separation, while being theoretically possible, is too impractical. The
						measurements model is: <span class="math display">\[\mathbf{a_A}(t) =
							\mathbf{R}_{f2b}\{\mathbf{q}(t)\}\mathbf{a}(t) + \mathbf{a_A}^\epsilon\]</span> where the
						covariance matrix of the noise of the accelerometer is <span
							class="math inline">\({\mathbf{R}_{\mathbf{a_A}}}_{3 \times 3}\)</span> and <span
							class="math display">\[\mathbf{a_A}^\epsilon \sim \mathcal{N}(\mathbf{0},
							\mathbf{R}_{\mathbf{a_A}})\]</span>.</p>
				</li>
				<li>
					<p><strong>Gyroscope</strong>:It generates <span
							class="math inline">\(\mathbf{\boldsymbol{\omega}_G}\)</span> a measurement of the angular
						velocity in the body frame of the drone at the last timestep at a <strong>high</strong> sampling
						rate. The measurement model is: <span class="math display">\[\mathbf{\boldsymbol{\omega}_G}(t) =
							\boldsymbol{\omega} + \mathbf{\boldsymbol{\omega}_G}^\epsilon\]</span> where the covariance
						matrix of the noise of the accelerometer is <span
							class="math inline">\({\mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}}}_{3 \times 3}\)</span>
						and <span class="math display">\[\mathbf{\boldsymbol{\omega}_G}^\epsilon_t \sim
							\mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}})\]</span>.</p>
				</li>
				<li>
					<p><strong>Position</strong>: It generates <span class="math inline">\(\mathbf{p_V}\)</span> a
						measurement of the current position at a <strong>low</strong> sampling rate. This is usually
						provided by a <strong>Vicon</strong> (for indoor), <strong>GPS</strong>, a
						<strong>Tango</strong> or any other position sensor. The measurement model is: <span
							class="math display">\[\mathbf{p_V}(t) = \mathbf{p}(t) + \mathbf{p_V}^\epsilon\]</span>
						where the covariance matrix of the noise of the position is <span
							class="math inline">\({\mathbf{R}_{\mathbf{p_V}}}_{3 \times 3}\)</span> and <span
							class="math display">\[\mathbf{p_V}^\epsilon \sim \mathcal{N}(\mathbf{0},
							\mathbf{R}_{\mathbf{p_V}})\]</span>.</p>
				</li>
				<li>
					<p><strong>Attitude</strong>: It generates <span class="math inline">\(\mathbf{q_V}\)</span> a
						measurement of the current attitude. This is usually provided in addition to the position by a
						<strong>Vicon</strong> or a <strong>Tango</strong> at a <strong>low</strong> sampling rate or
						the <strong>Magnemoter</strong> at a <strong>high</strong> sampling rate if the environment
						permits it (no high magnetic interference nearby like iron contamination). The magnetometer
						retrieves the attitude by assuming that the sensed magnetic field corresponds to the earth’s
						magnetic field. The measurement model is: <span class="math display">\[\mathbf{q_V}(t) =
							\mathbf{q}(t)*R2Q(\mathbf{q_V}^\epsilon)\]</span> where the <span class="math inline">\(3
							\times 3\)</span> covariance matrix of the noise of the attitude in radian before being
						converted by <span class="math inline">\(R2Q\)</span> is <span
							class="math inline">\({\mathbf{R}_{\mathbf{q_V}}}_{3 \times 3}\)</span> and <span
							class="math display">\[\mathbf{q_V}^\epsilon \sim \mathcal{N}(\mathbf{0},
							\mathbf{R}_{\mathbf{q_V}})\]</span>.</p>
				</li>
				<li>
					<p><strong>Optical Flow</strong>: A camera that keeps track of the movement by comparing the
						difference of the position of some reference points. By using a companion distance sensor, it is
						able to retrieve the difference between the two perspective and thus the change in angle and
						position. <span class="math display">\[\mathbf{dq_O}(t) =
							(\mathbf{q}(t-k)\mathbf{q}(t))*R2Q(\mathbf{dq_O}^\epsilon)\]</span> <span
							class="math display">\[\mathbf{dp_O}(t) = (\mathbf{p}(t) - \mathbf{p}(t-k)) +
							\mathbf{dp_O}^\epsilon\]</span></p>
				</li>
			</ul>
			<p>where the <span class="math inline">\(3 \times 3\)</span> covariance matrix of the noise of the attitude
				variation in radian before being converted by <span class="math inline">\(R2Q\)</span> is <span
					class="math inline">\({\mathbf{R}_{\mathbf{dq_O}}}_{3 \times 3}\)</span> and <span
					class="math display">\[\mathbf{dq_O}^\epsilon \sim \mathcal{N}(\mathbf{0},
					\mathbf{R}_{\mathbf{dq_O}})\]</span> and the position variation covariance matrix <span
					class="math inline">\({\mathbf{R}_{\mathbf{dp_O}}}_{3 \times 3}\)</span> and <span
					class="math display">\[\mathbf{dp_O}^\epsilon \sim \mathcal{N}(\mathbf{0},
					\mathbf{R}_{\mathbf{dp_O}})\]</span>.</p>
			<div class="figure">
				<img src="opflow.jpg" alt="Optical flow from a moving drone" />
				<p class="caption">Optical flow from a moving drone</p>
			</div>
			<p>The notable difference with the position or attitude sensor is that the optical flow sensor, like the
				IMU, only captures time variation, not absolute values.</p>
			<ul>
				<li><strong>Altimeter</strong>: An altimeter is a sensor that measure the altitude of the drone. For
					instance a LIDAR measure the time for the laser wave to reflect on a surface that is assumed to be
					the ground. A smart strategy is to only use the altimeter which is oriented with a low angle to the
					earth, else you also have to account that angle in the estimation of the altitude. <span
						class="math display">\[z_A(t) = \sin(\text{pitch}(\mathbf{q(t)}))(\mathbf{p}(t).z +
						z_A^\epsilon)\]</span> <span class="math inline">\({R_{z_A}}_{3 \times 3}\)</span> and <span
						class="math display">\[z_A^\epsilon \sim \mathcal{N}(0, R_{z_A})\]</span></li>
			</ul>
			<div class="figure">
				<img src="altimeter.jpg" alt="Rendering of the LIDAR laser of an altimeter" />
				<p class="caption">Rendering of the LIDAR laser of an altimeter</p>
			</div>
			<p>Some sensors are more relevant indoor and some others outdoor:</p>
			<ul>
				<li><strong>Indoor</strong>: The sensors available indoor are the accelerometer, the gyroscope and the
					<strong>Vicon</strong>. The Vicon is a system composed of many sensors around a room that is able to
					track very accurately the position and orientation a mobile object. One issue with relying solely on
					the <strong>Vicon</strong> is that the sampling rate is low.</li>
			</ul>
			<div class="figure">
				<img src="vicon.jpg" alt="A Vicon setup" />
				<p class="caption">A Vicon setup</p>
			</div>
			<ul>
				<li><strong>Outdoor</strong>: The sensors available outdoor are the accelerometer, the gyroscope, the
					magnetometer, two GPS, an optical flow and an altimeter.</li>
			</ul>
			<p>We assume that since the biases of the sensor could be known prior to the flight, both the sensor output
				measurements have been calibrated with no bias. Some filters like the <a
					href="https://dev.px4.io/en/tutorials/tuning_the_ecl_ekf.html">ekf2</a> of the px4 flight stack keep
				track of the sensor biases but this is a state augmentation that was not deemed worthwhile.</p>
			<h2 id="control-inputs">Control inputs</h2>
			<p>Observations from the control input are not strictly speaking measurements but input of the
				state-transition model. The IMU is a sensor, thus strictly speaking, its measurements are not control
				inputs. However, in the literature, it is standard to use its measurements as control inputs. One of the
				advantage is that the IMU measures exactly the data we need for a prediction through the model dynamic.
				If we used instead a transformation of the thrust sent as command to the rotors, we would have to
				account for the rotors imprecision, the wind and other disturbances. Another advantage is that since the
				IMU has very high sampling rate, we can update very frequently the state with new transitions. The
				drawback is that the accelerometer is noisy. Fortunately, we can take into account the noise as a
				process model noise.</p>
			<p>The control inputs at disposition are:</p>
			<ul>
				<li><strong>Acceleration</strong>: <span class="math inline">\(\mathbf{a_A}_t\)</span> from the
					acceloremeter</li>
				<li><strong>Angular velocity</strong>: <span
						class="math inline">\(\mathbf{\boldsymbol{\omega}_G}_t\)</span> from the gyroscope.</li>
			</ul>
			<h2 id="model-dynamic">Model dynamic</h2>
			<ul>
				<li><span class="math inline">\(\mathbf{a}(t+1) = \mathbf{R}_{b2f}\{\mathbf{q}(t+1)\}(\mathbf{a_A}_t +
						\mathbf{a_A}^\epsilon_t)\)</span> where <span class="math inline">\(\mathbf{a}^\epsilon_t \sim
						\mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{a}_t })\)</span></li>
				<li><span class="math inline">\(\mathbf{v}(t+1) = \mathbf{v}(t) + \Delta t \mathbf{a}(t) +
						\mathbf{v}^\epsilon_t\)</span> where <span class="math inline">\(\mathbf{v}^\epsilon_t \sim
						\mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{v}_t })\)</span></li>
				<li><span class="math inline">\(\mathbf{p}(t+1) = \mathbf{p}(t) + \Delta t \mathbf{v}(t) +
						\mathbf{p}^\epsilon_t\)</span> where <span class="math inline">\(\mathbf{p}^\epsilon_t \sim
						\mathcal{N}(\mathbf{0}, \mathbf{Q}_{\mathbf{p}_t })\)</span></li>
				<li><span class="math inline">\(\boldsymbol{\omega}(t+1) = \mathbf{\boldsymbol{\omega}_G}_t +
						\mathbf{\boldsymbol{\omega}_G}^\epsilon_t\)</span> where <span
						class="math inline">\(\mathbf{p}^\epsilon_t \sim \mathcal{N}(\mathbf{0},
						\mathbf{Q}_{\mathbf{\boldsymbol{\omega}_G}_t })\)</span></li>
				<li><span class="math inline">\(\mathbf{q}(t+1) = \mathbf{q}(t)*R2Q(\Delta t \boldsymbol{ \omega(t)
						})\)</span></li>
			</ul>
			<p>Note that in our model, <span class="math inline">\(\mathbf{q}(t+1)\)</span> must be known. Fortunately,
				as we will see later, our Rao-Blackwellized Particle Filter is conditioned under the attitude so it is
				known.</p>
			<h2 id="state">State</h2>
			<p>The time series of the variables of our dynamic model constitute a hidden Markov chain. Indeed, the model
				is “memoryless” and depends only on the current state and a sampled transition.</p>
			<p>States contain variables that enable us to keep track of some of those hidden variables which is our
				ultimate goal (for POSE <span class="math inline">\(\mathbf{p}\)</span> and <span
					class="math inline">\(\mathbf{q}\)</span>). States at time <span class="math inline">\(t\)</span>
				are denoted by <span class="math inline">\(\mathbf{x}_t\)</span>. Different filters require different
				state variables depending on their structure and assumptions.</p>
			<h2 id="observation">Observation</h2>
			<p>Observations are revealed variables conditioned under the variables of our dynamic model. Our ultimate
				goal is to deduce the states from the observations.</p>
			<p>Observations contain the control input <span class="math inline">\(\mathbf{u}\)</span> and the
				measurements <span class="math inline">\(\mathbf{z}\)</span>.</p>
			<p><span class="math display">\[\mathbf{y}_t = (\mathbf{z}_t, \mathbf{u}_t)^T = (\mathbf{p_V}_t,
					\mathbf{q_V}_t), ({t_C}_t, \mathbf{\boldsymbol{\omega}_C}_t))^T\]</span></p>
			<h2 id="filtering-and-smoothing">Filtering and smoothing</h2>
			<p><strong>Smoothing</strong> is the statistical task of finding the expectation of the state variable from
				the past history of observations and multiple observation variables ahead</p>
			<p><span class="math display">\[\mathbb{E}[g(\mathbf{x}_{0:t}) | \mathbf{y}_{1:t+k}]\]</span></p>
			<p>Which expand to,</p>
			<p><span class="math display">\[\mathbb{E}[(\mathbf{p}_{0:t}, \mathbf{q}_{0:t}) | (\mathbf{z}_{1:t+k},
					\mathbf{u}_{1:t+k})]\]</span></p>
			<p><span class="math inline">\(k\)</span> is a contant and the first observation is <span
					class="math inline">\(y_1\)</span></p>
			<p><strong>Filtering</strong> is a kind of smoothing where you only have at disposal the current observation
				variable (<span class="math inline">\(k=0\)</span>)</p>
			<h2 id="complementary-filter">Complementary Filter</h2>
			<p>The complementary filter is the simplest of all filters and is commonly used to retrieve the attitude
				because of its low computational complexity. The gyroscope and accelerometer both provide a measurement
				that can help us to estimate the attitude. Indeed, the gyroscope reads noisy measurement of the angular
				velocity from which we can retrieve the new attitude from the past one by time integration: <span
					class="math inline">\(\mathbf{q}_t = \mathbf{q}_{t-1}*R2Q(\Delta t \mathbf{\omega})\)</span>.</p>
			<p>This is commonly called “Dead reckoning”<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>
				and is prone to accumulation error, referred to as drift. Indeed, like Brownian motions, even if the
				process is unbiased, the variance grows with time. Reducing the noise cannot solve the issue entirely:
				even with extremely precise instruments, you are subject to floating-point errors.</p>
			<p>Fortunately, even though the accelerometer gives us a highly noisy (vibrations, wind, etc … ) measurement
				of the orientation, it is not impacted by the effects of drifting because it does not rely on
				accumulation. Indeed, if not subject to other accelerations, the accelerometer measures the gravity
				field orientation. Since this field is oriented toward earth, it is possible to retrieve the current
				rotation from that field and by extension the attitude. However, a drone is under the influence of
				continuous and significant acceleration and vibration. Hence, the assumption that we retrieve the
				gravity field directly is wrong. Nevertheless, we could solve this by subtracting the acceleration
				deduced from the thrust control input. It is unpractical so this approach is not pursued in this work,
				but understanding this filter is still useful.</p>
			<p>The idea of the filter itself is to combine the precise “short-term” measurements of the gyroscope
				subject to drift with the “long-term” measurements of the accelerometer.</p>
			<h3 id="state-1">State</h3>
			<p>This filter is very simple. The only requirement is that the last estimated attitude must be stored along
				with its timestamp in order to calculate <span class="math inline">\(\Delta t\)</span>. <span
					class="math display">\[\mathbf{x}_t = \mathbf{q}_t\]</span> <span
					class="math display">\[\hat{\mathbf{q}}_{t+1} = \alpha (\hat{\mathbf{q}}_t + \Delta t
					\mathbf{\omega}_t) + (1 - \alpha) {\mathbf{q_A}}_{t+1}\]</span> <span class="math inline">\(\alpha
					\in [0, 1]\)</span>. Usually, <span class="math inline">\(\alpha\)</span> is set to a high-value
				like <span class="math inline">\(0.98\)</span>. It is very intuitive to see why this should
				approximately “work”, the data from the accelerometer continuously corrects the drift from the
				gyroscope.</p>
			<pre class="graph"><code>┌──────┐ ┌───────────────────────────────────────────┐                         
│      │ │                                           │                         
│      │&lt;┘┌───────────────────────────┐  ┌────────┐  │                         
│      ├──┘                           │  │        │  │ ┌─────────┐             
│Buffer│     ┌─────┐    ┌───────┐     └─&gt;│        │  │ │         │             
│      │     │     │    │       │        │Rotation│  │ │         │  ┌─────────┐
│      ├────&gt;│     ├───&gt;│BR2Quat├───────&gt;│        │  └─┤         │  │         │
│      │     │Integ│    │       │        │        ├───&gt;│Combining├─&gt;│Block out│
└──────┘  ┌─&gt;│     │    └───────┘        └────────┘    │         │  │         │
          │  │     │                                ┌─&gt;│         │  └─────────┘
┌───────┐ │  └─────┘┌────────────────┐   ┌────────┐ │  │         │             
│       │ │         │                │   │        │ │  └─────────┘             
│       ├─┘         │┌─────────────┐ └──&gt;│        │ │                          
│Map IMU├───────────┘│             │     │ACC2Quat├─┘                          
│       │            │Map CI Thrust├────&gt;│        │                            
│       │            │             │     │        │                            
└───────┘            └─────────────┘     └────────┘                            </code></pre>
			<div class="figure">
				<img src="empty.jpg" alt="Complementary Filter graph structure" />
				<p class="caption">Complementary Filter graph structure</p>
			</div>
			<p>Figure 9 is the plot of the distance from the true quaternion after 15s of an arbitrary trajectory when
				<span class="math inline">\(\alpha = 1.0\)</span> meaning that the accelerometer does not correct the
				drift.</p>
			<div class="figure">
				<img src="cf100.png" alt="CF with alpha = 1.0" />
				<p class="caption">CF with alpha = 1.0</p>
			</div>
			<p>Figure 10 is that same trajectory with <span class="math inline">\(\alpha = 0.98\)</span>.</p>
			<div class="figure">
				<img src="cf098.png" alt="CF with alpha = 0.98" />
				<p class="caption">CF with alpha = 0.98</p>
			</div>
			<p>We can observe here the long-term importance of being able to correct the drift, even if ever so slightly
				at each timestep.</p>
			<h2 id="asynchronous-augmented-complementary-filter">Asynchronous Augmented Complementary Filter</h2>
			<p>As explained previously, in this highly-dynamic setting, combining the gyroscope and the accelerometer to
				retrieve the attitude is not satisfactory. However, we can reuse the intuition from the complementary
				filter, which is to combine precise but drifting short-term measurements to other measurements that do
				not suffer from drifting. This enables a simple and computationally inexpensive novel filter that we
				will be able to use later as a baseline. In this context, the short-term measurements are the
				acceleration and angular velocity from the IMU, and the non-drifting measurements are the position and
				attitude from the Vicon.</p>
			<p>We will also add the property that the data from the sensors are asynchronous. As with all following
				filters, we deal with asynchronicity by updating the state to the most likely state so far for any new
				sensor measurement incoming. This is a consequence of the sensors having different sampling rate.</p>
			<ul>
				<li>
					<p><strong>IMU</strong> update <span class="math display">\[\mathbf{v}_t = \mathbf{v}_{t-1} + \Delta
							t_v \mathbf{a_A}_t\]</span> <span class="math display">\[\boldsymbol{\omega}_t =
							\boldsymbol{\mathbf{\omega_G}}_t\]</span> <span class="math display">\[\mathbf{p}_t =
							\mathbf{p}_{t-1} + \Delta t \mathbf{v}_{t-1}\]</span> <span
							class="math display">\[\mathbf{q}_t = \mathbf{q}_{t-1}R2Q(\Delta t
							\boldsymbol{\omega}_{t-1})\]</span></p>
				</li>
				<li>
					<p><strong>Vicon</strong> update <span class="math display">\[\mathbf{p}_t = \alpha \mathbf{p_V} +
							(1 - \alpha) (\mathbf{p}_{t-1} + \Delta t \mathbf{v}_{t-1})\]</span> <span
							class="math display">\[\mathbf{q}_t = \alpha \mathbf{q_V} + (1 - \alpha)
							(\mathbf{q}_{t-1}R2Q(\Delta t \boldsymbol{\omega}_{t-1}))\]</span></p>
				</li>
			</ul>
			<h3 id="state-2">State</h3>
			<p>The state has to be more complex because the filter now estimates both the position and the attitude.
				Furthermore, because of asynchronicity, we have to store the last angular velocity, the last linear
				velocity, and the last time the linear velocity has been updated (to retrieve <span
					class="math inline">\(\Delta t_v = t - t_a\)</span> where <span class="math inline">\(t_a\)</span>
				is the last time we had an update from the accelerometer).</p>
			<p><span class="math display">\[\mathbf{x}_t = (\mathbf{p}_t, \mathbf{q}_t, \boldsymbol{\omega}_t,
					\mathbf{a}_t, t_a)\]</span></p>
			<p>The structure of this filter and all of the filters presented thereafter is as follow:</p>
			<pre class="graph"><code> ┌───────┐                          ┌──────┐   ┌─────┐  ┌─────────┐
 │       │                          │      │   │     │  │         │
 │Map IMU├─┐   ┌─────┐  ┌───────┐   │      ├──&gt;│P &amp; Q├─&gt;│Block out│
 │       │ │   │     │  │       ├──&gt;│Update│   │     │  │         │
 └───────┘ └──&gt;│     │  │       │   │      ├─┐ └─────┘  └─────────┘
               │Merge├─&gt;│ZipLast│   │      │ │                     
┌─────────┐ ┌─&gt;│     │  │       │&lt;┐ └──────┘ │           ┌──────┐  
│         │ │  │     │  │       │ │          │           │      │  
│Map Vicon├─┘  └─────┘  └───────┘ │          │           │      │  
│         │                       │          └──────────&gt;│Buffer│  
└─────────┘                       └──────────────────────┤      │  
                                                         │      │  
                                                         └──────┘  </code></pre>
			<div class="figure">
				<img src="empty.jpg" alt="A graph of the filters structure in scala-flow" />
				<p class="caption">A graph of the filters structure in scala-flow</p>
			</div>
			<h2 id="kalman-filter">Kalman Filter</h2>
			<h3 id="bayesian-inference">Bayesian inference</h3>
			<p>Bayesian inference is a method of statistical inference in which Bayes’ theorem is used to update the
				probability for a hypothesis when more evidence or information becomes available. In this Bayes setting,
				the prior is the estimated distribution of the previous state at time <span
					class="math inline">\(t-1\)</span>, the likelihood correspond to the likelihood of getting the new
				data from the sensor given the prior and finally, the posterior is the updated estimated distribution.
			</p>
			<h3 id="model-1">Model</h3>
			<p>The Kalman filter requires that both the model process and the measurement process are <strong>linear
					gaussian</strong>. Linear gaussian processes are of the form: <span
					class="math display">\[\mathbf{x}_t = f(\mathbf{x}_{t-1}) + \mathbf{w}_t\]</span> where <span
					class="math inline">\(f\)</span> is a linear function and <span
					class="math inline">\(\mathbf{w}_t\)</span> a gaussian process: it is sampled from an arbitrary
				gaussian distribution.</p>
			<p>The Kalman filter is a direct application of bayesian inference. It combines the prediction of the
				distribution given the estimated prior state and the state-transition model.</p>
			<p><span class="math display">\[\mathbf{x}_t = \mathbf{F}_t \mathbf{x}_{t-1} + \mathbf{B}_t \mathbf{u}_t +
					\mathbf{w}_t \]</span></p>
			<ul>
				<li><span class="math inline">\(\mathbf{x}_t\)</span> the state</li>
				<li><span class="math inline">\(\mathbf{F}_t\)</span> the state transition model</li>
				<li><span class="math inline">\(\mathbf{B}_t\)</span> the control-input model</li>
				<li><span class="math inline">\(\mathbf{u}_t\)</span> the control vector</li>
				<li><span class="math inline">\(\mathbf{w}_t\)</span> process noise drawn from <span
						class="math inline">\(\mathbf{w}_t \sim N(0, \mathbf{Q}_k)\)</span></li>
			</ul>
			<p>and the estimated distribution given the data coming from the sensors.</p>
			<p><span class="math display">\[\mathbf{y}_t = \mathbf{H}_t \mathbf{x}_{t} + \mathbf{v}_t \]</span></p>
			<ul>
				<li><span class="math inline">\(\mathbf{y}_t\)</span> measurements</li>
				<li><span class="math inline">\(\mathbf{H}_t\)</span> the state to measurement matrix</li>
				<li><span class="math inline">\(\mathbf{w}_t\)</span> measurement noise drawn from <span
						class="math inline">\(\mathbf{w}_t \sim N(0, \mathbf{R}_k)\)</span></li>
			</ul>
			<p>Because, both the model process and the sensor process are assumed to be linear Gaussian, we can combine
				them into a Gaussian distribution. Indeed, the product of the distribution of two Gaussian forms a new
				Gaussian distribution.</p>
			<p><span class="math display">\[P(\mathbf{x}_{t}) \propto P(\mathbf{x}^{-}_{t}|\mathbf{x}_{t-1}) \cdot
					P(\mathbf{x}_t | \mathbf{y}_t )\]</span> <span class="math display">\[\mathcal{N}(\mathbf{x}_{t})
					\propto \mathcal{N}(\mathbf{x}^{-}_{t}|\mathbf{x}_{t-1}) \cdot \mathcal{N}(\mathbf{x}_t |
					\mathbf{y}_t )\]</span></p>
			<p>where <span class="math inline">\(\mathbf{x}^{-}_{t}\)</span> is the predicted state from the previous
				state and the state-transition model.</p>
			<p>Kalman filter keeps track of the parameters of that gaussian: the mean state and the covariance of the
				state which represents the uncertainty about our last prediction. The mean of that distribution is also
				the best current state estimation of the filter.</p>
			<p>By keeping track of the uncertainty, we can optimally combine the normals by knowing what importance to
				give to the difference between the expected sensor data and the actual sensor data. That factor is the
				Kalman gain.</p>
			<ul>
				<li><strong>predict</strong>:
					<ul>
						<li>predicted <strong>state</strong>: <span class="math inline">\(\hat{\mathbf{x}}^{-}_t =
								\mathbf{F}_t \mathbf{x}_{t-1} + \mathbf{B}_t \mathbf{u}_t\)</span></li>
						<li>predicted <strong>covariance</strong>: <span class="math inline">\(\mathbf{\Sigma}^{-}_t =
								\mathbf{F}_{t-1} \mathbf{\Sigma}^{-}_{t-1} \mathbf{F}_{t-1}^T + \mathbf{Q}_t\)</span>
						</li>
					</ul>
				</li>
				<li><strong>update</strong>:
					<ul>
						<li>predicted <strong>measurements</strong>: <span class="math inline">\(\hat{\mathbf{z}} =
								\mathbf{H}_t \hat{\mathbf{x}}^{-}_t\)</span></li>
						<li><strong>innovation</strong>: <span class="math inline">\((\mathbf{z}_t -
								\hat{\mathbf{z}})\)</span></li>
						<li><strong>innovation covariance</strong>: <span class="math inline">\(\mathbf{S} =
								\mathbf{H}_t \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T + \mathbf{R}_t\)</span></li>
						<li>optimal <strong>kalman gain</strong>: <span class="math inline">\(\mathbf{K} =
								\mathbf{\Sigma}^{-}_t \mathbf{H}_t^T \mathbf{S}^{-1}\)</span></li>
						<li>updated <strong>state</strong>: <span class="math inline">\(\mathbf{\Sigma}_t =
								\mathbf{\Sigma}^-_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\)</span></li>
						<li>updated <strong>covariance</strong>: <span class="math inline">\(\hat{\mathbf{x}}_t =
								\hat{\mathbf{x}}^{-}_t + \mathbf{K}(\mathbf{z}_t - \hat{\mathbf{z}})\)</span></li>
					</ul>
				</li>
			</ul>
			<h2 id="asynchronous-kalman-filter">Asynchronous Kalman Filter</h2>
			<p>It is not necessary to apply the full Kalman update at each measurement. Indeed, <span
					class="math inline">\(\mathbf{H}\)</span> can be sliced to correspond to the measurements currently
				available.</p>
			<p>To be truly asynchronous, you also have to account for the different sampling rates. There are two cases
				:</p>
			<ul>
				<li>The required data for the update step (the control inputs) can arrive multiple times before any of
					the data of the update step (the measurements) occur.</li>
				<li>Inversely, it is possible that the measurements occur at a higher sampling rate than the control
					inputs.</li>
			</ul>
			<p>The strategy chosen here is as follows:</p>
			<ol style="list-style-type: decimal">
				<li>Multiple prediction steps without any update step may happen without making the algorithm
					inconsistent.</li>
				<li>An update is <strong>always</strong> immediately preceded by a prediction step. This is a
					consequence of the requirement that the innovation must measure the difference between the predicted
					measurement from the state at the exact current time and the measurements. Thus, if the measurements
					are not synchronized with the control inputs, use the most likely control input for the prediction
					step. Repeating the last control input was the method used for the accelerometer and the gyroscope
					data as control input.</li>
			</ol>
			<h2 id="extended-kalman-filters">Extended Kalman Filters</h2>
			<p>In the previous section, we have shown that the Kalman Filter is only applicable when both the process
				model and the measurement model are linear Gaussian processes.</p>
			<ul>
				<li>The noise of the measurements and of the state-transition must be Gaussian</li>
				<li>The state-transition function and the measurement to state function must be linear.</li>
			</ul>
			<p>Furthermore, it is provable that Kalman filters are optimal linear filters.</p>
			<p>However, in our context, one component of the state, the attitude, is intrinsically non-linear. Indeed,
				rotations and attitudes belong to <span class="math inline">\(SO(3)\)</span> which is not a vector
				space. Therefore, we cannot use <em>vanilla</em> Kalman filters. The filters that we present thereafter
				relax those requirements.</p>
			<p>One example of such extension is the extended Kalman filter (EKF) that we will present here. The EKF
				relax the linearity requirement by using differentiation to calculate an approximation of the first
				order of the functions required to be linear. Our state transition function and measurement function can
				now be expressed in the free forms <span class="math inline">\(f(\mathbf{x}_t)\)</span> and <span
					class="math inline">\(h(\mathbf{x}_t)\)</span> and we define the matrix <span
					class="math inline">\(\mathbf{F}_t\)</span> and <span class="math inline">\(\mathbf{H}_t\)</span> as
				their Jacobian.</p>
			<p><span class="math display">\[{\mathbf{F}_t}_{10 \times 10} = \left . \frac{\partial f}{\partial
					\mathbf{x} } \right \vert _{\hat{\mathbf{x}}_{t-1},\mathbf{u}_{t-1}}\]</span></p>
			<p><span class="math display">\[{\mathbf{H}_t}_{7 \times 7} = \left . \frac{\partial h}{\partial \mathbf{x}
					} \right \vert _{\hat{\mathbf{x}}_{t}}\]</span></p>
			<ul>
				<li><strong>predict</strong>:
					<ul>
						<li>predicted <strong>state</strong>: <span class="math inline">\(\hat{\mathbf{x}}^{-}_t =
								f(\mathbf{x}_{t-1}) + \mathbf{B}_t \mathbf{u}_t\)</span></li>
						<li>predicted <strong>covariance</strong>: <span class="math inline">\(\mathbf{\Sigma}^{-}_t =
								\mathbf{F}_{t-1} \mathbf{\Sigma}^{-}_{t-1} \mathbf{F}_{t-1}^T + \mathbf{Q}_t\)</span>
						</li>
					</ul>
				</li>
				<li><strong>update</strong>:
					<ul>
						<li>predicted <strong>measurements</strong>: <span class="math inline">\(\hat{\mathbf{z}} =
								h(\hat{\mathbf{x}}^{-}_t)\)</span></li>
						<li><strong>innovation</strong>: <span class="math inline">\((\mathbf{z}_t -
								\hat{\mathbf{z}})\)</span></li>
						<li><strong>innovation covariance</strong>: <span class="math inline">\(\mathbf{S} =
								\mathbf{H}_t \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T + \mathbf{R}_t\)</span></li>
						<li>optimal <strong>kalman gain</strong>: <span class="math inline">\(\mathbf{K} =
								\mathbf{\Sigma}^{-}_t \mathbf{H}_t^T \mathbf{S}^{-1}\)</span></li>
						<li>updated <strong>state</strong>: <span class="math inline">\(\mathbf{\Sigma}_t =
								\mathbf{\Sigma}^-_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\)</span></li>
						<li>updated <strong>covariance</strong>: <span class="math inline">\(\hat{\mathbf{x}}_t =
								\hat{\mathbf{x}}^{-}_t + \mathbf{K}(\mathbf{z}_t - \hat{\mathbf{z}})\)</span></li>
					</ul>
				</li>
			</ul>
			<h3 id="ekf-for-pose">EKF for POSE</h3>
			<h3 id="state-3">State</h3>
			<p>For the EKF, we will use the following state:</p>
			<p><span class="math display">\[\mathbf{x}_t = (\mathbf{v}_t, \mathbf{p}_t, \mathbf{q}_t)^T\]</span></p>
			<p>Initial state <span class="math inline">\(\mathbf{x}_0\)</span> at <span
					class="math inline">\((\mathbf{0}, \mathbf{0}, (1, 0, 0, 0))\)</span></p>
			<h3 id="indoor-measurements-model">Indoor Measurements model</h3>
			<ol style="list-style-type: decimal">
				<li>Position: <span class="math display">\[\mathbf{p_V}(t) = \mathbf{p}(t)^{(i)} +
						\mathbf{p_V}^\epsilon_t\]</span> where <span class="math inline">\(\mathbf{p_V}^\epsilon_t \sim
						\mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{p_V}_t })\)</span></li>
				<li>Attitude: <span class="math display">\[\mathbf{q_V}(t) =
						\mathbf{q}(t)^{(i)}*R2Q(\mathbf{q_V}^\epsilon_t)\]</span> where <span
						class="math inline">\(\mathbf{q_V}^\epsilon_t \sim \mathcal{N}(\mathbf{0},
						\mathbf{R}_{\mathbf{q_V}_t })\)</span></li>
			</ol>
			<h3 id="kalman-prediction">Kalman prediction</h3>
			<p>The model dynamic defines the following model, state-transition function <span
					class="math inline">\(f(\mathbf{x}, \mathbf{u})\)</span> and process noise <span
					class="math inline">\(\mathbf{w}\)</span> with covariance matrix <span
					class="math inline">\(\mathbf{Q}\)</span></p>
			<p><span class="math display">\[\mathbf{x}_t = f(\mathbf{x}_{t-1}, \mathbf{u}_t) + \mathbf{w}_t\]</span></p>
			<p><span class="math display">\[f((\mathbf{v}, \mathbf{p}, \mathbf{q}), (\mathbf{a_A},
					\mathbf{\boldsymbol{\omega}_G})) = \left( \begin{array}{c}
					\mathbf{v} + \Delta t \mathbf{R}_{b2f}\{\mathbf{q}_{t-1}\} \mathbf{a} \\
					\mathbf{p} + \Delta t \mathbf{v} \\
					\mathbf{q}*R2Q({\Delta t} \boldsymbol{\omega}_G)
					\end{array} \right)\]</span></p>
			<p>Now, we need to derive the jacobian of <span class="math inline">\(f\)</span>. We will use sagemath to
				retrieve the 28 relevant different partial derivatives of <span class="math inline">\(q\)</span>.</p>
			<p><span class="math display">\[{\mathbf{F}_t}_{10 \times 10} = \left . \frac{\partial f}{\partial
					\mathbf{x} } \right \vert _{\hat{\mathbf{x}}_{t-1},\mathbf{u}_{t-1}}\]</span></p>
			<p><span class="math display">\[\hat{\mathbf{x}}^{-(i)}_t = f(\mathbf{x}^{(i)}_{t-1}, \mathbf{u}_t)\]</span>
				<span class="math display">\[\mathbf{\Sigma}^{-(i)}_t = \mathbf{F}_{t-1} \mathbf{\Sigma}^{-(i)}_{t-1}
					\mathbf{F}_{t-1}^T + \mathbf{Q}_t\]</span></p>
			<h3 id="kalman-measurements-update">Kalman measurements update</h3>
			<p><span class="math display">\[\mathbf{z}_t = h(\mathbf{x}_t) + \mathbf{v}_t\]</span></p>
			<p>The <a href="#measurements-model">measurement model</a> defines <span
					class="math inline">\(h(\mathbf{x})\)</span></p>
			<p><span class="math display">\[\left( \begin{array}{c}
					\mathbf{p_V}\\
					\mathbf{q_V}\\
					\end{array} \right) = h((\mathbf{v}, \mathbf{p}, \mathbf{q})) = \left( \begin{array}{c}
					\mathbf{p}\\
					\mathbf{q}\\
					\end{array} \right)\]</span></p>
			<p>The only complex partial derivatives to calculate are the ones of the acceleration, because they have to
				be rotated first. Once again, we use sagemath: <span class="math inline">\(\mathbf{H_a}\)</span> is
				defined by the script in the appendix B.</p>
			<p><span class="math display">\[{\mathbf{H}_t}_{10 \times 7} = \left . \frac{\partial h}{\partial \mathbf{x}
					} \right \vert _{\hat{\mathbf{x}}_{t}} = \left( \begin{array}{ccc}
					\mathbf{0}_{3 \times 3} &amp; &amp; \\
					&amp; \mathbf{I}_{3 \times 3} &amp; \\
					&amp; &amp; \mathbf{I}_{4 \times 4}\\
					\end{array} \right)\]</span></p>
			<p><span class="math display">\[{\mathbf{R}_t}_{7 \times 7} =
					\left( \begin{array}{cc}
					\mathbf{R}_{\mathbf{p_V}} &amp; \\
					&amp; {\mathbf{R}'_{\mathbf{q_V}}}_{4 \times 4}\\
					\end{array} \right)\]</span></p>
			<p><span class="math inline">\(\mathbf{R}'_{\mathbf{q_V}}\)</span> has to be <span class="math inline">\(4
					\times 4\)</span> and has to represent the covariance of the quaternion. However, the actual
				covariance matrix <span class="math inline">\(\mathbf{R}_{\mathbf{q_V}}\)</span> is <span
					class="math inline">\(3 \times 3\)</span> and represent the noise in terms of a <em>rotation
					vector</em> around the x, y, z axes.</p>
			<p>We transform this rotation vector into a quaternion using our function <span
					class="math inline">\(R2Q\)</span>. We can compute the new covariance matrix <span
					class="math inline">\(\mathbf{R}'_{\mathbf{q_V}}\)</span> using Unscented Transform.</p>
			<h3 id="unscented-transform">Unscented Transform</h3>
			<p>The unscented transform (UT) is a mathematical function used to estimate statistics after applying a
				given nonlinear transformation to a probability distribution. The idea is to use points that are
				representative of the original distribution, sigma points. We apply the transformation to those sigma
				points and calculate new statistics using the transformed sigma points. The sigma points must have the
				same mean and covariance as the original distribution.</p>
			<p>The minimal set of symmetric sigma points can be found using the covariance of the initial distribution.
				The <span class="math inline">\(2N + 1\)</span> minimal symmetric set of sigma points are the mean and
				the set of points corresponding to the mean plus and minus one of the direction corresponding to the
				covariance matrix. In one dimension, the square root of the variance is enough. In N-dimensions, you
				must use the Cholesky decomposition of the covariance matrix. The Cholesky decomposition finds the
				matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(\Sigma =
					LL^t\)</span>.</p>
			<div class="figure">
				<img src="unscented.jpg" alt="Unscented tranformation" />
				<p class="caption">Unscented tranformation</p>
			</div>
			<h3 id="kalman-update">Kalman update</h3>
			<p><span class="math display">\[\mathbf{S} = \mathbf{H}_t \mathbf{\Sigma}^{-}_t \mathbf{H}_t^T +
					\mathbf{R}_t\]</span> <span class="math display">\[\hat{\mathbf{z}} =
					h(\hat{\mathbf{x}}^{-}_t)\]</span> <span class="math display">\[\mathbf{K} = \mathbf{\Sigma}^{-}_t
					\mathbf{H}_t^T \mathbf{S}^{-1}\]</span> <span class="math display">\[\mathbf{\Sigma}_t =
					\mathbf{\Sigma}^-_t + \mathbf{K} \mathbf{S} \mathbf{K}^T\]</span> <span
					class="math display">\[\hat{\mathbf{x}}_t = \hat{\mathbf{x}}^{-}_t + \mathbf{K}(\mathbf{z}_t -
					\hat{\mathbf{z}})\]</span></p>
			<h3 id="f-partial-derivatives">F partial derivatives</h3>
			<div class="sourceCode">
				<pre class="sourceCode python"><code class="sourceCode python">Q.<span class="op">&lt;</span>i,j,k<span class="op">&gt;</span> <span class="op">=</span> QuaternionAlgebra(SR, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)

var(<span class="st">'q0, q1, q2, q3'</span>)
var(<span class="st">'dt'</span>)
var(<span class="st">'wx, wy, wz'</span>)

q <span class="op">=</span> q0 <span class="op">+</span> q1<span class="op">*</span>i <span class="op">+</span> q2<span class="op">*</span>j <span class="op">+</span> q3<span class="op">*</span>k

w <span class="op">=</span> vector([wx, wy, wz])<span class="op">*</span>dt
w_norm <span class="op">=</span> sqrt(w[<span class="dv">0</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> w[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span> w[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>)
ang <span class="op">=</span> w_norm<span class="op">/</span><span class="dv">2</span>
w_normalized <span class="op">=</span> w<span class="op">/</span>w_norm
sin2 <span class="op">=</span> sin(ang)
qd <span class="op">=</span> cos(ang) <span class="op">+</span> w_normalized[<span class="dv">0</span>]<span class="op">*</span>sin2<span class="op">*</span>i <span class="op">+</span> w_normalized[<span class="dv">1</span>]<span class="op">*</span>sin2<span class="op">*</span>j 
	<span class="op">+</span> w_normalized[<span class="dv">2</span>]<span class="op">*</span>sin2<span class="op">*</span>k

nq <span class="op">=</span> q<span class="op">*</span>qd

v <span class="op">=</span> vector(nq.coefficient_tuple())

<span class="cf">for</span> sym <span class="kw">in</span> [wx, wy, wz, q0, q1, q2, q3]:
    d <span class="op">=</span> diff(v, sym)
    exps <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> x: x.canonicalize_radical().full_simplify(), d)
    <span class="cf">for</span> i, e <span class="kw">in</span> <span class="bu">enumerate</span>(exps):
        <span class="bu">print</span>(sym, i, e) 
		</code></pre>
			</div>
			<h2 id="unscented-kalman-filters">Unscented Kalman Filters</h2>
			<p>The EKF has three flaws in our case:</p>
			<ul>
				<li>The linearization gives an approximate form which result in approximation errors</li>
				<li>The prediction step of the EKF assumes that the linearized form of the transformation can capture
					all the information needed to apply the transformation to the gaussian distribution
					pre-transformation. Unfortunately, this is only true near the region of the mean. The transformation
					of the tail of the gaussian distribution may need to be very different.</li>
				<li>It attempts to define a Gaussian covariance matrix for the attitude quaternion. This does not make
					sense because it does not account for the requirement of the quaternion being in a 4 dimensional
					unit sphere.</li>
			</ul>
			<p>The Unscented Kalman Filter (UKF) does not suffer from the two first flaws, but it is more
				computationally expensive as it requires a Cholesky factorisation that grows exponentially in complexity
				with the number of dimensions.</p>
			<p>Indeed, the UKF applies an unscented transformation to sigma points of the current approximated
				distribution. The statistics of the new approximated Gaussian are found through this unscented
				transform. The EKF linearizes the transformation, the UKF approximates the resulting Gaussian after the
				transformation. Hence, the UKF can take into account the effects of the transformation away from the
				mean which might be drastically different.</p>
			<p>The implementation of an UKF still suffers greatly from quaternions not belonging to a vector space. The
				approach taken by <span class="citation">[<a href="#ref-edgar_quaternion-based_nodate">3</a>]</span> is
				to use the error quaternion defined by <span class="math inline">\(\mathbf{e}_i =
					\mathbf{q}_i\bar{\mathbf{q}}\)</span>. This approach has the advantage that similar quaternion
				differences result in similar error. But apart from that, it does not have any profound justification.
				We must compute a sound average weighted quaternion of all sigma points. An algorithm is described in
				the following section.</p>
			<h3 id="average-quaternion">Average quaternion</h3>
			<p>Unfortunately, the average of quaternions components <span class="math inline">\(\frac{1}{N} \sum
					q_i\)</span> or <em>barycentric</em> mean is unsound: Indeed, attitudes do not belong to a vector
				space but a homogenous Riemannian manifold (the four dimensional unit sphere). To convince yourself of
				the unsoundness of the <em>barycentric</em> mean, see that the addition and barycentric mean of two unit
				quaternions is not necessarily a unit quaternion (<span class="math inline">\((1, 0, 0, 0)\)</span> and
				<span class="math inline">\((-1, 0, 0, 0)\)</span> for instance. Furthermore, angles being periodic, the
				<em>barycentric</em> mean of a quaternion with angle <span class="math inline">\(-178^\circ\)</span> and
				another with same body-axis and angle <span class="math inline">\(180^\circ\)</span> gives <span
					class="math inline">\(1^\circ\)</span> instead of the expected <span
					class="math inline">\(-179^\circ\)</span>.</p>
			<p>To calculate the average quaternion, we use an algorithm which minimizes a metric that corresponds to the
				weighted attitude difference to the average, namely the weighted sum of the squared Frobenius norms of
				attitude matrix differences.</p>
			<p><span class="math display">\[\bar{\mathbf{q}} = arg \min_{q \in \mathbb{S}^3} \sum w_i \| A(\mathbf{q}) -
					A(\mathbf{q}_i) \|^2_F\]</span></p>
			<p>where <span class="math inline">\(\mathbb{S}^3\)</span> denotes the unit sphere.</p>
			<p>The attitude matrix <span class="math inline">\(A(\mathbf{q})\)</span> and its corresponding Frobenius
				norm have been described in the quaternion section.</p>
			<h3 id="intuition">Intuition</h3>
			<p>The intuition of keeping track of multiple representatives of the distribution is exactly the approach
				taken by the particle filter. The particle filter has the advantage that the distribution is never
				transformed back to a gaussian so there are fewer assumptions made about the noise and the
				transformation. It is only required to be able to calculate the expectation from a weighted set of
				particles.</p>
			<h2 id="particle-filter">Particle Filter</h2>
			<p>Particle filters are computationally expensive. This is the reason why their usage is not very popular
				currently for low-powered embedded systems like drones. However, they are used in Avionics for planes
				since the computational resources are less scarce but precision crucial. Accelerating hardware could
				widen the usage of particle filters to embedded systems.</p>
			<p>Particle filters are sequential Monte Carlo methods. Like all Monte Carlo methods, they rely on repeated
				sampling for estimation of a distribution.</p>
			<div class="figure">
				<img src="mc.gif" alt="Monte Carlo estimation of pi" />
				<p class="caption">Monte Carlo estimation of pi</p>
			</div>
			<p>The particle filter is itself a weighted particle representation of the posterior:</p>
			<p><span class="math display">\[p(\mathbf{x}) = \sum w^{(i)}\delta(\mathbf{x} - \mathbf{x}^{(i)})\]</span>
				where <span class="math inline">\(\delta\)</span> is the dirac delta function. The dirac delta function
				is zero everywhere except at zero, with an integral of one over the entire real line. It represents here
				the ideal probability density of a particle.</p>
			<h3 id="importance-sampling">Importance sampling</h3>
			<p>Weights are computed through importance sampling. With importance sampling, each particle does not
				equally represent the distribution. Importance sampling enables us to use sampling from another
				distribution to estimate properties from the target distribution of interest. In most cases, it can be
				used to focus sampling on a specific region of the distribution. In our case, by choosing the right
				importance distribution (the dynamics of the model as we will see later), we can re-weight particles
				based on the likelihood from the measurements (<span class="math inline">\(p(\mathbf{y} |
					\mathbf{x})\)</span>.</p>
			<p>Importance sampling is based on the identity:</p>
			<p><span class="math display">\[
					\begin{aligned}
					\mathbb{E}[\mathbf{g}(\mathbf{x}) | \mathbf{y}_{1:T}] &amp;= \int
					\mathbf{g}(\mathbf{x})p(\mathbf{x}|\mathbf{y}_{1:T})d\mathbf{x} \\
					&amp;= \int \left
					[\mathbf{g}(\mathbf{x})\frac{p(\mathbf{x}|\mathbf{y}_{1:T})}{\mathbf{\pi}(\mathbf{x}|\mathbf{y}_{1:T})}
					\right ] \mathbf{\pi}(\mathbf{x}|\mathbf{y}_{1:T}) d\mathbf{x}
					\end{aligned}
					\]</span></p>
			<p>Thus, it can be approximated as <span class="math display">\[
					\begin{aligned}
					\mathbb{E}[\mathbf{g}(\mathbf{x}) | \mathbf{y}_{1:T}] &amp;\approx \frac{1}{N} \sum_i^N
					\frac{p(\mathbf{x}^{(i)}|\mathbf{y}_{1:T})}{\mathbf{\pi}(\mathbf{x}^{(i)}|\mathbf{y}_{1:T})}\mathbf{g}(\mathbf{x}^{(i)})
					&amp;\approx \sum^N_i w^{(i)} \mathbf{g}(\mathbf{x}^{(i)})
					\end{aligned}
					\]</span></p>
			<p>where <span class="math inline">\(N\)</span> samples of <span class="math inline">\(\mathbf{x}\)</span>
				are drawn from the importance distribution <span
					class="math inline">\(\mathbf{\pi}(\mathbf{x}|\mathbf{y}_{1:T})\)</span></p>
			<p>And the weights are defined as:</p>
			<p><span class="math display">\[w^{(i)} = \frac{1}{N}
					\frac{p(\mathbf{x}^{(i)}|\mathbf{y}_{1:T})}{\mathbf{\pi}(\mathbf{x}^{(i)}|\mathbf{y}_{1:T})}\]</span>
			</p>
			<p>Computing <span class="math inline">\(p(\mathbf{x}^{(i)}|\mathbf{y}_{1:T}\)</span> is hard (if not
				impossible), but fortunately we can compute the unnormalized weight instead:</p>
			<p><span class="math display">\[w^{(i)}* =
					p(\mathbf{y}_{1:T}|\mathbf{x}^{(i)})p(\mathbf{x}^{(i))}{\mathbf{\pi}(\mathbf{x}^{(i)}|\mathbf{y}_{1:T})}\]</span>
			</p>
			<p>and normalizing it afterwards</p>
			<p><span class="math display">\[\sum^N_i w^{(i)*} = 1 \Rightarrow w^{(i)} = \frac{w^{*(i)}}{\sum^N_j
					w^{*(i)}}\]</span></p>
			<h3 id="sequential-importance-sampling">Sequential Importance Sampling</h3>
			<p>The last equation becomes more and more computationally expensive as T grows larger (the joint variable
				of the time series grows larger). Fortunately, Sequential Importance Sampling is an alternative
				recursive algorithm that has a fixed amount of computation at each iteration:</p>
			<p><span class="math display">\[
					\begin{aligned}
					p(\mathbf{x}_{0:k} | \mathbf{y}_{0:k}) &amp;\propto p(\mathbf{y}_k | \mathbf{x}_{0:k},
					\mathbf{y}_{1:k-1})p(\mathbf{x}_k | \mathbf{y}_{1:k-1}) \\
					&amp;\propto p(\mathbf{y}_k | \mathbf{x}_{k})p(\mathbf{x}_k | \mathbf{x}_{0:k-1},
					\mathbf{y}_{1:k-1})p(\mathbf{x}_{0:k-1} | \mathbf{y}_{1:k-1}) \\
					&amp;\propto p(\mathbf{y}_k | \mathbf{x}_{k})p(\mathbf{x}_k | \mathbf{x}_{k-1})p(\mathbf{x}_{0:k-1}
					| \mathbf{y}_{1:k-1})
					\end{aligned}
					\]</span></p>
			<p>The importance distribution is such that <span class="math inline">\(\mathbf{x}^i_{0:k} \sim
					\pi(\mathbf{x}_{0:k} | \mathbf{y}_{1:k})\)</span> with the according importance weight: <span
					class="math display">\[w^{(i)}_k \propto \frac{p(\mathbf{y}_k |
					\mathbf{x}^{(i)}_{k})p(\mathbf{x}^{(i)}_k | \mathbf{x}^{(i)}_{k-1})p(\mathbf{x}^{(i)}_{0:k-1} |
					\mathbf{y}_{1:k-1})}{\pi(\mathbf{x}_{0:k} | \mathbf{y}_{1:k})}\]</span></p>
			<p>We can express the importance distribution recursively: <span class="math display">\[\pi(\mathbf{x}_{0:k}
					| \mathbf{y}_{1:k}) = \pi(\mathbf{x}_{k} |\mathbf{x}_{0:k-1},
					\mathbf{y}_{1:k})\pi(\mathbf{x}_{0:k-1} | \mathbf{y}_{1:k-1})\]</span></p>
			<p>The recursive structure propagates to the weight itself:</p>
			<p><span class="math display">\[
					\begin{aligned}
					w^{(i)}_k &amp;\propto \frac{p(\mathbf{y}_k | \mathbf{x}^{(i)}_{k})p(\mathbf{x}^{(i)}_k |
					\mathbf{x}^{(i)}_{k-1})}{\pi(\mathbf{x}_{k} |\mathbf{x}_{0:k-1}, \mathbf{y}_{1:k})}
					\frac{p(\mathbf{x}^{(i)}_{0:k-1} | \mathbf{y}_{1:k-1})}{\pi(\mathbf{x}_{0:k-1} |
					\mathbf{y}_{1:k-1})} \\
					&amp;\propto \frac{p(\mathbf{y}_k | \mathbf{x}^{(i)}_{k})p(\mathbf{x}^{(i)}_k |
					\mathbf{x}^{(i)}_{k-1})}{\pi(\mathbf{x}_{k} |\mathbf{x}_{0:k-1}, \mathbf{y}_{1:k})} w^{(i)}_{k-1}
					\end{aligned}
					\]</span></p>
			<p>We can further simplify the formula by choosing the importance distribution to be the dynamics of the
				model: <span class="math display">\[\pi(\mathbf{x}_{k} |\mathbf{x}_{0:k-1}, \mathbf{y}_{1:k}) =
					p(\mathbf{x}^{(i)}_k | \mathbf{x}^{(i)}_{k-1})\]</span> <span class="math display">\[ w^{*(i)}_k =
					p(\mathbf{y}_k | \mathbf{x}^{(i)}_{k}) w^{(i)}_{k-1}\]</span></p>
			<p>As previously, it is then only needed to normalize the resulting weight.</p>
			<p><span class="math display">\[\sum^N_i w^{(i)*} = 1 \Rightarrow w^{(i)} = \frac{w^{*(i)}}{\sum^N_j
					w^{*(i)}}\]</span></p>
			<h3 id="resampling">Resampling</h3>
			<p>When the number of effective particles is too low (less than <span class="math inline">\(1/10\)</span> of
				N having weight <span class="math inline">\(1/10\)</span>), we apply systematic resampling. The idea
				behind resampling is simple. The distribution is represented by a number of particles with different
				weights. As time goes on, the repartition of weights degenerates. A large subset of particles end up
				having negligible weight which make them irrelevant and only a few particles represent most of the
				distribution. In the most extreme case, one particle represents the whole distribution.</p>
			<p>To avoid that degeneration, when the weights are too unbalanced, we resample from the weights
				distribution: pick N times among the particle and assign them a weight of <span
					class="math inline">\(1/N\)</span>, each pick has odd <span class="math inline">\(w_i\)</span> to
				pick the particle <span class="math inline">\(p_i\)</span>. Thus, some particles with large weights are
				split up into smaller clone particle and others with small weights are never picked. This process is
				remotely similar to evolution: at each generation, the most promising branch survives and replicate
				while the less promising dies off.</p>
			<p>A popular method for resampling is systematic sampling as described by <span class="citation">[<a
						href="#ref-doucet_tutorial_2009">4</a>]</span>:</p>
			<p>Sample <span class="math inline">\(U_1 \sim \mathcal{U} [0, \frac{1}{N} ]\)</span> and define <span
					class="math inline">\(U_i = U_1 + \frac{i-1 }{N}\)</span> for <span class="math inline">\(i = 2,
					\ldots, N\)</span></p>
			<h2 id="rao-blackwellized-particle-filter">Rao-Blackwellized Particle Filter</h2>
			<h3 id="introduction-1">Introduction</h3>
			<p>Compared to a plain particle filter, RBPF leverages the linearity of some components of the state by
				assuming our model to be Gaussian conditioned on a latent variable: Given the attitude <span
					class="math inline">\(q_t\)</span>, our model is linear. This is where RBPF shines: We use particle
				filtering to estimate our latent variable, the attitude, and we use the optimal kalman filter to
				estimate the state variable. If a plain particle can be seen as the simple average of particle states,
				then the RBPF can be seen as the “average” of many Gaussians. Each particle is an optimal kalman filter
				conditioned on the particle’s latent variable, the attitude.</p>
			<p>Indeed, the advantage of particle filters is that they assume no particular form for the posterior
				distribution and transformation of the state. But as the state widens in dimensions, the number of
				needed particles to keep a good estimation grows exponentially. This is a consequence of [“the curse of
				dimensionality”}(https://en.wikipedia.org/wiki/Curse_of_dimensionality): for each dimension, we would
				have to consider all additional combination of state components. In our context, we have 10 dimensions
				(<span class="math inline">\(\mathbf{v}\)</span>,<span class="math inline">\(\mathbf{p}\)</span>,<span
					class="math inline">\(\mathbf{q}\)</span>), which is already large, and it would be computationally
				expensive to simulate a too large number of particles.</p>
			<p>Kalman filters on the other hand do not suffer from such exponential growth, but as explained previously,
				they are inadequate for non-linear transformations. RBPF is the best of both worlds by combining a
				particle filter for the non-linear components of the state (the attitude) as a latent variable, and
				Kalman filters for the linear components of the state (velocity and position). For ease of notation, the
				linear component of the state will be referred to as the state and designated by <span
					class="math inline">\(\mathbf{x}\)</span> even though the actual state we are concerned with should
				include the latent variable <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
			<h3 id="related-work">Related work</h3>
			<p>Related work of this approach is <span class="citation">[<a
						href="#ref-vernaza_rao-blackwellized_2006">5</a>]</span>. However, it differs by:</p>
			<ul>
				<li>adapting the filter to drones by taking into account that the system is too dynamic for assuming
					that the accelerometer simply output the gravity vector. This is solved by augmenting the state with
					the acceleration as shown later.</li>
				<li>add an attitude sensor.</li>
			</ul>
			<h3 id="latent-variable">Latent variable</h3>
			<p>We introduce the latent variable <span class="math inline">\(\boldsymbol{\theta}\)</span></p>
			<p>The latent variable <span class="math inline">\(\boldsymbol{\theta}\)</span> has for sole component the
				attitude: <span class="math display">\[\boldsymbol{\theta} = (\mathbf{q})\]</span></p>
			<p><span class="math inline">\(q_t\)</span> is estimated from the product of the attitude of all particles
				<span class="math inline">\(\mathbf{\theta^{(i)}} = \mathbf{q}^{(i)}_t\)</span> as the “average”
				quaternion <span class="math inline">\(\mathbf{q}_t = avgQuat(\mathbf{q}^n_t)\)</span>. <span
					class="math inline">\(x^n\)</span> designates the product of all n arbitrary particle.</p>
			<p>As stated in the previous section, The weight definition is:</p>
			<p><span class="math display">\[w^{(i)}_t = \frac{p(\boldsymbol{\theta}^{(i)}_{0:t} |
					\mathbf{y}_{1:t})}{\pi(\boldsymbol{\theta}^{(i)}_{0:t} | \mathbf{y}_{1:t})}\]</span></p>
			<p>From the definition and the previous section, it is provable that:</p>
			<p><span class="math display">\[w^{(i)}_t \propto \frac{p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1},
					\mathbf{y}_{1:t-1})p(\boldsymbol{\theta}^{(i)}_t |
					\boldsymbol{\theta}^{(i)}_{t-1})}{\pi(\boldsymbol{\theta}^{(i)}_t |
					\boldsymbol{\theta}^{(i)}_{1:t-1}, \mathbf{y}_{1:t})} w^{(i)}_{t-1}\]</span></p>
			<p>We choose the dynamics of the model as the importance distribution:</p>
			<p><span class="math display">\[\pi(\boldsymbol{\theta}^{(i)}_t | \boldsymbol{\theta}^{(i)}_{1:t-1},
					\mathbf{y}_{1:t}) = p(\boldsymbol{\theta}^{(i)}_t | \boldsymbol{\theta}^{(i)}_{t-1}) \]</span></p>
			<p>Hence,</p>
			<p><span class="math display">\[w^{*(i)}_t \propto p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1},
					\mathbf{y}_{1:t-1}) w^{(i)}_{t-1}\]</span></p>
			<p>We then sum all <span class="math inline">\(w^{*(i)}_t\)</span> to find the normalization constant and
				retrieve the actual <span class="math inline">\(w^{(i)}_t\)</span></p>
			<h3 id="state-4">State</h3>
			<p><span class="math display">\[\mathbf{x}_t = (\mathbf{v}_t, \mathbf{p}_t)^T\]</span></p>
			<p>Initial state <span class="math inline">\(\mathbf{x}_0 = (\mathbf{0}, \mathbf{0}, \mathbf{0})\)</span>
			</p>
			<p>Initial covariance matrix <span class="math inline">\(\mathbf{\Sigma}_{6 \times 6} = \epsilon
					\mathbf{I}_{6 \times 6}\)</span></p>
			<h3 id="latent-variable-1">Latent variable</h3>
			<p><span class="math display">\[\mathbf{q}^{(i)}_{t+1} = \mathbf{q}^{(i)}_t*R2Q({\Delta t}
					(\mathbf{\boldsymbol{\omega}_G}_t+\mathbf{\boldsymbol{\omega}_G}^\epsilon_t))\]</span></p>
			<p><span class="math inline">\(\mathbf{\boldsymbol{\omega}_G}^\epsilon_t\)</span> represents the error from
				the control input and is sampled from <span
					class="math inline">\(\mathbf{\boldsymbol{\omega}_G}^\epsilon_t \sim \mathcal{N}(\mathbf{0},
					\mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}_t })\)</span></p>
			<p>Initial attitude <span class="math inline">\(\mathbf{q_0}\)</span> is sampled such that the drone pitch
				and roll are none (parallel to the ground) but the yaw is unknown and uniformly distributed.</p>
			<p>Note that <span class="math inline">\(\mathbf{q}(t+1)\)</span> is known in the <a
					href="#model-dynamic">model dynamic</a> because the model is conditioned under <span
					class="math inline">\(\boldsymbol{\theta}^{(i)}_{t+1}\)</span>.</p>
			<h3 id="indoor-measurement-model">Indoor Measurement model</h3>
			<ol style="list-style-type: decimal">
				<li>Position: <span class="math display">\[\mathbf{p_V}(t) = \mathbf{p}(t)^{(i)} +
						\mathbf{p_V}^\epsilon_t\]</span> where <span class="math inline">\(\mathbf{p_V}^\epsilon_t \sim
						\mathcal{N}(\mathbf{0}, \mathbf{R}_{\mathbf{p_V}_t })\)</span></li>
				<li>Attitude: <span class="math display">\[\mathbf{q_V}(t) =
						\mathbf{q}(t)^{(i)}*R2Q(\mathbf{q_V}^\epsilon_t)\]</span> where <span
						class="math inline">\(\mathbf{q_V}^\epsilon_t \sim \mathcal{N}(\mathbf{0},
						\mathbf{R}_{\mathbf{q_V}_t })\)</span></li>
			</ol>
			<h3 id="kalman-prediction-1">Kalman prediction</h3>
			<p>The model dynamics define the following model, state-transition matrix <span
					class="math inline">\(\mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span>, the control-input matrix
				<span class="math inline">\(\mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span>, the process noise
				<span class="math inline">\(\mathbf{w}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span> for the Kalman filter
				and its covariance <span class="math inline">\(\mathbf{Q}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span></p>
			<p><span class="math display">\[\mathbf{x}_t = \mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{x}_{t-1}
					+ \mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{u}_t +
					\mathbf{w}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span></p>
			<p><span class="math display">\[\mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 6} =
					\left( \begin{array}{cc}
					\mathbf{I}_{3 \times 3} &amp; 0 \\
					\Delta t~\mathbf{I}_{3 \times 3} &amp; \mathbf{I}_{3 \times 3}
					\end{array} \right)\]</span></p>
			<p><span class="math display">\[\mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 3} =
					\left( \begin{array}{c}
					\mathbf{R}_{b2f}\{\mathbf{q}^{(i)}_{t}\}\mathbf{a_A} \\
					\mathbf{0}_{3 \times 3} \\
					\end{array} \right)\]</span></p>
			<p><span class="math display">\[\mathbf{Q}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 6} =
					\left( \begin{array}{cc}
					\mathbf{R}_{b2f}\{\mathbf{q}^{(i)}_{t}\}(\mathbf{Q}_{\mathbf{a}_t } *
					dt^2)\mathbf{R}^t_{b2f}\{\mathbf{q}^{(i)}_{t}\} &amp; \\
					&amp; \mathbf{Q}_{\mathbf{v}_t }\\
					\end{array} \right)\]</span></p>
			<p><span class="math display">\[\hat{\mathbf{x}}^{-(i)}_t = \mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\}
					\mathbf{x}^{(i)}_{t-1} + \mathbf{B}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{u}_t \]</span> <span
					class="math display">\[ \mathbf{\Sigma}^{-(i)}_t = \mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\}
					\mathbf{\Sigma}^{-(i)}_{t-1} (\mathbf{F}_t\{\boldsymbol{\theta}^{(i)}_t\})^T +
					\mathbf{Q}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span></p>
			<h3 id="kalman-measurement-update">Kalman measurement update</h3>
			<p>The <a href="#measurements-model-1">measurement model</a> defines how to compute <span
					class="math inline">\(p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1},
					\mathbf{y}_{1:t-_K1})\)</span></p>
			<p>Indeed, The measurement model defines the observation matrix <span
					class="math inline">\(\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span>, the observation noise
				<span class="math inline">\(\mathbf{v}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span> and its covariance
				matrix <span class="math inline">\(\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}\)</span> for the Kalman
				filter.</p>
			<p><span class="math display">\[(\mathbf{a_A}_t, \mathbf{p_V}_t)^T =
					\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\} (\mathbf{v}_t, \mathbf{p}_t)^T +
					\mathbf{v}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span></p>
			<p><span class="math display">\[\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}_{6 \times 3} =
					\left( \begin{array}{cc}
					\mathbf{0}_{3 \times 3} &amp; \\
					&amp; \mathbf{I}_{3 \times 3} \\
					\end{array} \right)\]</span></p>
			<p><span class="math display">\[\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}_{3 \times 3} =
					\left( \begin{array}{c}
					\mathbf{R}_{\mathbf{p_V}_t}
					\end{array} \right)\]</span></p>
			<h3 id="kalman-update-1">Kalman update</h3>
			<p><span class="math display">\[\mathbf{S} = \mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}
					\mathbf{\Sigma}^{-(i)}_t (\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\})^T +
					\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}\]</span> <span class="math display">\[\hat{\mathbf{z}} =
					\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\} \hat{\mathbf{x}}^{-(i)}_t\]</span> <span
					class="math display">\[\mathbf{K} = \mathbf{\Sigma}^{-(i)}_t
					\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}^T \mathbf{S}^{-1}\]</span> <span
					class="math display">\[\mathbf{\Sigma}^{(i)}_t = \mathbf{\Sigma}^{-(i)}_t + \mathbf{K} \mathbf{S}
					\mathbf{K}^T\]</span> <span class="math display">\[\hat{\mathbf{x}}^{(i)}_t =
					\hat{\mathbf{x}}^{-(i)}_t + \mathbf{K}((\mathbf{a_A}_t, \mathbf{p_V}_t)^T -
					\hat{\mathbf{z}})\]</span> <span class="math display">\[p(\mathbf{y}_t |
					\boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) = \mathcal{N}((\mathbf{a_A}_t,
					\mathbf{p_V}_t)^T; \hat{\mathbf{z}}_t, \mathbf{S})\]</span></p>
			<h3 id="asynchronous-measurements">Asynchronous measurements</h3>
			<p>Our measurements might have different sampling rate so instead of doing full kalman update, we only apply
				a partial kalman update corresponding to the current type of measurement <span
					class="math inline">\(\mathbf{z}_t\)</span>.</p>
			<p>For indoor drones, there is only one kind of sensor for the Kalman update: <span
					class="math inline">\(\mathbf{p_V}\)</span></p>
			<h3 id="attitude-re-weighting">Attitude re-weighting</h3>
			<p>In the <a href="#measurements-model">measurement model</a>, the attitude defines another re-weighting for
				importance sampling.</p>
			<p><span class="math display">\[p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) =
					\mathcal{N}(Q2R({\mathbf{q}^{(i)}}^{-1}\mathbf{q_V}_t);~ 0 ,~ \mathbf{R}_{\mathbf{q_V}})\]</span>
			</p>
			<h2 id="algorithm-summary">Algorithm summary</h2>
			<ol style="list-style-type: decimal">
				<li>Initiate <span class="math inline">\(N\)</span> particles with <span
						class="math inline">\(\mathbf{x}_0\)</span>, <span class="math inline">\(\mathbf{q}_0 ~ \sim
						p(\mathbf{q}_0)\)</span>, <span class="math inline">\(\mathbf{\Sigma}_0\)</span> and <span
						class="math inline">\(w = 1/N\)</span></li>
				<li>While new sensor measurements <span class="math inline">\((\mathbf{z}_t, \mathbf{u}_t)\)</span></li>
			</ol>
			<ul>
				<li>foreach <span class="math inline">\(N\)</span> particles <span class="math inline">\((i)\)</span>:
					<ol style="list-style-type: decimal">
						<li>Depending on the type of observation: - <strong>IMU</strong>:<br />
							1. store <span class="math inline">\(\boldsymbol{\mathbf{\omega_G}}_t\)</span> and <span
								class="math inline">\(\mathbf{a_A}_t\)</span> as last control inputs 2. sample new
							latent variable <span class="math inline">\(\boldsymbol{\theta_t}\)</span> from <span
								class="math inline">\(\boldsymbol{\mathbf{\omega_G}}_t\)</span> (which correspond to the
							last control inputs) 3. apply kalman prediction from <span
								class="math inline">\(\mathbf{a_A}_t\)</span> (which correspond to the last control
							inputs) - <strong>Vicon</strong>: 1. sample new latent variable <span
								class="math inline">\(\boldsymbol{\theta_t}\)</span> from <span
								class="math inline">\(\boldsymbol{\mathbf{\omega_G}}_t\)</span> (which correspond to the
							last control inputs) 2. apply kalman prediction from <span
								class="math inline">\(\mathbf{a_A}_t\)</span> (which correspond to the last control
							inputs) 3. Partial kalman update with: <span
								class="math display">\[\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\}_{3 \times 6} =
								(\mathbf{0}_{3 \times 3} ~~~~ \mathbf{I}_{3 \times 3} )\]</span> <span
								class="math display">\[\mathbf{R}_t\{\boldsymbol{\theta}^{(i)}_t\}_{3 \times 3} =
								\mathbf{R}_{\mathbf{p_V}_t }\]</span> <span class="math display">\[\mathbf{x}^{(i)}_t =
								\mathbf{H}_t\{\boldsymbol{\theta}^{(i)}_t\} \mathbf{x}^{(i)}_{t-1} +
								\mathbf{K}(\mathbf{p_V}_t - \hat{\mathbf{z}})\]</span> <span
								class="math display">\[p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1},
								\mathbf{y}_{1:t-1}) = \mathcal{N}(\mathbf{q_V}_t; \mathbf{q}^{(i)}_t,~
								\mathbf{R}_{\mathbf{q_V}_t } )\mathcal{N}(\mathbf{p_V}_t; \hat{\mathbf{z}}_t,
								\mathbf{S})\]</span>
							<ul>
								<li><strong>Other sensors (Outdoor)</strong>: As for <strong>Vicon</strong> but use the
									corresponding partial Kalman update</li>
							</ul>
						</li>
						<li>Update <span class="math inline">\(w^{(i)}_t\)</span>: <span class="math inline">\(w^{(i)}_t
								= p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1})
								w^{(i)}_{t-1}\)</span><br />
						</li>
					</ol>
				</li>
				<li>Normalize all <span class="math inline">\(w^{(i)}\)</span> by scalaing by <span
						class="math inline">\(1/(\sum w^{(i)})\)</span> such that <span class="math inline">\(\sum
						w^{(i)}= 1\)</span></li>
				<li>Compute <span class="math inline">\(\mathbf{p}_t\)</span> and <span
						class="math inline">\(\mathbf{q}_t\)</span> as the expectation from the distribution
					approximated by the N particles.</li>
				<li>Resample if the number of effective particle is too low</li>
			</ul>
			<h3 id="extension-to-outdoors">Extension to outdoors</h3>
			<p>As highlighted in the Algorithm summary, the RBPF if easily extensible to other sensors. Indeed,
				measurements are either:</p>
			<ul>
				<li>giving information about position or velocity and their update is similar to the vicon position
					update as a kalman partial update</li>
				<li>giving information about the orientation and their update is similar to the vicon attitude update as
					a pure importance sampling re-weighting.</li>
			</ul>
			<p>A proof-of-concept alternative Rao-blackwellized particle filter specialized for outdoor use has been
				developed that integrates the following sensors:</p>
			<ul>
				<li>IMU with accelerometer, gyroscope <strong>and magnetometer</strong></li>
				<li>Altimeter</li>
				<li>Dual GPS (2 GPS)</li>
				<li>Optical Flow</li>
			</ul>
			<p>The optical flow measurements are assumed to be of the form <span class="math inline">\((\Delta
					\mathbf{p}, \Delta \mathbf{q})\)</span> for a <span class="math inline">\(\Delta t\)</span>
				corresponding to its sampling rate. It is inputed to the particle filter as a likelihood:</p>
			<p><span class="math display">\[p(\mathbf{y}_t | \boldsymbol{\theta}^{(i)}_{0:t-1}, \mathbf{y}_{1:t-1}) =
					\mathcal{N}(\mathbf{p}_{t1} + \Delta p; \mathbf{p}_{t2};
					\mathbf{R}_{\mathbf{dp_O}_t})\mathcal{N}(\Delta \mathbf{q}; \mathbf{q}_{t1}^{-1}\mathbf{q}_{t2};
					\mathbf{R}_{\mathbf{dq_O}_t})\]</span></p>
			<p>where <span class="math inline">\(t2 = t1 + \Delta t\)</span>, <span
					class="math inline">\(\mathbf{p}_{t2}\)</span> is the latest kalman prediction and <span
					class="math inline">\(\mathbf{q}_{t2}\)</span> is the latest latent variable through sampling of the
				attitude updates.</p>
			<h2 id="results">Results</h2>
			<p>We present a comparison of the 4 filters in 6 settings. The metrics is the RMSE of the l2-norm of the
				position and of the Froebius norm of the attitude as described previously. All the filters share a
				sampling frequency of <strong>200Hz</strong> for the IMU and <strong>4Hz</strong> for the Vicon. The
				RBPF is set to <strong>1000</strong> particles</p>
			<p>In all scenarios, the covariance matrices of the sensors’ measurements are diagonal:</p>
			<ul>
				<li><span class="math inline">\(\mathbf{R}_{\mathbf{a_A}} = \sigma^2_{\mathbf{\mathbf{a_A}}}
						\mathbf{I}_{3 \times 3}\)</span></li>
				<li><span class="math inline">\(\mathbf{R}_{\mathbf{\boldsymbol{\omega}_G}} =
						\sigma^2_{\mathbf{\boldsymbol{\omega}_G}} \mathbf{I}_{3 \times 3}\)</span></li>
				<li><span class="math inline">\(\mathbf{R}_{\mathbf{p_V}} = \sigma^2_{\mathbf{p_V}} \mathbf{I}_{3 \times
						3}\)</span></li>
				<li><span class="math inline">\(\mathbf{R}_{\mathbf{q_V}} = \sigma^2_{\mathbf{q_V}} \mathbf{I}_{3 \times
						3}\)</span></li>
			</ul>
			<p>with the following settings:</p>
			<ul>
				<li><strong>Vicon</strong>:
					<ul>
						<li>High-precision <span class="math inline">\(\sigma^2_{\mathbf{p_V}} = \sigma^2_{\mathbf{q_V}}
								= 0.01\)</span></li>
						<li>Low-precision <span class="math inline">\(\sigma^2_{\mathbf{p_V}} = \sigma^2_{\mathbf{q_V}}
								= 0.1\)</span><br />
						</li>
					</ul>
				</li>
				<li><strong>Accelerometer</strong>:
					<ul>
						<li>High-precision: <span class="math inline">\(\sigma^2_{\mathbf{\mathbf{a_A}}} = 0.1\)</span>
						</li>
						<li>Low-precision: <span class="math inline">\(\sigma^2_{\mathbf{\mathbf{a_A}}} = 1.0\)</span>
						</li>
					</ul>
				</li>
				<li><strong>Gyroscope</strong>:
					<ul>
						<li>High-precision: <span class="math inline">\(\sigma^2_{\mathbf{\boldsymbol{\omega}_G}} =
								0.1\)</span></li>
						<li>Low-precision: <span class="math inline">\(\sigma^2_{\mathbf{\boldsymbol{\omega}_G}} =
								1.0\)</span></li>
					</ul>
				</li>
			</ul>
			<table style="width:88%;">
				<caption>position RMSE over 5 random trajectories of 20 seconds</caption>
				<colgroup>
					<col width="7%" />
					<col width="8%" />
					<col width="8%" />
					<col width="17%" />
					<col width="12%" />
					<col width="13%" />
					<col width="18%" />
				</colgroup>
				<thead>
					<tr class="header">
						<th>Vicon preci sion</th>
						<th>Accel. preci.</th>
						<th>Gyros. preci.</th>
						<th>Augmented Complementary Filter</th>
						<th>Extended Kalman Filter</th>
						<th>Unscented Kalman Filter</th>
						<th>Rao -Blackwellized Particle Filter</th>
					</tr>
				</thead>
				<tbody>
					<tr class="odd">
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>6.88e-02</p>
						</td>
						<td>
							<p>3.26e-02</p>
						</td>
						<td>
							<p>3.45e-02</p>
						</td>
						<td>
							<p><strong>1.45e-02</strong></p>
						</td>
					</tr>
					<tr class="even">
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>6.10e-02</p>
						</td>
						<td>
							<p>1.13e-01</p>
						</td>
						<td>
							<p>9.20e-02</p>
						</td>
						<td>
							<p><strong>2.17e-02</strong></p>
						</td>
					</tr>
					<tr class="odd">
						<td>
							<p>High</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>4.05e-02</p>
						</td>
						<td>
							<p>5.24e-02</p>
						</td>
						<td>
							<p>3.29e-02</p>
						</td>
						<td>
							<p><strong>1.61e-02</strong></p>
						</td>
					</tr>
					<tr class="even">
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>5.05e-01</p>
						</td>
						<td>
							<p>5.05e-01</p>
						</td>
						<td>
							<p>2.90e-01</p>
						</td>
						<td>
							<p><strong>1.27e-01</strong></p>
						</td>
					</tr>
					<tr class="odd">
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>6.16e-01</p>
						</td>
						<td>
							<p>1.09e+00</p>
						</td>
						<td>
							<p>9.30e-01</p>
						</td>
						<td>
							<p><strong>1.22e-01</strong></p>
						</td>
					</tr>
					<tr class="even">
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>3.57e-01</p>
						</td>
						<td>
							<p>2.66e-01</p>
						</td>
						<td>
							<p>3.27e-01</p>
						</td>
						<td>
							<p><strong>1.19e-01</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<table style="width:88%;">
				<caption>attitude RMSE over 5 random trajectories of 20 seconds</caption>
				<colgroup>
					<col width="7%" />
					<col width="8%" />
					<col width="8%" />
					<col width="17%" />
					<col width="12%" />
					<col width="13%" />
					<col width="18%" />
				</colgroup>
				<thead>
					<tr class="header">
						<th>Vicon preci sion</th>
						<th>Accel. preci.</th>
						<th>Gyros. preci.</th>
						<th>Augmented Complementary Filter</th>
						<th>Extended Kalman Filter</th>
						<th>Unscented Kalman Filter</th>
						<th>Rao -Blackwellized Particle Filter</th>
					</tr>
				</thead>
				<tbody>
					<tr class="odd">
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>7.36e-03</p>
						</td>
						<td>
							<p>5.86e-03</p>
						</td>
						<td>
							<p>5.17e-03</p>
						</td>
						<td>
							<p><strong>1.01e-04</strong></p>
						</td>
					</tr>
					<tr class="even">
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>6.37e-03</p>
						</td>
						<td>
							<p>1.37e-02</p>
						</td>
						<td>
							<p>9.17e-03</p>
						</td>
						<td>
							<p><strong>6.50e-04</strong></p>
						</td>
					</tr>
					<tr class="odd">
						<td>
							<p>High</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>6.25e-03</p>
						</td>
						<td>
							<p>1.69e-02</p>
						</td>
						<td>
							<p>1.02e-02</p>
						</td>
						<td>
							<p><strong>8.34e-04</strong></p>
						</td>
					</tr>
					<tr class="even">
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>5.30e-01</p>
						</td>
						<td>
							<p>3.28e-01</p>
						</td>
						<td>
							<p>3.26e-01</p>
						</td>
						<td>
							<p><strong>5.82e-03</strong></p>
						</td>
					</tr>
					<tr class="odd">
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>High</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>5.18e-01</p>
						</td>
						<td>
							<p>2.99e-01</p>
						</td>
						<td>
							<p>2.95e-01</p>
						</td>
						<td>
							<p><strong>5.78e-03</strong></p>
						</td>
					</tr>
					<tr class="even">
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>Low</p>
						</td>
						<td>
							<p>5.90e-01</p>
						</td>
						<td>
							<p>3.28e-01</p>
						</td>
						<td>
							<p>3.24e-01</p>
						</td>
						<td>
							<p><strong>3.97e-03</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Figure 1.13 is a bar plot of the first line of each table.</p>
			<div class="figure">
				<img src="barplot.png" alt="Bar plot in the High/High/High setting" />
				<p class="caption">Bar plot in the High/High/High setting</p>
			</div>
			<p>Figure 1.14 is the plot of the tracking of the position (x, y, z) and attitute (r, i, j, k) in the
				<strong>low</strong> vicon precision, <strong>low</strong> accelerometer precision and
				<strong>low</strong> gyroscope precision setting for one of random trajectory.</p>
			<div class="figure">
				<img src="full-plot.png" alt="Plot of the tracking of the different filters" style="width:120.0%" />
				<p class="caption">Plot of the tracking of the different filters</p>
			</div>
			<h2 id="conclusion">Conclusion</h2>
			<p>The Rao-Blackwellized Particle Filter developed is more accurate than the alternatives, mathematically
				sound and computationally feasible. When implemented on hardware, this filter can be executed in real
				time with sensors of high and asynchronous sampling rate. It could improve POSE estimation for all the
				existing drone and other robots. These improvements could unlock new abilities, potentials and increase
				the safeness of drone.</p>
			<h2 id="part-ii">Part II</h2>
			<p><a href="../../posts/th2/2017-08-16-thesis-part-2.html">Continue here to read the section about
					scala-flow (II/IV)</a></p>
			<h2 id="references" class="unnumbered">References</h2>
			<div id="refs" class="references">
				<div id="ref-mueller_computationally_2015">
					<p>[1] M. W. Mueller, M. Hehn, and R. D’Andrea, “A computationally efficient motion primitive for
						quadrocopter trajectory generation,” <em>IEEE Transactions on Robotics</em>, vol. 31, no. 6, pp.
						1294–1310, 2015.</p>
				</div>
				<div id="ref-markley_averaging_2007">
					<p>[2] F. L. Markley, Y. Cheng, J. L. Crassidis, and Y. Oshman, “Averaging quaternions,” <em>Journal
							of Guidance, Control, and Dynamics</em>, vol. 30, no. 4, pp. 1193–1197, 2007.</p>
				</div>
				<div id="ref-edgar_quaternion-based_nodate">
					<p>[3] K. Edgar, “A Quaternion-based Unscented Kalman Filter for Orientation Tracking.”.</p>
				</div>
				<div id="ref-doucet_tutorial_2009">
					<p>[4] A. Doucet and A. M. Johansen, “A tutorial on particle filtering and smoothing: Fifteen years
						later,” <em>Handbook of nonlinear filtering</em>, vol. 12, nos. 656-704, p. 3, 2009.</p>
				</div>
				<div id="ref-vernaza_rao-blackwellized_2006">
					<p>[5] P. Vernaza and D. D. Lee, “Rao-Blackwellized particle filtering for 6-DOF estimation of
						attitude and position via GPS and inertial sensors,” in <em>Robotics and Automation, 2006. ICRA
							2006. Proceedings 2006 IEEE International Conference on</em>, 2006, pp. 1571–1578.</p>
				</div>
			</div>
			<div class="footnotes">
				<hr />
				<ol>
					<li id="fn1">
						<p>The observation that the number of transistors in a dense integrated circuit doubles
							approximately every two years.<a href="#fnref1">↩</a></p>
					</li>
					<li id="fn2">
						<p>An embarrassingly parallel task is one where little or no effort is needed to separate the
							problem into a number of parallel tasks. This is often the case where there is little or no
							dependency or need for communication between those parallel tasks, or for results between
							them.<a href="#fnref2">↩</a></p>
					</li>
					<li id="fn3">
						<p>Gimbal lock is the loss of one degree of freedom in a three-dimensional, three-gimbal
							mechanism that occurs when the axes of two of the three gimbals are driven into a parallel
							configuration, “locking” the system into rotation in a degenerate two-dimensional space.
							<!--  LocalWords:  moore nm moorelaw hwsf png
 --><a href="#fnref3">↩</a>
						</p>
					</li>
					<li id="fn4">
						<p>The etymology for “Dead reckoning” comes from the mariners of the XVIIth century that used to
							calculate the position of the vessel with log book. The interpretation of “dead” is subject
							to debate. Some argue that it is a misspelling of “ded” as in “deduced”. Others argue that
							it should be read by its old meaning: <em>absolute</em>.<a href="#fnref4">↩</a></p>
					</li>
				</ol>
			</div>
		</div>
		<div class="license" style="margin-left:80%;">
			<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Licence Creative Commons"
					style="border-width:0" src="../../images/cc.png" /></a>
		</div>
	</div>

	<hr>

	<div id="footer">
		<section class="social">
			<a href="https://ch.linkedin.com/in/rubenfiszel" target="_blank"><i class="fa fa-linkedin fa-2x"></i></a>
			<a href="https://github.com/rubenfiszel" target="_blank"><i class="fa fa-github fa-2x"></i></a>
			<a href="mailto:ruben@rubenfiszel.com"><i class="fa fa-envelope fa-2x"></i></a>
			<a href="assets/RubenFiszel_resume.pdf"><i class="fa fa-file fa-2x"></i></a>
		</section>
	</div>

	<script src="../../js/mermaid.min.js"></script>
	<script>mermaid.initialize({startOnLoad:true});</script>

	<script>
		(function (i, s, o, g, r, a, m) {
			i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
				(i[r].q = i[r].q || []).push(arguments)
			}, i[r].l = 1 * new Date(); a = s.createElement(o),
				m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
		})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

		ga('create', 'UA-3040887-4', 'auto');
		ga('send', 'pageview');

	</script>
</body>

</html>
